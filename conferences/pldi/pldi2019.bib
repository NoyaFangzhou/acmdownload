@inproceedings{10.1145/3314221.3314624,
author = {Pulte, Christopher and Pichon-Pharabod, Jean and Kang, Jeehoon and Lee, Sung-Hwan and Hur, Chung-Kil},
title = {Promising-ARM/RISC-V: A Simpler and Faster Operational Concurrency Model},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314624},
doi = {10.1145/3314221.3314624},
abstract = {For ARMv8 and RISC-V, there are concurrency models in two styles, extensionally equivalent: axiomatic models, expressing the concurrency semantics in terms of global properties of complete executions; and operational models, that compute incrementally. The latter are in an abstract microarchitectural style: they execute each instruction in multiple steps, out-of-order and with explicit branch speculation. This similarity to hardware implementations has been important in developing the models and in establishing confidence, but involves complexity that, for programming and model-checking, one would prefer to avoid.  We present new more abstract operational models for ARMv8 and RISC-V, and an exploration tool based on them. The models compute the allowed concurrency behaviours incrementally based on thread-local conditions and are significantly simpler than the existing operational models: executing instructions in a single step and (with the exception of early writes) in program order, and without branch speculation. We prove the models equivalent to the existing ARMv8 and RISC-V axiomatic models in Coq. The exploration tool is the first such tool for ARMv8 and RISC-V fast enough for exhaustively checking the concurrency behaviour of a number of interesting examples. We demonstrate using the tool for checking several standard concurrent datastructure and lock implementations, and for interactively stepping through model-allowed executions for debugging.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1–15},
numpages = {15},
keywords = {Operational Semantics, RISC-V, ARM, Relaxed Memory Models},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314611,
author = {Liu, Lun and Millstein, Todd and Musuvathi, Madanlal},
title = {Accelerating Sequential Consistency for Java with Speculative Compilation},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314611},
doi = {10.1145/3314221.3314611},
abstract = {A memory consistency model (or simply a memory model) specifies the granularity and the order in which memory accesses by one thread become visible to other threads in the program. We previously proposed the volatile-by-default (VBD) memory model as a natural form of sequential consistency (SC) for Java. VBD is significantly stronger than the Java memory model (JMM) and incurs relatively modest overheads in a modified HotSpot JVM running on Intel x86 hardware. However, the x86 memory model is already quite close to SC. It is expected that the cost of VBD will be much higher on the other widely used hardware platform today, namely ARM, whose memory model is very weak. In this paper, we quantify this expectation by building and evaluating a baseline volatile-by-default JVM for ARM called VBDA-HotSpot, using the same technique previously used for x86. Through this baseline we report, to the best of our knowledge, the first comprehensive study of the cost of providing language-level SC for a production compiler on ARM. VBDA-HotSpot indeed incurs a considerable performance penalty on ARM, with average overheads on the DaCapo benchmarks on two ARM servers of 57% and 73% respectively. Motivated by these experimental results, we then present a novel speculative technique to optimize language-level SC. While several prior works have shown how to optimize SC in the context of an offline, whole-program compiler, to our knowledge this is the first optimization approach that is compatible with modern implementation technology, including dynamic class loading and just-in-time (JIT) compilation. The basic idea is to modify the JIT compiler to treat each object as thread-local initially, so accesses to its fields can be compiled without fences. If an object is ever accessed by a second thread, any speculatively compiled code for the object is removed, and future JITed code for the object will include the necessary fences in order to ensure SC. We demonstrate that this technique is effective, reducing the overhead of enforcing VBD by one-third on average, and additional experiments validate the thread-locality hypothesis that underlies the approach.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {16–30},
numpages = {15},
keywords = {sequential consistency, volatile by default, Java virtual machine, memory consistency models, speculative compilation},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.5281/zenodo.2645128,
author = {Liu, Lun and Millstein, Todd and Musuvathi, Madanlal},
title = {Schotspot-Aarch64: First Release for Artifact Registration for PLDI 2019},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.2645128},
abstract = {
    <p>This is the first release of the artifact of the PLDI 2019 paper "Accelerating Sequential Consistency for Java with Speculative Compilation". For the latest updates please check out the GitHub repo: https://github.com/Lun-Liu/schotspot-aarch64</p>
},
keywords = {Java virtual machine, memory consistency, sequential consistency, speculative compilation, volatile by default}
}

@inproceedings{10.1145/3314221.3314637,
author = {Prokopec, Aleksandar and Ros\`{a}, Andrea and Leopoldseder, David and Duboscq, Gilles and T\r{u}ma, Petr and Studener, Martin and Bulej, Lubom\'{\i}r and Zheng, Yudi and Villaz\'{o}n, Alex and Simon, Doug and W\"{u}rthinger, Thomas and Binder, Walter},
title = {Renaissance: Benchmarking Suite for Parallel Applications on the JVM},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314637},
doi = {10.1145/3314221.3314637},
abstract = {Established benchmark suites for the Java Virtual Machine (JVM), such as DaCapo, ScalaBench, and SPECjvm2008, lack workloads that take advantage of the parallel programming abstractions and concurrency primitives offered by the JVM and the Java Class Library. However, such workloads are fundamental for understanding the way in which modern applications and data-processing frameworks use the JVM's concurrency features, and for validating new just-in-time (JIT) compiler optimizations that enable more efficient execution of such workloads. We present Renaissance, a new benchmark suite composed of modern, real-world, concurrent, and object-oriented workloads that exercise various concurrency primitives of the JVM. We show that the use of concurrency primitives in these workloads reveals optimization opportunities that were not visible with the existing workloads. We use Renaissance to compare performance of two state-of-the-art, production-quality JIT compilers (HotSpot C2 and Graal), and show that the performance differences are more significant than on existing suites such as DaCapo and SPECjvm2008. We also use Renaissance to expose four new compiler optimizations, and we analyze the behavior of several existing ones. We use Renaissance to compare performance of two state-of-the-art, production-quality JIT compilers (HotSpot C2 and Graal), and show that the performance differences are more significant than on existing suites such as DaCapo and SPECjvm2008. We also use Renaissance to expose four new compiler optimizations, and we analyze the behavior of several existing ones.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {31–47},
numpages = {17},
keywords = {benchmarks, JVM, object-oriented programming benchmarks, JIT compilation, Big Data benchmarks, parallelism, functional programming benchmarks, concurrency},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325986,
author = {Prokopec, Aleksandar and Ros\`{a}, Andrea and Leopoldseder, David and Duboscq, Gilles and T?ma, Petr and Studener, Martin and Bulej, Lubom\'{\i}r and Zheng, Yudi and Villaz\'{o}n, Alex and Simon, Doug and W\"{u}rthinger, Thomas and Binder, Walter},
title = {Renaissance: Benchmarking Suite for Parallel Applications on the JVM},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325986},
abstract = {
    <p>Renaissance is a new benchmark suite composed of modern, real-world, concurrent, and object-oriented workloads that exercise various concurrency primitives of the JVM, with the goal of revealing optimization opportunities that were not visible with prior benchmark workloads. The artifact contains the Renaissance suite together with tools used to analyze the workloads and compare performance of two production level JIT compilers. The artifact also contains complete data sets of experiments described in the associated paper and scripts used to process those data sets for the figures and tables presented in the paper.</p>
},
keywords = {benchmarks, JIT compilation, parallelism}
}

@inproceedings{10.1145/3314221.3314631,
author = {Vollmer, Michael and Koparkar, Chaitanya and Rainey, Mike and Sakka, Laith and Kulkarni, Milind and Newton, Ryan R.},
title = {LoCal: A Language for Programs Operating on Serialized Data},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314631},
doi = {10.1145/3314221.3314631},
abstract = {In a typical data-processing program, the representation of data in memory is distinct from its representation in a serialized form on disk. The former has pointers and arbitrary, sparse layout, facilitating easy manipulation by a program, while the latter is packed contiguously, facilitating easy I/O. We propose a language, LoCal, to unify in-memory and serialized formats. LoCal extends a region calculus into a location calculus, employing a type system that tracks the byte-addressed layout of all heap values. We formalize LoCal and prove type safety, and show how LoCal programs can be inferred from unannotated source terms. We transform the existing Gibbon compiler to use LoCal as an intermediate language, with the goal of achieving a balance between code speed and data compactness by introducing just enough indirection into heap layouts, preserving the asymptotic complexity of traditional representations, but working with mostly or completely serialized data. We show that our approach yields significant performance improvement over prior approaches to operating on packed data, without abandoning idiomatic programming with recursive functions.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {48–62},
numpages = {15},
keywords = {Data Encoding, Region Calculus, Tree Traversal, Compiler Optimization},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325974,
author = {Vollmer, Michael and Koparkar, Chaitanya and Rainey, Mike and Sakka, Laith and Kulkarni, Milind and Newton, Ryan R.},
title = {Artifact for "LoCal: A Language for Programs Operating on Serialized Data"},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325974},
abstract = {
    <p>The artifact for our paper is the Gibbon compiler. It also includes the benchmarks presented in the paper, and some scripts to run them and plot appropriate graphs.</p>
},
keywords = {Compilers, Program Optimization, Tree Traversals}
}

@inproceedings{10.1145/3314221.3314633,
author = {Fremont, Daniel J. and Dreossi, Tommaso and Ghosh, Shromona and Yue, Xiangyu and Sangiovanni-Vincentelli, Alberto L. and Seshia, Sanjit A.},
title = {Scenic: A Language for Scenario Specification and Scene Generation},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314633},
doi = {10.1145/3314221.3314633},
abstract = {We propose a new probabilistic programming language for the design and analysis of perception systems, especially those based on machine learning. Specifically, we consider the problems of training a perception system to handle rare events, testing its performance under different conditions, and debugging failures. We show how a probabilistic programming language can help address these problems by specifying distributions encoding interesting types of inputs and sampling these to generate specialized training and test sets. More generally, such languages can be used for cyber-physical systems and robotics to write environment models, an essential prerequisite to any formal analysis. In this paper, we focus on systems like autonomous cars and robots, whose environment is a scene, a configuration of physical objects and agents. We design a domain-specific language, Scenic, for describing scenarios that are distributions over scenes. As a probabilistic programming language, Scenic allows assigning distributions to features of the scene, as well as declaratively imposing hard and soft constraints over the scene. We develop specialized techniques for sampling from the resulting distribution, taking advantage of the structure provided by Scenic's domain-specific syntax. Finally, we apply Scenic in a case study on a convolutional neural network designed to detect cars in road images, improving its performance beyond that achieved by state-of-the-art synthetic data generation methods.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {63–78},
numpages = {16},
keywords = {deep learning, fuzz testing, scenario description language, synthetic data, probabilistic programming, automatic test generation},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314597,
author = {Gopinath, Sridhar and Ghanathe, Nikhil and Seshadri, Vivek and Sharma, Rahul},
title = {Compiling KB-Sized Machine Learning Models to Tiny IoT Devices},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314597},
doi = {10.1145/3314221.3314597},
abstract = {Recent advances in machine learning (ML) have produced KiloByte-size models that can directly run on constrained IoT devices. This approach avoids expensive communication between IoT devices and the cloud, thereby enabling energy-efficient real-time analytics. However, ML models are expressed typically in floating-point, and IoT hardware typically does not support floating-point. Therefore, running these models on IoT devices requires simulating IEEE-754 floating-point using software, which is very inefficient. We present SeeDot, a domain-specific language to express ML inference algorithms and a compiler that compiles SeeDot programs to fixed-point code that can efficiently run on constrained IoT devices. We propose 1)&nbsp;a novel compilation strategy that reduces the search space for some key parameters used in the fixed-point code, and 2)&nbsp;new efficient implementations of expensive operations. SeeDot compiles state-of-the-art KB-sized models to various microcontrollers and low-end FPGAs. We show that SeeDot outperforms 1) software emulation of floating-point (Arduino), 2) high-bitwidth fixed-point (MATLAB), 3) post-training quantization (TensorFlow-Lite), and 4) floating- and fixed-point FPGA implementations generated using high-level synthesis tools.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {79–95},
numpages = {17},
keywords = {Fixed-point, Microcontroller, Programming Language, Compiler, FPGA, IoT device, Machine Learning},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314609,
author = {Kokologiannakis, Michalis and Raad, Azalea and Vafeiadis, Viktor},
title = {Model Checking for Weakly Consistent Libraries},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314609},
doi = {10.1145/3314221.3314609},
abstract = {We present GenMC, a model checking algorithm for concurrent programs that is parametric in the choice of memory model and can be used for verifying clients of concurrent libraries. Subject to a few basic conditions about the memory model, our algorithm is sound, complete and optimal, in that it explores each consistent execution of the program according to the model exactly once, and does not explore inconsistent executions or embark on futile exploration paths. We implement GenMC as a tool for verifying C programs. Despite the generality of the algorithm, its performance is comparable to the state-of-art specialized model checkers for specific memory models, and in certain cases exponentially faster thanks to its coarse partitioning of executions.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {96–110},
numpages = {15},
keywords = {Model checking, weak memory models},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325979,
author = {Kokologiannakis, Michalis and Raad, Azalea and Vafeiadis, Viktor},
title = {Replication Package for Article: "Model Checking for Weakly Consistent Libraries"},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325979},
abstract = {
    <p>We consider our paper's artifact to be the set of benchmarks we used in the paper, as well as the results we got by running particular versions of model checking tools (Nidhugg, Tracer, RCMC, and GenMC) on the benchmarks set. We do not consider the artifact of the paper to be GenMC, as it will evolve over time. We have made GenMC publicly available on GitHub (https://github.com/MPI-SWS/genmc), to further facilitate access to the tool. The artifact consists of a Virtual Machine (VM) containing binaries for Nidhugg, Tracer, RCMC, and GenMC, along with all the benchmarks used in the submitted version of our paper.</p>
},
keywords = {Model checking, weak memory models}
}

@inproceedings{10.1145/3314221.3314595,
author = {Jiang, Hanru and Liang, Hongjin and Xiao, Siyang and Zha, Junpeng and Feng, Xinyu},
title = {Towards Certified Separate Compilation for Concurrent Programs},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314595},
doi = {10.1145/3314221.3314595},
abstract = {Certified separate compilation is important for establishing end-to-end guarantees for certified systems consisting of multiple program modules. There has been much work building certified compilers for sequential programs. In this paper, we propose a language-independent framework consisting of the key semantics components and lemmas that bridge the verification gap between the compilers for sequential programs and those for (race-free) concurrent programs, so that the existing verification work for the former can be reused. One of the key contributions of the framework is a novel footprint-preserving compositional simulation as the compilation correctness criterion. The framework also provides a new mechanism to support confined benign races which are usually found in efficient implementations of synchronization primitives.  With our framework, we develop CASCompCert, which extends CompCert for certified separate compilation of race-free concurrent Clight programs. It also allows linking of concurrent Clight modules with x86-TSO implementations of synchronization primitives containing benign races. All our work has been implemented in the Coq proof assistant.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {111–125},
numpages = {15},
keywords = {Certified Compilers, Data-Race-Freedom, Concurrency, Simulations},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314604,
author = {Lahav, Ori and Margalit, Roy},
title = {Robustness against Release/Acquire Semantics},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314604},
doi = {10.1145/3314221.3314604},
abstract = {We present an algorithm for automatically checking robustness of concurrent programs against C/C++11 release/acquire semantics, namely verifying that all program behaviors under release/acquire are allowed by sequential consistency. Our approach reduces robustness verification to a reachability problem under (instrumented) sequential consistency. We have implemented our algorithm in a prototype tool called Rocker and applied it to several challenging concurrent algorithms. To the best of our knowledge, this is the first precise method for verifying robustness against a high-level programming language weak memory semantics.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {126–141},
numpages = {16},
keywords = {weak memory models, robustness, release/acquire, C/C++11},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325978,
author = {Lahav, Ori and Margalit, Roy},
title = {Rocker: Robustness Checker},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325978},
abstract = {
    <p>A program to transform TPL programs with C/C++11 release/acquire semantics to promela. The promela program can then be verified using Spin model checker.</p>
},
keywords = {C/C++11, D, release/acquire, robustness, weak memory models}
}

@inproceedings{10.1145/3314221.3314628,
author = {Dathathri, Roshan and Saarikivi, Olli and Chen, Hao and Laine, Kim and Lauter, Kristin and Maleki, Saeed and Musuvathi, Madanlal and Mytkowicz, Todd},
title = {CHET: An Optimizing Compiler for Fully-Homomorphic Neural-Network Inferencing},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314628},
doi = {10.1145/3314221.3314628},
abstract = {Fully Homomorphic Encryption (FHE) refers to a set of encryption schemes that allow computations on encrypted data without requiring a secret key. Recent cryptographic advances have pushed FHE into the realm of practical applications. However, programming these applications remains a huge challenge, as it requires cryptographic domain expertise to ensure correctness, security, and performance.  CHET is a domain-specific optimizing compiler designed to make the task of programming FHE applications easier. Motivated by the need to perform neural network inference on encrypted medical and financial data, CHET supports a domain-specific language for specifying tensor circuits. It automates many of the laborious and error prone tasks of encoding such circuits homomorphically, including encryption parameter selection to guarantee security and accuracy of the computation, determining efficient tensor layouts, and performing scheme-specific optimizations.  Our evaluation on a collection of popular neural networks shows that CHET generates homomorphic circuits that outperform expert-tuned circuits and makes it easy to switch across different encryption schemes. We demonstrate its scalability by evaluating it on a version of SqueezeNet, which to the best of our knowledge, is the deepest neural network to be evaluated homomorphically.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {142–156},
numpages = {15},
keywords = {domain-specific compiler, neural networks, privacy-preserving machine learning, Homomorphic encryption},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314636,
author = {Mercadier, Darius and Dagand, Pierre-\'{E}variste},
title = {Usuba: High-Throughput and Constant-Time Ciphers, by Construction},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314636},
doi = {10.1145/3314221.3314636},
abstract = {Cryptographic primitives are subject to diverging imperatives. Functional correctness and auditability pushes for the use of a high-level programming language. Performance and the threat of timing attacks push for using no more abstract than an assembler to exploit (or avoid!) the micro-architectural features of a given machine. We believe that a suitable programming language can reconcile both views and actually improve on the state of the art of both. Usuba is an opinionated dataflow programming language in which block ciphers become so simple as to be “obviously correct” and whose types document and enforce valid parallelization strategies at the granularity of individual bits. Its optimizing compiler, Usubac, produces high-throughput, constant-time implementations performing on par with hand-tuned reference implementations. The cornerstone of our approach is a systematization and generalization of bitslicing, an implementation trick frequently used by cryptographers.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {157–173},
numpages = {17},
keywords = {Optimizing Compiler, Vectorization, Bitslicing},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325982,
author = {Mercadier, Darius and Dagand, Pierre-\'{E}variste},
title = {Replication Package for Article: Usuba: High-Throughput &amp; Constant-Time Ciphers, by Construction},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325982},
abstract = {
    <p>This artifact builds a docker image, which, once built, will contain the necessary software to reproduce the results of the paper "Usuba: high-throughput &amp; constant-time ciphers, by construction". The docker image contains Usubac, the compiler for Usuba, and its sources, the Usuba sources of the ciphers presented if our paper, as well as the benchmarks evaluating our work.</p>
},
keywords = {Bitslicing, Block and stream ciphers, Dataflow languages, Domain specific languages, Optimizing Compiler, Vectorization}
}

@inproceedings{10.1145/3314221.3314605,
author = {Cauligi, Sunjay and Soeller, Gary and Johannesmeyer, Brian and Brown, Fraser and Wahby, Riad S. and Renner, John and Gr\'{e}goire, Benjamin and Barthe, Gilles and Jhala, Ranjit and Stefan, Deian},
title = {FaCT: A DSL for Timing-Sensitive Computation},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314605},
doi = {10.1145/3314221.3314605},
abstract = {Real-world cryptographic code is often written in a subset of C intended to execute in constant-time, thereby avoiding timing side channel vulnerabilities. This C subset eschews structured programming as we know it: if-statements, looping constructs, and procedural abstractions can leak timing information when handling sensitive data. The resulting obfuscation has led to subtle bugs, even in widely-used high-profile libraries like OpenSSL. To address the challenge of writing constant-time cryptographic code, we present FaCT, a crypto DSL that provides high-level but safe language constructs. The FaCT compiler uses a secrecy type system to automatically transform potentially timing-sensitive high-level code into low-level, constant-time LLVM bitcode. We develop the language and type system, formalize the constant-time transformation, and present an empirical evaluation that uses FaCT to implement core crypto routines from several open-source projects including OpenSSL, libsodium, and curve25519-donna. Our evaluation shows that FaCT’s design makes it possible to write readable, high-level cryptographic code, with efficient, constant-time behavior.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {174–189},
numpages = {16},
keywords = {cryptography, domain-specific language, program transformation},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325970,
author = {Cauligi, Sunjay and Soeller, Gary and Johannesmeyer, Brian and Brown, Fraser and Wahby, Riad S. and Renner, John and Gr\'{e}goire, Benjamin and Barthe, Gilles and Jhala, Ranjit and Stefan, Deian},
title = {Evaluation Artifact for FaCT: A DSL for Timing-Sensitive Computation},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325970},
abstract = {
    <p>We provide a virtual machine image that contains our compiler and our case studies. The image comes with instructions for building the compiler, building the case studies, and running the evaluation.</p>
},
keywords = {cryptography, domain-specific language, program transformation}
}

@inproceedings{10.1145/3314221.3314639,
author = {Smolka, Steffen and Kumar, Praveen and Kahn, David M. and Foster, Nate and Hsu, Justin and Kozen, Dexter and Silva, Alexandra},
title = {Scalable Verification of Probabilistic Networks},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314639},
doi = {10.1145/3314221.3314639},
abstract = {This paper presents McNetKAT, a scalable tool for verifying probabilistic network programs. McNetKAT is based on a new semantics for the guarded and history-free fragment of Probabilistic NetKAT in terms of finite-state, absorbing Markov chains. This view allows the semantics of all programs to be computed exactly, enabling construction of an automatic verification tool. Domain-specific optimizations and a parallelizing backend enable McNetKAT to analyze networks with thousands of nodes, automatically reasoning about general properties such as probabilistic program equivalence and refinement, as well as networking properties such as resilience to failures. We evaluate McNetKAT's scalability using real-world topologies, compare its performance against state-of-the-art tools, and develop an extended case study on a recently proposed data center network design.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {190–203},
numpages = {14},
keywords = {Probabilistic Programming, Network verification},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.5281/zenodo.2649613,
author = {Smolka, Steffen and Kumar, Praveen and Kahn, David M. and Foster, Nate and Hsu, Justin and Kozen, Dexter and Silva, Alexandra},
title = {McNetKAT: Scalable Verification of Probabilistic Networks},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.2649613},
abstract = {
    <p>This is the artifact associated with the following paper: Steffen Smolka, Praveen Kumar, David M. Kahn, Nate Foster, Justin Hsu, Dexter Kozen, and Alexandra Silva. 2019. Scalable Verification of Probabilistic Networks. In PLDI  19. https://doi.org/10.1145/3314221.3314639. Please refer to artifact-page/index.html for instruction on how to install McNetKAT and reproduce the experiments from the paper.</p>
},
keywords = {McNetKAT, Network verification, Probabilistic Programming}
}

@inproceedings{10.1145/3314221.3314581,
author = {Wang, Peixin and Fu, Hongfei and Goharshady, Amir Kafshdar and Chatterjee, Krishnendu and Qin, Xudong and Shi, Wenjun},
title = {Cost Analysis of Nondeterministic Probabilistic Programs},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314581},
doi = {10.1145/3314221.3314581},
abstract = {We consider the problem of expected cost analysis over nondeterministic probabilistic programs, which aims at automated methods for analyzing the resource-usage of such programs. Previous approaches for this problem could only handle nonnegative bounded costs. However, in many scenarios, such as queuing networks or analysis of cryptocurrency protocols, both positive and negative costs are necessary and the costs are unbounded as well.  In this work, we present a sound and efficient approach to obtain polynomial bounds on the expected accumulated cost of nondeterministic probabilistic programs. Our approach can handle (a) general positive and negative costs with bounded updates in variables; and (b) nonnegative costs with general updates to variables. We show that several natural examples which could not be handled by previous approaches are captured in our framework.  Moreover, our approach leads to an efficient polynomial-time algorithm, while no previous approach for cost analysis of probabilistic programs could guarantee polynomial runtime. Finally, we show the effectiveness of our approach using experimental results on a variety of programs for which we efficiently synthesize tight resource-usage bounds.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {204–220},
numpages = {17},
keywords = {Probabilistic Programs, Program Cost Analysis},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.5281/zenodo.2640455,
author = {Wang, Peixin and Fu, Hongfei and Goharshady, Amir Kafshdar and Chatterjee, Krishnendu and Qin, Xudong and Shi, Wenjun},
title = {Replication Package for Article: Cost Analysis of Nondeterministic Probabilistic Programs},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.2640455},
abstract = {
    <p>The artifact contains an implementation of our approach in Matlab. Please see the readme file.</p>
},
keywords = {Probabilistic Programs, Program Cost Analysis}
}

@inproceedings{10.1145/3314221.3314642,
author = {Cusumano-Towner, Marco F. and Saad, Feras A. and Lew, Alexander K. and Mansinghka, Vikash K.},
title = {Gen: A General-Purpose Probabilistic Programming System with Programmable Inference},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314642},
doi = {10.1145/3314221.3314642},
abstract = {Although probabilistic programming is widely used for some restricted classes of statistical models, existing systems lack the flexibility and efficiency needed for practical use with more challenging models arising in fields like computer vision and robotics. This paper introduces Gen, a general-purpose probabilistic programming system that achieves modeling flexibility and inference efficiency via several novel language constructs: (i) the generative function interface for encapsulating probabilistic models; (ii) interoperable modeling languages that strike different flexibility/efficiency trade-offs; (iii) combinators that exploit common patterns of conditional independence; and (iv) an inference library that empowers users to implement efficient inference algorithms at a high level of abstraction. We show that Gen outperforms state-of-the-art probabilistic programming systems, sometimes by multiple orders of magnitude, on diverse problems including object tracking, estimating 3D body pose from a depth image, and inferring the structure of a time series.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {221–236},
numpages = {16},
keywords = {variational inference, Markov chain Monte Carlo, sequential Monte Carlo, Probabilistic programming},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314623,
author = {Zhang, Jieyuan and Xue, Jingling},
title = {Incremental Precision-Preserving Symbolic Inference for Probabilistic Programs},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314623},
doi = {10.1145/3314221.3314623},
abstract = {We present ISymb an incremental symbolic inference framework for probabilistic programs in situations when some loop-manipulated array data, upon which their probabilistic models are conditioned, undergoes small changes. To tackle the path explosion challenge, ISymb is intra-procedurally path-sensitive except that it conducts a “meet-over-all-paths” analysis within an iteration of a loop (conditioned on some observed array data). By recomputing only the probability distributions for the paths affected, ISymb avoids expensive symbolic inference from scratch while also being precision-preserving. Our evaluation with a set of existing benchmarks shows that ISymb can lead to orders of magnitude performance improvements compared to its non-incremental counterpart (under small changes in observed array data).},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {237–252},
numpages = {16},
keywords = {Incremental Computation, Symbolic Inference, Static Analysis, Probabilistic Programming},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314602,
author = {Knoth, Tristan and Wang, Di and Polikarpova, Nadia and Hoffmann, Jan},
title = {Resource-Guided Program Synthesis},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314602},
doi = {10.1145/3314221.3314602},
abstract = {This article presents resource-guided synthesis, a technique for synthesizing recursive programs that satisfy both a functional specification and a symbolic resource bound. The technique is type-directed and rests upon a novel type system that combines polymorphic refinement types with potential annotations of automatic amortized resource analysis. The type system enables efficient constraint-based type checking and can express precise refinement-based resource bounds. The proof of type soundness shows that synthesized programs are correct by construction. By tightly integrating program exploration and type checking, the synthesizer can leverage the user-provided resource bound to guide the search, eagerly rejecting incomplete programs that consume too many resources. An implementation in the resource-guided synthesizer ReSyn is used to evaluate the technique on a range of recursive data structure manipulations. The experiments show that ReSyn synthesizes programs that are asymptotically more efficient than those generated by a resource-agnostic synthesizer. Moreover, synthesis with ReSyn is faster than a naive combination of synthesis and resource analysis. ReSyn is also able to generate implementations that have a constant resource consumption for fixed input sizes, which can be used to mitigate side-channel attacks.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {253–268},
numpages = {16},
keywords = {Refinement Types, Automated Amortized Resource Analysis, Program Synthesis},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325977,
author = {Knoth, Tristan and Wang, Di and Polikarpova, Nadia and Hoffmann, Jan},
title = {Replication Package for: Resource-Guided Program Synthesis},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325977},
abstract = {
    <p>Included is the source for resyn, a program synthesizer that generates functional programs from refinement-type signatures. Resyn also takes an upper bound on the codes' resource usage; using a novel resource type system to guide the search process.</p>
},
keywords = {Automatic amortized resource analysis, Program synthesis, refinement types.}
}

@inproceedings{10.1145/3314221.3314591,
author = {Shen, Jiasi and Rinard, Martin C.},
title = {Using Active Learning to Synthesize Models of Applications That Access Databases},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314591},
doi = {10.1145/3314221.3314591},
abstract = {We present Konure, a new system that uses active learning to infer models of applications that access relational databases. Konure comprises a domain-specific language (each model is a program in this language) and associated inference algorithm that infers models of applications whose behavior can be expressed in this language. The inference algorithm generates inputs and database configurations, runs the application, then observes the resulting database traffic and outputs to progressively refine its current model hypothesis. Because the technique works with only externally observable inputs, outputs, and database configurations, it can infer the behavior of applications written in arbitrary languages using arbitrary coding styles (as long as the behavior of the application is expressible in the domain-specific language). Konure also implements a regenerator that produces a translated Python implementation of the application that systematically includes relevant security and error checks.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {269–285},
numpages = {17},
keywords = {Active learning, Program regeneration, Program inference},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314588,
author = {Wang, Yuepeng and Dong, James and Shah, Rushi and Dillig, Isil},
title = {Synthesizing Database Programs for Schema Refactoring},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314588},
doi = {10.1145/3314221.3314588},
abstract = {Many programs that interact with a database need to undergo schema refactoring several times during their life cycle. Since this process typically requires making significant changes to the program's implementation, schema refactoring is often non-trivial and error-prone. Motivated by this problem, we propose a new technique for automatically synthesizing a new version of a database program given its original version and the source and target schemas. Our method does not require manual user guidance and ensures that the synthesized program is equivalent to the original one. Furthermore, our method is quite efficient and can synthesize new versions of database programs (containing up to 263 functions) that are extracted from real-world web applications with an average synthesis time of 69.4 seconds.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {286–300},
numpages = {15},
keywords = {Relational Databases, Program Sketching, Program Synthesis},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3322485,
author = {Iyer, Arun and Jonnalagedda, Manohar and Parthasarathy, Suresh and Radhakrishna, Arjun and Rajamani, Sriram K.},
title = {Synthesis and Machine Learning for Heterogeneous Extraction},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3322485},
doi = {10.1145/3314221.3322485},
abstract = {We present a way to combine techniques from the program synthesis and machine learning communities to extract structured information from heterogeneous data. Such problems arise in several situations such as extracting attributes from web pages, machine-generated emails, or from data obtained from multiple sources. Our goal is to extract a set of structured attributes from such data.  We use machine learning models ("ML models") such as conditional random fields to get an initial labeling of potential attribute values. However, such models are typically not interpretable, and the noise produced by such models is hard to manage or debug. We use (noisy) labels produced by such ML models as inputs to program synthesis, and generate interpretable programs that cover the input space. We also employ type specifications (called "field constraints") to certify well-formedness of extracted values. Using synthesized programs and field constraints, we re-train the ML models with improved confidence on the labels. We then use these improved labels to re-synthesize a better set of programs. We iterate the process of re-synthesizing the programs and re-training the ML models, and find that such an iterative process improves the quality of the extraction process. This iterative approach, called HDEF, is novel, not only the in way it combines the ML models with program synthesis, but also in the way it adapts program synthesis to deal with noise and heterogeneity.  More broadly, our approach points to ways by which machine learning and programming language techniques can be combined to get the best of both worlds --- handling noise, transferring signals from one context to another using ML, producing interpretable programs using PL, and minimizing user intervention.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {301–315},
numpages = {15},
keywords = {Heterogeneous data, Machine Learning, Data extraction, Program synthesis},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314608,
author = {Shull, Thomas and Huang, Jian and Torrellas, Josep},
title = {AutoPersist: An Easy-to-Use Java NVM Framework Based on Reachability},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314608},
doi = {10.1145/3314221.3314608},
abstract = {Byte-addressable, non-volatile memory (NVM) is emerging as a revolutionary memory technology that provides persistency, near-DRAM performance, and scalable capacity. To facilitate its use, many NVM programming models have been proposed. However, most models require programmers to explicitly specify the data structures or objects that should reside in NVM. Such requirement increases the burden on programmers, complicates software development, and introduces opportunities for correctness and performance bugs.  We believe that requiring programmers to identify the data structures that should reside in NVM is untenable. Instead, programmers should only be required to identify durable roots - the entry points to the persistent data structures at recovery time. The NVM programming framework should then automatically ensure that all the data structures reachable from these roots are in NVM, and stores to these data structures are persistently completed in an intuitive order.  To this end, we present a new NVM programming framework, named AutoPersist, that only requires programmers to identify durable roots. AutoPersist then persists all the data structures that can be reached from the durable roots in an automated and transparent manner. We implement AutoPersist as a thread-safe extension to the Java language and perform experiments with a variety of applications running on Intel Optane DC persistent memory. We demonstrate that AutoPersist requires minimal code modifications, and significantly outperforms expert-marked Java NVM applications.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {316–332},
numpages = {17},
keywords = {JIT Compilation, Java, Non-Volatile Memory},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314582,
author = {Powers, Bobby and Tench, David and Berger, Emery D. and McGregor, Andrew},
title = {Mesh: Compacting Memory Management for C/C++ Applications},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314582},
doi = {10.1145/3314221.3314582},
abstract = {Programs written in C/C++ can suffer from serious memory fragmentation, leading to low utilization of memory, degraded performance, and application failure due to memory exhaustion. This paper introduces Mesh, a plug-in replacement for malloc that, for the first time, eliminates fragmentation in unmodified C/C++ applications. Mesh combines novel randomized algorithms with widely-supported virtual memory operations to provably reduce fragmentation, breaking the classical Robson bounds with high probability. Mesh generally matches the runtime performance of state-of-the-art memory allocators while reducing memory consumption; in particular, it reduces the memory of consumption of Firefox by 16% and Redis by 39%.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {333–346},
numpages = {14},
keywords = {Memory management, unmanaged languages, runtime systems},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325987,
author = {Powers, Bobby and Tench, David and Berger, Emery D. and McGregor, Andrew},
title = {Replication Package For Article: Mesh: Compacting Memory Management for C/C++ Applications},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325987},
abstract = {
    <p>The provided Linux virtual machine (in VirtualBox OVA format) can be used to verify the claims made in the Mesh paper. The VM boots up into an Ubuntu desktop with an open terminal and instructions on running the artifact.</p>
},
keywords = {Memory management, runtime systems, unmanaged languages}
}

@inproceedings{10.1145/3314221.3314650,
author = {Wang, Chenxi and Cui, Huimin and Cao, Ting and Zigman, John and Volos, Haris and Mutlu, Onur and Lv, Fang and Feng, Xiaobing and Xu, Guoqing Harry},
title = {Panthera: Holistic Memory Management for Big Data Processing over Hybrid Memories},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314650},
doi = {10.1145/3314221.3314650},
abstract = {Modern data-parallel systems such as Spark rely increasingly on in-memory computing that can significantly improve the efficiency of iterative algorithms. To process real-world datasets, modern data-parallel systems often require extremely large amounts of memory, which are both costly and energy-inefficient. Emerging non-volatile memory (NVM) technologies offers high capacity compared to DRAM and low energy compared to SSDs. Hence, NVMs have the potential to fundamentally change the dichotomy between DRAM and durable storage in Big Data processing. However, most Big Data applications are written in managed languages (e.g., Scala and Java) and executed on top of a managed runtime (e.g., the Java Virtual Machine) that already performs various dimensions of memory management. Supporting hybrid physical memories adds in a new dimension, creating unique challenges in data replacement and migration.  This paper proposes Panthera, a semantics-aware, fully automated memory management technique for Big Data processing over hybrid memories. Panthera analyzes user programs on a Big Data system to infer their coarse-grained access patterns, which are then passed down to the Panthera runtime for efficient data placement and migration. For Big Data applications, the coarse-grained data division is accurate enough to guide GC for data layout, which hardly incurs data monitoring and moving overhead. We have implemented Panthera in OpenJDK and Apache Spark. An extensive evaluation with various datasets and applications demonstrates that Panthera reduces energy by 32 – 52% at only a 1 – 9% execution time overhead.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {347–362},
numpages = {16},
keywords = {Big Data systems, hybrid memories, memory management, garbage collection},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314589,
author = {van Tonder, Rijnard and Le Goues, Claire},
title = {Lightweight Multi-Language Syntax Transformation with Parser Parser Combinators},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314589},
doi = {10.1145/3314221.3314589},
abstract = {Automatically transforming programs is hard, yet critical for automated program refactoring, rewriting, and repair. Multi-language syntax transformation is especially hard due to heterogeneous representations in syntax, parse trees, and abstract syntax trees (ASTs). Our insight is that the problem can be decomposed such that (1) a common grammar expresses the central context-free language (CFL) properties shared by many contemporary languages and (2) open extension points in the grammar allow customizing syntax (e.g., for balanced delimiters) and hooks in smaller parsers to handle language-specific syntax (e.g., for comments). Our key contribution operationalizes this decomposition using a Parser Parser combinator (PPC), a mechanism that generates parsers for matching syntactic fragments in source code by parsing declarative user-supplied templates. This allows our approach to detach from translating input programs to any particular abstract syntax tree representation, and lifts syntax rewriting to a modularly-defined parsing problem. A notable effect is that we skirt the complexity and burden of defining additional translation layers between concrete user input templates and an underlying abstract syntax representation. We demonstrate that these ideas admit efficient and declarative rewrite templates across 12 languages, and validate effectiveness of our approach by producing correct and desirable lightweight transformations on popular real-world projects (over 50 syntactic changes produced by our approach have been merged into 40+). Our declarative rewrite patterns require an order of magnitude less code compared to analog implementations in existing, language-specific tools.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {363–378},
numpages = {16},
keywords = {transformation, parsers, syntax, rewriting},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.5281/zenodo.2642857,
author = {van Tonder, Rijnard and Le Goues, Claire},
title = {Lightweight Multi-Language Syntax Transformation with Parser Parser Combinators: PLDI 2019 Artifact},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.2642857},
abstract = {
    <p>The PLDI 2019 VM artifact is archived for the associated paper and it's emphasis is to produce results consistent with those in the paper. It includes a research-grade implementation, associated transformations, and repository data to reproduce the tables in the paper. For those interested in actively trying or using the out the tool, consider the newer, actively maintained variety of this software at https://github.com/comby-tools/comby.</p>
},
keywords = {parsers, refactoring, rewriting, syntax, transformation}
}

@inproceedings{10.1145/3314221.3314625,
author = {Krishnaswami, Neelakantan R. and Yallop, Jeremy},
title = {A Typed, Algebraic Approach to Parsing},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314625},
doi = {10.1145/3314221.3314625},
abstract = {In this paper, we recall the definition of the context-free expressions (or µ-regular expressions), an algebraic presentation of the context-free languages. Then, we define a core type system for the context-free expressions which gives a compositional criterion for identifying those context-free expressions which can be parsed unambiguously by predictive algorithms in the style of recursive descent or LL(1). Next, we show how these typed grammar expressions can be used to derive a parser combinator library which both guarantees linear-time parsing with no backtracking and single-token lookahead, and which respects the natural denotational semantics of context-free expressions. Finally, we show how to exploit the type information to write a staged version of this library, which produces dramatic increases in performance, even outperforming code generated by the standard parser generator tool ocamlyacc.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {379–393},
numpages = {15},
keywords = {Kleene algebra, type theory, parsing, context-free languages},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325972,
author = {Krishnaswami, Neelakantan R. and Yallop, Jeremy},
title = {Replication Package for Paper: A Typed, Algebraic Approach to Parsing},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325972},
abstract = {
    <p>The artifact is a Docker image that makes it possible to explore the library described in the paper and re-run the performance experiments to reproduce the results.</p>
},
keywords = {context-free languages, functional programming, multi-stage programming, parser combinators, parsing, type theory}
}

@inproceedings{10.1145/3314221.3314594,
author = {Campagna, Giovanni and Xu, Silei and Moradshahi, Mehrad and Socher, Richard and Lam, Monica S.},
title = {Genie: A Generator of Natural Language Semantic Parsers for Virtual Assistant Commands},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314594},
doi = {10.1145/3314221.3314594},
abstract = {To understand diverse natural language commands, virtual assistants today are trained with numerous labor-intensive, manually annotated sentences. This paper presents a methodology and the Genie toolkit that can handle new compound commands with significantly less manual effort. We advocate formalizing the capability of virtual assistants with a Virtual Assistant Programming Language (VAPL) and using a neural semantic parser to translate natural language into VAPL code. Genie needs only a small realistic set of input sentences for validating the neural model. Developers write templates to synthesize data; Genie uses crowdsourced paraphrases and data augmentation, along with the synthesized data, to train a semantic parser. We also propose design principles that make VAPL languages amenable to natural language translation. We apply these principles to revise ThingTalk, the language used by the Almond virtual assistant. We use Genie to build the first semantic parser that can support compound virtual assistants commands with unquoted free-form parameters. Genie achieves a 62% accuracy on realistic user inputs. We demonstrate Genie’s generality by showing a 19% and 31% improvement over the previous state of the art on a music skill, aggregate functions, and access control.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {394–410},
numpages = {17},
keywords = {data engineering, virtual assistants, training data generation, semantic parsing, data augmentation},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@dataset{10.1145/3325975,
author = {Campagna, Giovanni and Xu, Silei and Moradshahi, Mehrad and Socher, Richard and Lam, Monica S.},
title = {Public Dataset for "Genie: A Generator of Natural Language Semantic Parsers for Virtual Assistant Commands"},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325975},
abstract = {
    <p>The artifact contains the benchmark dataset associated with the paper "Genie: A Generator of Natural Language Semantic Parsers for Virtual Assistant Commands", published in PLDI 2019. Documentation is included in the README file.</p>
},
keywords = {natural language processing, open datasets, semantic parsing, virtual assistants}
}

@inproceedings{10.1145/3314221.3314618,
author = {Hallahan, William T. and Xue, Anton and Bland, Maxwell Troy and Jhala, Ranjit and Piskac, Ruzica},
title = {Lazy Counterfactual Symbolic Execution},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314618},
doi = {10.1145/3314221.3314618},
abstract = {We present counterfactual symbolic execution, a new approach that produces counterexamples that localize the causes of failure of static verification. First, we develop a notion of symbolic weak head normal form and use it to define lazy symbolic execution reduction rules for non-strict languages like Haskell. Second, we introduce counterfactual branching, a new method to identify places where verification fails due to imprecise specifications (as opposed to incorrect code). Third, we show how to use counterfactual symbolic execution to localize refinement type errors, by translating refinement types into assertions. We implement our approach in a new Haskell symbolic execution engine, G2, and evaluate it on a corpus of 7550 errors gathered from users of the LiquidHaskell refinement type system. We show that for 97.7% of these errors, G2 is able to quickly find counterexamples that show how the code or specifications must be fixed to enable verification.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {411–424},
numpages = {14},
keywords = {counterfactual, lazy, Haskell, symbolic execution, counterexamples},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325980,
author = {Hallahan, William T. and Xue, Anton and Bland, Maxwell Troy and Jhala, Ranjit and Piskac, Ruzica},
title = {G2: Lazy Counterfactual Symbolic Execution},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325980},
abstract = {
    <p>An artifact, consisting of code and benchmarks, for Lazy Counterfactual Symbolic Execution, by William T. Hallahan, Anton Xue, Maxwell Troy Bland, Ranjit Jhala, and Ruzica Piskac.</p>
},
keywords = {counterexamples, counterfactual, Haskell, lazy, symbolic execution}
}

@inproceedings{10.1145/3314221.3314645,
author = {Loring, Blake and Mitchell, Duncan and Kinder, Johannes},
title = {Sound Regular Expression Semantics for Dynamic Symbolic Execution of JavaScript},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314645},
doi = {10.1145/3314221.3314645},
abstract = {Support for regular expressions in symbolic execution-based tools for test generation and bug finding is insufficient. Common aspects of mainstream regular expression engines, such as backreferences or greedy matching, are ignored or imprecisely approximated, leading to poor test coverage or missed bugs. In this paper, we present a model for the complete regular expression language of ECMAScript 2015 (ES6), which is sound for dynamic symbolic execution of the test and exec functions. We model regular expression operations using string constraints and classical regular expressions and use a refinement scheme to address the problem of matching precedence and greediness. We implemented our model in ExpoSE, a dynamic symbolic execution engine for JavaScript, and evaluated it on over 1,000 Node.js packages containing regular expressions, demonstrating that the strategy is effective and can significantly increase the number of successful regular expression queries and therefore boost coverage.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {425–438},
numpages = {14},
keywords = {JavaScript, regular expressions, Dynamic symbolic execution, SMT},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325985,
author = {Loring, Blake and Mitchell, Duncan and Kinder, Johannes},
title = {Artifact for PLDI 2019 Paper: Sound Regular Expression Semantics for Dynamic Symbolic Execution of JavaScript},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325985},
abstract = {
    <p>The central contribution of this paper is a modification to the ExpoSE symbolic execution engine adding support for complex JavaScript regular expressions. The support for these regular expressions will often improve dynamic symbolic execution of JavaScript applications as they are widely used. A core part of this support is the introduction of a CEGAR loop on top of the SMT solver that automatically refines SMT problems to account for incorrect matching precedence. This artifact submission accompanies our PLDI 2019 paper. Our artifact is a virtual machine image with the paper software and experiments pre-installed.</p>
},
keywords = {Dynamic symbolic execution, JavaScript, regular expressions, SMT}
}

@inproceedings{10.1145/3314221.3314632,
author = {Fu, Zhoulai and Su, Zhendong},
title = {Effective Floating-Point Analysis via Weak-Distance Minimization},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314632},
doi = {10.1145/3314221.3314632},
abstract = {This work studies the connection between the problem of analyzing floating-point code and that of function minimization. It formalizes this connection as a reduction theory, where the semantics of a floating-point program is measured as a generalized metric, called weak distance, which faithfully captures any given analysis objective. It is theoretically guaranteed that minimizing the weak distance (e.g., via mathematical optimization) solves the underlying problem. This reduction theory provides a general framework for analyzing numerical code. Two important separate analyses from the literature, branch-coverage-based testing and quantifier-free floating-point satisfiability, are its instances.  To further demonstrate our reduction theory’s generality and power, we develop three additional applications, including boundary value analysis, path reachability and overflow detection. Critically, these analyses do not rely on the modeling or abstraction of floating-point semantics; rather, they explore a program’s input space guided by runtime computation and minimization of the weak distance. This design, combined with the aforementioned theoretical guarantee, enables the application of the reduction theory to real-world floating-point code. In our experiments, our boundary value analysis is able to find all reachable boundary conditions of the GNU sin function, which is complex with several hundred lines of code, and our floating-point overflow detection detects a range of overflows and inconsistencies in the widely-used numerical library GSL, including two latent bugs that developers have already confirmed.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {439–452},
numpages = {14},
keywords = {Theoretical Guarantee, Mathematical Optimization, Program Analysis, Floating-point Code},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314644,
author = {Khan, Tanvir Ahmed and Zhao, Yifan and Pokam, Gilles and Mozafari, Barzan and Kasikci, Baris},
title = {Huron: Hybrid False Sharing Detection and Repair},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314644},
doi = {10.1145/3314221.3314644},
abstract = {Writing efficient multithreaded code that can leverage the full parallelism of underlying hardware is difficult. A key impediment is insidious cache contention issues, such as false sharing. False sharing occurs when multiple threads from different cores access disjoint portions of the same cache line, causing it to go back and forth between the caches of different cores and leading to substantial slowdown.  Alas, existing techniques for detecting and repairing false sharing have limitations. On the one hand, in-house (i.e., offline) techniques are limited to situations where falsely-shared data can be determined statically, and are otherwise inaccurate. On the other hand, in-production (i.e., run-time) techniques incur considerable overhead, as they constantly monitor a program to detect false sharing. In-production repair techniques are also limited by the types of modifications they can perform on the fly, and are therefore less effective.  We present Huron, a hybrid in-house/in-production false sharing detection and repair system. Huron detects and repairs as much false sharing as it can in-house, and relies on its lightweight in-production mechanism for remaining cases. The key idea behind Huron's in-house false sharing repair is to group together data that is accessed by the same set of threads, to shift falsely-shared data to different cache lines. Huron's in-house repair technique can generalize to previously-unobserved inputs. Our evaluation shows that Huron can detect more false sharing bugs than all state-of-the-art techniques, and with a lower overhead. Huron improves runtime performance by 3.82\texttimes{} on average (up to 11\texttimes{}), which is 2.11-2.27\texttimes{} better than the state of the art.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {453–468},
numpages = {16},
keywords = {Performance optimization, False sharing},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325966,
author = {Khan, Tanvir Ahmed and Zhao, Yifan and Pokam, Gilles and Mozafari, Barzan and Kasikci, Baris},
title = {Software Artifact for Huron: Hybrid False Sharing Detection and Repair},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325966},
abstract = {
    <p>The artifact consists of a VirtualBox virtual machine disk (VMDK file) containing the Huron software and source code, evaluation data, instructions, and all open source evaluation applications and input files.</p>
},
keywords = {False sharing, Performance optimization}
}

@inproceedings{10.1145/3314221.3314653,
author = {Kong, Martin and Pouchet, Louis-No\"{e}l},
title = {Model-Driven Transformations for Multi- and Many-Core CPUs},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314653},
doi = {10.1145/3314221.3314653},
abstract = {Modern polyhedral compilers excel at aggressively optimizing codes with static control parts, but the state-of-practice to find high-performance polyhedral transformations especially for different hardware targets still largely involves auto-tuning. In this work we propose a novel customizable polyhedral scheduling technique, with the aim of delivering high performance for several hardware targets. We design constraints and objectives that model several crucial aspects of performance such as stride optimization or the trade-off between parallelism and reuse, while considering important architectural features of the target machine. We evaluate our work using the PolyBench/C benchmark suite and experimentally validate it against large optimization spaces generated with the Pluto compiler on 3 representative architectures: an IBM Power9, an Intel Xeon Phi and an Intel Core-i9. Our results show we can achieve comparable or superior performance to Pluto on the majority of benchmarks, without implementing tiling in the source code nor using experimental autotuning.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {469–484},
numpages = {16},
keywords = {Affine transformations, single-shot ILP, polyhedral optimizations, scheduling, composable transformations},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314621,
author = {Yoga, Adarsh and Nagarakatte, Santosh},
title = {Parallelism-Centric What-If and Differential Analyses},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314621},
doi = {10.1145/3314221.3314621},
abstract = {This paper proposes TaskProf2, a parallelism profiler and an adviser for task parallel programs. As a parallelism profiler, TaskProf2 pinpoints regions with serialization bottlenecks, scheduling overheads, and secondary effects of execution. As an adviser, TaskProf2 identifies regions that matter in improving parallelism. To accomplish these objectives, it uses a performance model that captures series-parallel relationships between various dynamic execution fragments of tasks and includes fine-grained measurement of computation in those fragments. Using this performance model, TaskProf2’s what-if analyses identify regions that improve the parallelism of the program while considering tasking overheads. Its differential analyses perform fine-grained differencing of an oracle and the observed performance model to identify static regions experiencing secondary effects. We have used TaskProf2 to identify regions with serialization bottlenecks and secondary effects in many applications.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {485–501},
numpages = {17},
keywords = {Profilers, What-if analyses, Parallelism},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325965,
author = {Yoga, Adarsh and Nagarakatte, Santosh},
title = {TaskProf2: A Parallelism Profiler and an Adviser for Task Parallel Programs.},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325965},
abstract = {
    <p>TaskProf2 is a parallelism profiler and an adviser for task parallel programs that use the Intel Threading Building Blocks (TBB). As a parallelism profiler, it identifies regions with serialization bottlenecks, tasking overheads, and the secondary effects of execution. As an adviser, it automatically identifies a set of code regions that matter in improving parallelism with its what-if analyses.</p>
},
keywords = {differential analysis, parallelism, profiler, what-if analyses}
}

@inproceedings{10.1145/3314221.3322484,
author = {Scalas, Alceste and Yoshida, Nobuko and Benussi, Elias},
title = {Verifying Message-Passing Programs with Dependent Behavioural Types},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3322484},
doi = {10.1145/3314221.3322484},
abstract = {Concurrent and distributed programming is notoriously hard. Modern languages and toolkits ease this difficulty by offering message-passing abstractions, such as actors (e.g., Erlang, Akka, Orleans) or processes (e.g., Go): they allow for simpler reasoning w.r.t. shared-memory concurrency, but do not ensure that a program implements a given specification. To address this challenge, it would be desirable to specify and verify the intended behaviour of message-passing applications using types, and ensure that, if a program type-checks and compiles, then it will run and communicate as desired. We develop this idea in theory and practice. We formalise a concurrent functional language λ≤π, with a new blend of behavioural types (from π-calculus theory), and dependent function types (from the Dotty programming language, a.k.a. the future Scala 3). Our theory yields four main payoffs: (1) it verifies safety and liveness properties of programs via type-level model checking; (2) unlike previous work, it accurately verifies channel-passing (covering a typical pattern of actor programs) and higher-order interaction (i.e., sending/receiving mobile code); (3) it is directly embedded in Dotty, as a toolkit called Effpi, offering a simplified actor-based API; (4) it enables an efficient runtime system for Effpi, for highly concurrent programs with millions of processes/actors.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {502–516},
numpages = {15},
keywords = {temporal logic, model checking, actors, behavioural types, Scala, processes, Dotty, dependent types},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325968,
author = {Scalas, Alceste and Yoshida, Nobuko and Benussi, Elias},
title = {Effpi: A Toolkit for Verified Message-Passing Programs in Dotty},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325968},
abstract = {
    <p>This artifact contains the source code of Effpi: a toolkit for strongly-typed concurrent programming in Dotty (a.k.a. the future Scala 3 programming language), with model-checking-based verification capabilities. The toolkit is described in sections 1 and 5 of the companion paper: Alceste Scalas, Nobuko Yoshida, and Elias Benussi. Verified Message-Passing Programs with Dependent Behavioural Types. PLDI 2019. https://doi.org/10.1145/3314221.3322484 This artifact contains instruction for reproducing the benchmarks in the companion paper: see README.txt It also contains a ready-to-use virtual machine (based on a minimal Ubuntu installation) with Effpi's software dependencies. For the latest version of Effpi, visit: https://alcestes.github.io/effpi</p>
},
keywords = {actors, behavioural types, dependent types, Dotty, model checking, processes, Scala, temporal logic}
}

@inproceedings{10.1145/3314221.3314627,
author = {Kuhlenschmidt, Andre and Almahallawi, Deyaaeldeen and Siek, Jeremy G.},
title = {Toward Efficient Gradual Typing for Structural Types via Coercions},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314627},
doi = {10.1145/3314221.3314627},
abstract = {Gradual typing combines static and dynamic typing in the same program. Siek et al. (2015) describe five criteria for gradually typed languages, including type soundness and the gradual guarantee. A significant number of languages have been developed in academia and industry that support some of these criteria (TypeScript, Typed Racket, Safe TypeScript, Transient Reticulated Python, Thorn, etc.) but relatively few support all the criteria (Nom, Gradualtalk, Guarded Reticulated Python). Of those that do, only Nom does so efficiently. The Nom experiment shows that one can achieve efficient gradual typing in languages with only nominal types, but many languages have structural types: function types, tuples, record and object types, generics, etc.  In this paper we present a compiler, named Grift, that addresses the difficult challenge of efficient gradual typing for structural types. The input language includes a selection of difficult features: first-class functions, mutable arrays, and recursive types. We show that a close-to-the-metal implementation of run-time casts inspired by Henglein's coercions eliminates all of the catastrophic slowdowns without introducing significant average-case overhead. As a result, Grift exhibits lower overheads than those of Typed Racket.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {517–532},
numpages = {16},
keywords = {compilation, efficiency, gradual typing},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325989,
author = {Kuhlenschmidt, Andre and Almahallawi, Deyaaeldeen and Siek, Jeremy G.},
title = {Benchmarks Replication for Toward Efficient Gradual Typing for Structural Types via Coercions},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325989},
abstract = {
    <p>We setup the benchmarks that we performed for our paper in a virtual machine. This virtual machine provides documentation of our benchmarking setup and allows reviewers to spot check the results (expect some variation due to the overheads of virtualization). If more accurate results are required in the future, the benchmark suite is setup as a docker image to ease reproduction.</p>
},
keywords = {compilation, efficiency, gradual typing}
}

@inproceedings{10.1145/3314221.3314603,
author = {\c{C}i\c{c}ek, Ezgi and Qu, Weihao and Barthe, Gilles and Gaboardi, Marco and Garg, Deepak},
title = {Bidirectional Type Checking for Relational Properties},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314603},
doi = {10.1145/3314221.3314603},
abstract = {Relational type systems have been designed for several applications including information flow, differential privacy, and cost analysis. In order to achieve the best results, these systems often use relational refinements and relational effects to maximally exploit the similarity in the structure of the two programs being compared. Relational type systems are appealing for relational properties because they deliver simpler and more precise verification than what could be derived from typing the two programs separately. However, relational type systems do not yet achieve the practical appeal of their non-relational counterpart, in part because of the lack of a general foundation for implementing them.  In this paper, we take a step in this direction by developing bidirectional relational type checking for systems with relational refinements and effects. Our approach achieves the benefits of bidirectional type checking, in a relational setting. In particular, it significantly reduces the need for typing annotations through the combination of type checking and type inference. In order to highlight the foundational nature of our approach, we develop bidirectional versions of several relational type systems which incrementally combine many different components needed for expressive relational analysis.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {533–547},
numpages = {15},
keywords = {type-and-effect systems, relational type systems, refinement types, Bidirectional type-checking},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314651,
author = {Mathis, Bj\"{o}rn and Gopinath, Rahul and Mera, Micha\"{e}l and Kampmann, Alexander and H\"{o}schele, Matthias and Zeller, Andreas},
title = {Parser-Directed Fuzzing},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314651},
doi = {10.1145/3314221.3314651},
abstract = {To be effective, software test generation needs to well cover the space of possible inputs. Traditional fuzzing generates large numbers of random inputs, which however are unlikely to contain keywords and other specific inputs of non-trivial input languages. Constraint-based test generation solves conditions of paths leading to uncovered code, but fails on programs with complex input conditions because of path explosion. In this paper, we present a test generation technique specifically directed at input parsers. We systematically produce inputs for the parser and track comparisons made; after every rejection, we satisfy the comparisons leading to rejection. This approach effectively covers the input space: Evaluated on five subjects, from CSV files to JavaScript, our pFuzzer prototype covers more tokens than both random-based and constraint-based approaches, while requiring no symbolic analysis and far fewer tests than random fuzzers.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {548–560},
numpages = {13},
keywords = {security, parsers, fuzzing, test generation},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314616,
author = {Heo, Kihong and Raghothaman, Mukund and Si, Xujie and Naik, Mayur},
title = {Continuously Reasoning about Programs Using Differential Bayesian Inference},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314616},
doi = {10.1145/3314221.3314616},
abstract = {Programs often evolve by continuously integrating changes from multiple programmers. The effective adoption of program analysis tools in this continuous integration setting is hindered by the need to only report alarms relevant to a particular program change. We present a probabilistic framework, Drake, to apply program analyses to continuously evolving programs. Drake is applicable to a broad range of analyses that are based on deductive reasoning. The key insight underlying Drake is to compute a graph that concisely and precisely captures differences between the derivations of alarms produced by the given analysis on the program before and after the change. Performing Bayesian inference on the graph thereby enables to rank alarms by likelihood of relevance to the change. We evaluate Drake using Sparrow—a static analyzer that targets buffer-overrun, format-string, and integer-overflow errors—on a suite of ten widely-used C programs each comprising 13k–112k lines of code. Drake enables to discover all true bugs by inspecting only 30 alarms per benchmark on average, compared to 85 (3\texttimes{} more) alarms by the same ranking approach in batch mode, and 118 (4\texttimes{} more) alarms by a differential approach based on syntactic masking of alarms which also misses 4 of the 26 bugs overall.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {561–575},
numpages = {15},
keywords = {software evolution, continuous integration, Static analysis, alarm relevance, alarm prioritization},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325971,
author = {Heo, Kihong and Raghothaman, Mukund and Si, Xujie and Naik, Mayur},
title = {Replication Package for Article: Continuously Reasoning about Programs Using Differential Bayesian Inference},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325971},
abstract = {
    <p>This directory contains the artifact of the paper "Continuously Reasoning about Programs using Differential Bayesian Inference" published in PLDI 2019.</p>
},
keywords = {alarm prioritization, alarm relevance, continuous integration, software evolution, Static analysis}
}

@inproceedings{10.1145/3314221.3314635,
author = {Lidbury, Christopher and Donaldson, Alastair F.},
title = {Sparse Record and Replay with Controlled Scheduling},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314635},
doi = {10.1145/3314221.3314635},
abstract = {Modern applications include many sources of nondeterminism, e.g. due to concurrency, signals, and system calls that interact with the external environment. Finding and reproducing bugs in the presence of this nondeterminism has been the subject of much prior work in three main areas: (1) controlled concurrency-testing, where a custom scheduler replaces the OS scheduler to find subtle bugs; (2) record and replay, where sources of nondeterminism are captured and logged so that a failing execution can be replayed for debugging purposes; and (3) dynamic analysis for the detection of data races. We present a dynamic analysis tool for C++ applications, tsan11rec, which brings these strands of work together by integrating controlled concurrency testing and record and replay into the tsan11 framework for C++11 data race detection. Our novel twist on record and replay is a sparse approach, where the sources of nondeterminism to record can be configured per application. We show that our approach is effective at finding subtle concurrency bugs in small applications; is competitive in terms of performance with the state-of-the-art record and replay tool rr on larger applications; succeeds (due to our sparse approach) in replaying the I/O-intensive Zandronum and QuakeSpasm video games, which are out of scope for rr; but (due to limitations of our sparse approach) cannot faithfully replay applications where memory layout nondeterminism significantly affects application behaviour.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {576–593},
numpages = {18},
keywords = {concurrency, controlled concurrency test- ing, data race detection, record and replay},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314646,
author = {Mohammadi, Mahdi Soltan and Yuki, Tomofumi and Cheshmi, Kazem and Davis, Eddie C. and Hall, Mary and Dehnavi, Maryam Mehri and Nandy, Payal and Olschanowsky, Catherine and Venkat, Anand and Strout, Michelle Mills},
title = {Sparse Computation Data Dependence Simplification for Efficient Compiler-Generated Inspectors},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314646},
doi = {10.1145/3314221.3314646},
abstract = {This paper presents a combined compile-time and runtime loop-carried dependence analysis of sparse matrix codes and evaluates its performance in the context of wavefront parallellism. Sparse computations incorporate indirect memory accesses such as x[col[j]] whose memory locations cannot be determined until runtime. The key contributions of this paper are two compile-time techniques for significantly reducing the overhead of runtime dependence testing: (1) identifying new equality constraints that result in more efficient runtime inspectors, and (2) identifying subset relations between dependence constraints such that one dependence test subsumes another one that is therefore eliminated. New equality constraints discovery is enabled by taking advantage of domain-specific knowledge about index arrays, such as col[j]. These simplifications lead to automatically-generated inspectors that make it practical to parallelize such computations. We analyze our simplification methods for a collection of seven sparse computations. The evaluation shows our methods reduce the complexity of the runtime inspectors significantly. Experimental results for a collection of five large matrices show parallel speedups ranging from 2x to more than 8x running on a 8-core CPU.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {594–609},
numpages = {16},
keywords = {sparse matrices, Presburger arithmetic with uninterpreted functions, data dependence simplification, dependence analysis, SMT solvers, inspector-executor strategies},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.5281/zenodo.2644665,
author = {Mohammadi, Mahdi Soltan and Yuki, Tomofumi and Cheshmi, Kazem and Davis, Eddie C. and Hall, Mary and Dehnavi, Maryam Mehri and Nandy, Payal and Olschanowsky, Catherine and Venkat, Anand and Strout, Michelle Mills},
title = {Replication Package for Article: "Sparse Computation Data Dependence Simplification for Efficient Compiler-Generated Inspectors"},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.2644665},
abstract = {
    <p>We use four packages to implement our approach: IEGenLib library, ISL library, CHiLL compiler framework, and Omega+ codegen included in the CHiLL. CHiLL is a source-to-source compiler framework for composing and applying high-level loop transformations to improve the performance of nested loops written in C. We use CHiLL to extract the dependence relations from the benchmarks. The CHiLL compiler also includes the Omega+ library, a modified version of Omega, which is an integer set manipulation library with limited support for constraints that involve uninterpreted function calls. We have used Omega+'s codegen capability to generate the DAG construction portion of the wavefront inspector code. ISL is a library for manipulating integer sets and relations that only contain affine constraints. It can act as a constraint solver by testing the emptiness of integer sets. It is also equipped with other operations on integer sets for detecting equalities and testing subset relationships. ISL does not support uninterpreted functions, and thus cannot directly represent the dependence constraints in sparse matrix code. IEGenLib is a set manipulation library that can manipulate integer sets/relations that contain uninterpreted function symbols. It uses ISL for some of its functionalities. We implemented the detection of unsatisfiable dependences and finding the equalities utilizing the IEGenLib and ISL libraries. The following briefly describes how our driver, illustrated in Figure 3 of the paper, generates wavefront parallelization inspectors. First, the driver extracts the dependences using CHiLL, and stores them in IEGenLib data structures. The driver also reads the JSON file with user-defined, domain-specific knowledge about index arrays, and stores them in IEGenLib environment variables. Then, it makes a call to an IEGenLib function to simplify the dependences. IEGenLib instantiates universally quantified assertions using the procedure described in Section 5 to prove unsatisfiability and to detect equalities. The uninterpreted functions are removed by replacing each call with a fresh variable, and functional consistency is encoded with additional constraints, before calling ISL to test for satisfiability and to expose equalities. Once the satisfiable, simplified, dependences are obtained, the driver tests each pair of the remaining dependences using IEGenLib for subsets and discards any dependence subsumed by another. Finally, the inspectors for the remaining dependences are generated by Omega+. Since, the outermost loop in the inspectors that we generate are embarrassingly parallel, the driver turns Omega+ generated code into a parallel inspector by simply adding an "omp parallel for" pragma before the outermost loop. The reason why the inspectors are obviously parallel is that each iteration of their outermost loop just connects dependence edges for the row (column) of the same iteration in the dependence graph structure. There are number of different source codes and data sources in this artifact: (1) IEGenLib library: used as a platform for some parts of the implementations. IEGenLib has its own licensing that can be referred to. (2) CHiLL compiler framework: used as a platform for some parts of the implementations. CHiLL has its own licensing that can be referred to. (3) ISL library that is included as part of IEGenLib. ISL has its own licensing that can be referred to. (4) Sparse computations benchmark suit (inside data directory) that have several different sources, and their sources are referenced in the paper. (5) Other codes and scripts, the drivers, built scripts, etc, are implemented by authors.</p>
},
keywords = {CHiLL, Codegen+, data dependence simplification, dependence analysis, IEGenLib, inspector-executor strategies, ISL, Omega+, Presburger arithmetic with uninterpreted functions, SMT solvers, sparse matrices}
}

@inproceedings{10.1145/3314221.3314612,
author = {Farzan, Azadeh and Nicolet, Victor},
title = {Modular Divide-and-Conquer Parallelization of Nested Loops},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314612},
doi = {10.1145/3314221.3314612},
abstract = {We propose a methodology for automatic generation of divide-and-conquer parallel implementations of sequential nested loops. We focus on a class of loops that traverse read-only multidimensional collections (lists or arrays) and compute a function over these collections. Our approach is modular, in that, the inner loop nest is abstracted away to produce a simpler loop nest for parallelization. The summarized version of the loop nest is then parallelized. The main challenge addressed by this paper is that to perform the code transformations necessary in each step, the loop nest may have to be augmented (automatically) with extra computation to make possible the abstraction and/or the parallelization tasks. We present theoretical results to justify the correctness of our modular approach, and algorithmic solutions for automation. Experimental results demonstrate that our approach can parallelize highly non-trivial loop nests efficiently.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {610–624},
numpages = {15},
keywords = {Parallelization, Homomorphisms, Divide and Conquer, Program Synthesis},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314615,
author = {Augustine, Travis and Sarma, Janarthanan and Pouchet, Louis-No\"{e}l and Rodr\'{\i}guez, Gabriel},
title = {Generating Piecewise-Regular Code from Irregular Structures},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314615},
doi = {10.1145/3314221.3314615},
abstract = {Irregular data structures, as exemplified with sparse matrices, have proved to be essential in modern computing. Numerous sparse formats have been investigated to improve the overall performance of Sparse Matrix-Vector multiply (SpMV). But in this work we propose instead to take a fundamentally different approach: to automatically build sets of regular sub-computations by mining for regular sub-regions in the irregular data structure. Our approach leads to code that is specialized to the sparsity structure of the input matrix, but which does not need anymore any indirection array, thereby improving SIMD vectorizability. We particularly focus on small sparse structures (below 10M nonzeros), and demonstrate substantial performance improvements and compaction capabilities compared to a classical CSR implementation and Intel MKL IE's SpMV implementation, evaluating on 200+ different matrices from the SuiteSparse repository.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {625–639},
numpages = {15},
keywords = {SpMV, Polyhedral compilation, trace compression, sparse data structure},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314607,
author = {Liao, Kevin and Hammer, Matthew A. and Miller, Andrew},
title = {ILC: A Calculus for Composable, Computational Cryptography},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314607},
doi = {10.1145/3314221.3314607},
abstract = {The universal composability (UC) framework is the established standard for analyzing cryptographic protocols in a modular way, such that security is preserved under concurrent composition with arbitrary other protocols. However, although UC is widely used for on-paper proofs, prior attempts at systemizing it have fallen short, either by using a symbolic model (thereby ruling out computational reduction proofs), or by limiting its expressiveness. In this paper, we lay the groundwork for building a concrete, executable implementation of the UC framework. Our main contribution is a process calculus, dubbed the Interactive Lambda Calculus (ILC). ILC faithfully captures the computational model underlying UC—interactive Turing machines (ITMs)—by adapting ITMs to a subset of the π-calculus through an affine typing discipline. In other words, well-typed ILC programs are expressible as ITMs. In turn, ILC’s strong confluence property enables reasoning about cryptographic security reductions. We use ILC to develop a simplified implementation of UC called SaUCy.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {640–654},
numpages = {15},
keywords = {process calculus, universal composability, type systems, Provable security},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314619,
author = {Wang, Yuxin and Ding, Zeyu and Wang, Guanhong and Kifer, Daniel and Zhang, Danfeng},
title = {Proving Differential Privacy with Shadow Execution},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314619},
doi = {10.1145/3314221.3314619},
abstract = {Recent work on formal verification of differential privacy shows a trend toward usability and expressiveness -- generating a correctness proof of sophisticated algorithm while minimizing the annotation burden on programmers. Sometimes, combining those two requires substantial changes to program logics: one recent paper is able to verify Report Noisy Max automatically, but it involves a complex verification system using customized program logics and verifiers.  In this paper, we propose a new proof technique, called shadow execution, and embed it into a language called ShadowDP. ShadowDP uses shadow execution to generate proofs of differential privacy with very few programmer annotations and without relying on customized logics and verifiers. In addition to verifying Report Noisy Max, we show that it can verify a new variant of Sparse Vector that reports the gap between some noisy query answers and the noisy threshold. Moreover, ShadowDP reduces the complexity of verification: for all of the algorithms we have evaluated, type checking and verification in total takes at most 3 seconds, while prior work takes minutes on the same algorithms.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {655–669},
numpages = {15},
keywords = {dependent types, Differential privacy},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314580,
author = {Mamouras, Konstantinos and Stanford, Caleb and Alur, Rajeev and Ives, Zachary G. and Tannen, Val},
title = {Data-Trace Types for Distributed Stream Processing Systems},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314580},
doi = {10.1145/3314221.3314580},
abstract = {Distributed architectures for efficient processing of streaming data are increasingly critical to modern information processing systems. The goal of this paper is to develop type-based programming abstractions that facilitate correct and efficient deployment of a logical specification of the desired computation on such architectures. In the proposed model, each communication link has an associated type specifying tagged data items along with a dependency relation over tags that captures the logical partial ordering constraints over data items. The semantics of a (distributed) stream processing system is then a function from input data traces to output data traces, where a data trace is an equivalence class of sequences of data items induced by the dependency relation. This data-trace transduction model generalizes both acyclic synchronous data-flow and relational query processors, and can specify computations over data streams with a rich variety of partial ordering and synchronization characteristics. We then describe a set of programming templates for data-trace transductions: abstractions corresponding to common stream processing tasks. Our system automatically maps these high-level programs to a given topology on the distributed implementation platform Apache Storm while preserving the semantics. Our experimental evaluation shows that (1) while automatic parallelization deployed by existing systems may not preserve semantics, particularly when the computation is sensitive to the ordering of data items, our programming abstractions allow a natural specification of the query that contains a mix of ordering constraints while guaranteeing correct deployment, and (2) the throughput of the automatically compiled distributed code is comparable to that of hand-crafted distributed implementations.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {670–685},
numpages = {16},
keywords = {distributed data stream processing, types},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314638,
author = {Zhu, He and Xiong, Zikang and Magill, Stephen and Jagannathan, Suresh},
title = {An Inductive Synthesis Framework for Verifiable Reinforcement Learning},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314638},
doi = {10.1145/3314221.3314638},
abstract = {Despite the tremendous advances that have been made in the last decade on developing useful machine-learning applications, their wider adoption has been hindered by the lack of strong assurance guarantees that can be made about their behavior. In this paper, we consider how formal verification techniques developed for traditional software systems can be repurposed for verification of reinforcement learning-enabled ones, a particularly important class of machine learning systems. Rather than enforcing safety by examining and altering the structure of a complex neural network implementation, our technique uses blackbox methods to synthesizes deterministic programs, simpler, more interpretable, approximations of the network that can nonetheless guarantee desired safety properties are preserved, even when the network is deployed in unanticipated or previously unobserved environments. Our methodology frames the problem of neural network verification in terms of a counterexample and syntax-guided inductive synthesis procedure over these programs. The synthesis procedure searches for both a deterministic program and an inductive invariant over an infinite state transition system that represents a specification of an application's control logic. Additional specifications defining environment-based constraints can also be provided to further refine the search space. Synthesized programs deployed in conjunction with a neural network implementation dynamically enforce safety conditions by monitoring and preventing potentially unsafe actions proposed by neural policies. Experimental results over a wide range of cyber-physical applications demonstrate that software-inspired formal verification techniques can be used to realize trustworthy reinforcement learning systems with low overhead.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {686–701},
numpages = {16},
keywords = {Program Synthesis, Program Verification, Reinforcement Learning, Runtime Shielding, Invariant Inference},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325983,
author = {Zhu, He and Xiong, Zikang and Magill, Stephen and Jagannathan, Suresh},
title = {Replication Package for Article: An Inductive Synthesis Framework for Verifiable Reinforcement Learning},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325983},
abstract = {
    <p>A toolset for verify safety properties of reinforcement learning system.</p>
},
keywords = {Invariant Inference, Program Synthesis, Program Verification, Reinforcement Learning, Runtime Shielding}
}

@inproceedings{10.1145/3314221.3314593,
author = {Lee, Wen-Chuan and Liu, Peng and Liu, Yingqi and Ma, Shiqing and Zhang, Xiangyu},
title = {Programming Support for Autonomizing Software},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314593},
doi = {10.1145/3314221.3314593},
abstract = {Most traditional software systems are not built with the artificial intelligence support (AI) in mind. Among them, some may require human interventions to operate, e.g., the manual specification of the parameters in the data processing programs, or otherwise, would behave poorly. We propose a novel framework called Autonomizer to autonomize these systems by installing the AI into the traditional programs. Autonomizeris general so it can be applied to many real-world applications. We provide the primitives and the run-time support, where the primitives abstract common tasks of autonomization and the runtime support realizes them transparently. With the support of Autonomizer, the users can gain the AI support with little engineering efforts. Like many other AI applications, the challenge lies in the feature selection, which we address by proposing multiple automated strategies based on the program analysis. Our experiment results on nine real-world applications show that the autonomization only requires adding a few lines to the source code.Besides, for the data-processing programs, Autonomizer improves the output quality by 161% on average over the default settings. For the interactive programs such as game/driving,Autonomizer achieves higher success rate with lower training time than existing autonomized programs.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {702–716},
numpages = {15},
keywords = {Dynamic Program Analysis, Deep Learning, Software Autonomization, AI},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314652,
author = {Guan, Hui and Shen, Xipeng and Lim, Seung-Hwan},
title = {Wootz: A Compiler-Based Framework for Fast CNN Pruning via Composability},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314652},
doi = {10.1145/3314221.3314652},
abstract = {Convolutional Neural Networks (CNN) are widely used for Deep Learning tasks. CNN pruning is an important method to adapt a large CNN model trained on general datasets to fit a more specialized task or a smaller device. The key challenge is on deciding which filters to remove in order to maximize the quality of the pruned networks while satisfying the constraints. It is time-consuming due to the enormous configuration space and the slowness of CNN training.  The problem has drawn many efforts from the machine learning field, which try to reduce the set of network configurations to explore. This work tackles the problem distinctively from a programming systems perspective, trying to speed up the evaluations of the remaining configurations through computation reuse via a compiler-based framework. We empirically uncover the existence of composability in the training of a collection of pruned CNN models, and point out the opportunities for computation reuse. We then propose composability-based CNN pruning, and design a compression-based algorithm to efficiently identify the set of CNN layers to pre-train for maximizing their reuse benefits in CNN pruning. We further develop a compiler-based framework named Wootz, which, for an arbitrary CNN, automatically generates code that builds a Teacher-Student scheme to materialize composability-based pruning. Experiments show that network pruning enabled by Wootz shortens the state-of-art pruning process by up to 186X while producing significantly better pruning results.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {717–730},
numpages = {14},
keywords = {compiler, CNN, network pruning, composability},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314614,
author = {Anderson, Greg and Pailoor, Shankara and Dillig, Isil and Chaudhuri, Swarat},
title = {Optimization and Abstraction: A Synergistic Approach for Analyzing Neural Network Robustness},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314614},
doi = {10.1145/3314221.3314614},
abstract = {In recent years, the notion of local robustness (or robustness for short) has emerged as a desirable property of deep neural networks. Intuitively, robustness means that small perturbations to an input do not cause the network to perform misclassifications. In this paper, we present a novel algorithm for verifying robustness properties of neural networks. Our method synergistically combines gradient-based optimization methods for counterexample search with abstraction-based proof search to obtain a sound and (δ -)complete decision procedure. Our method also employs a data-driven approach to learn a verification policy that guides abstract interpretation during proof search. We have implemented the proposed approach in a tool called Charon and experimentally evaluated it on hundreds of benchmarks. Our experiments show that the proposed approach significantly outperforms three state-of-the-art tools, namely AI^2, Reluplex, and Reluval.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {731–744},
numpages = {14},
keywords = {Abstract Interpretation, Robustness, Optimization, Machine learning},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314640,
author = {Eberhardt, Jan and Steffen, Samuel and Raychev, Veselin and Vechev, Martin},
title = {Unsupervised Learning of API Aliasing Specifications},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314640},
doi = {10.1145/3314221.3314640},
abstract = {Real world applications make heavy use of powerful libraries and frameworks, posing a significant challenge for static analysis as the library implementation may be very complex or unavailable. Thus, obtaining specifications that summarize the behaviors of the library is important as it enables static analyzers to precisely track the effects of APIs on the client program, without requiring the actual API implementation.  In this work, we propose a novel method for discovering aliasing specifications of APIs by learning from a large dataset of programs. Unlike prior work, our method does not require manual annotation, access to the library's source code or ability to run its APIs. Instead, it learns specifications in a fully unsupervised manner, by statically observing usages of APIs in the dataset. The core idea is to learn a probabilistic model of interactions between API methods and aliasing objects, enabling identification of additional likely aliasing relations, and to then infer aliasing specifications of APIs that explain these relations. The learned specifications are then used to augment an API-aware points-to analysis.  We implemented our approach in a tool called USpec and used it to automatically learn aliasing specifications from millions of source code files. USpec learned over 2000 specifications of various Java and Python APIs, in the process improving the results of the points-to analysis and its clients.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {745–759},
numpages = {15},
keywords = {unsupervised machine learning, big code, specification, pointer analysis},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314648,
author = {Chibotaru, Victor and Bichsel, Benjamin and Raychev, Veselin and Vechev, Martin},
title = {Scalable Taint Specification Inference with Big Code},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314648},
doi = {10.1145/3314221.3314648},
abstract = {We present a new scalable, semi-supervised method for inferring taint analysis specifications by learning from a large dataset of programs. Taint specifications capture the role of library APIs (source, sink, sanitizer) and are a critical ingredient of any taint analyzer that aims to detect security violations based on information flow. The core idea of our method is to formulate the taint specification learning problem as a linear optimization task over a large set of information flow constraints. The resulting constraint system can then be efficiently solved with state-of-the-art solvers. Thanks to its scalability, our method can infer many new and interesting taint specifications by simultaneously learning from a large dataset of programs (e.g., as found on GitHub), while requiring few manual annotations. We implemented our method in an end-to-end system, called Seldon, targeting Python, a language where static specification inference is particularly hard due to lack of typing information. We show that Seldon is practically effective: it learned almost 7,000 API roles from over 210,000 candidate APIs with very little supervision (less than 300 annotations) and with high estimated precision (67%). Further, using the learned specifications, our taint analyzer flagged more than 20,000 violations in open source projects, 97% of which were undetectable without the inferred specifications.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {760–774},
numpages = {15},
keywords = {Taint Analysis, Big Code, Specification Inference},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314641,
author = {Astorga, Angello and Madhusudan, P. and Saha, Shambwaditya and Wang, Shiyu and Xie, Tao},
title = {Learning Stateful Preconditions modulo a Test Generator},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314641},
doi = {10.1145/3314221.3314641},
abstract = {In this paper, we present a novel learning framework for inferring stateful preconditions (i.e., preconditions constraining not only primitive-type inputs but also non-primitive-type object states) modulo a test generator, where the quality of the preconditions is based on their safety and maximality with respect to the test generator. We instantiate the learning framework with a specific learner and test generator to realize a precondition synthesis tool for C#. We use an extensive evaluation to show that the tool is highly effective in synthesizing preconditions for avoiding exceptions as well as synthesizing conditions under which methods commute.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {775–787},
numpages = {13},
keywords = {Synthesis, Data-Driven Inference, Specification Mining},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314634,
author = {Le, Ton Chanh and Zheng, Guolong and Nguyen, ThanhVu},
title = {SLING: Using Dynamic Analysis to Infer Program Invariants in Separation Logic},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314634},
doi = {10.1145/3314221.3314634},
abstract = {We introduce a new dynamic analysis technique to discover invariants in separation logic for heap-manipulating programs. First, we use a debugger to obtain rich program execution traces at locations of interest on sample inputs. These traces consist of heap and stack information of variables that point to dynamically allocated data structures. Next, we iteratively analyze separate memory regions related to each pointer variable and search for a formula over predefined heap predicates in separation logic to model these regions. Finally, we combine the computed formulae into an invariant that describes the shape of explored memory regions.  We present SLING, a tool that implements these ideas to automatically generate invariants in separation logic at arbitrary locations in C programs, e.g., program pre and postconditions and loop invariants. Preliminary results on existing benchmarks show that SLING can efficiently generate correct and useful invariants for programs that manipulate a wide variety of complex data structures.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {788–801},
numpages = {14},
keywords = {dynamic invariant analysis, separation logic},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325981,
author = {Le, Ton Chanh and Zheng, Guolong and Nguyen, ThanhVu},
title = {Replication Package for Article: SLING: Using Dynamic Analysis to Infer Program Invariants in Separation Logic},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325981},
abstract = {
    <p>SLING is a tool for dynamically inferring separation logic specifications. Current version of SLING works with C or C++. It uses LLDB to obtain traces and Python to infer separation logic invariants based on the traces.</p>
},
keywords = {dynamic invariant generation, program analysis, separation logic}
}

@inproceedings{10.1145/3314221.3314647,
author = {Wu, Meng and Wang, Chao},
title = {Abstract Interpretation under Speculative Execution},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314647},
doi = {10.1145/3314221.3314647},
abstract = {Analyzing the behavior of a program running on a processor that supports speculative execution is crucial for applications such as execution time estimation and side channel detection. Unfortunately, existing static analysis techniques based on abstract interpretation do not model speculative execution since they focus on functional properties of a program while speculative execution does not change the functionality. To fill the gap, we propose a method to make abstract interpretation sound under speculative execution. There are two contributions. First, we introduce the notion of virtual control flow to augment instructions that may be speculatively executed and thus affect subsequent instructions. Second, to make the analysis efficient, we propose optimizations to handle merges and loops and to safely bound the speculative execution depth. We have implemented and evaluated the proposed method in a static cache analysis for execution time estimation and side channel detection. Our experiments show that the new method, while guaranteed to be sound under speculative execution, outperforms state-of-the-art abstract interpretation techniques that may be unsound.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {802–815},
numpages = {14},
keywords = {WCET, abstract interpretation, cache, Static analysis, speculative execution, timing side channel},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314606,
author = {Gysi, Tobias and Grosser, Tobias and Brandner, Laurin and Hoefler, Torsten},
title = {A Fast Analytical Model of Fully Associative Caches},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314606},
doi = {10.1145/3314221.3314606},
abstract = {While the cost of computation is an easy to understand local property, the cost of data movement on cached architectures depends on global state, does not compose, and is hard to predict. As a result, programmers often fail to consider the cost of data movement. Existing cache models and simulators provide the missing information but are computationally expensive. We present a lightweight cache model for fully associative caches with least recently used (LRU) replacement policy that gives fast and accurate results. We count the cache misses without explicit enumeration of all memory accesses by using symbolic counting techniques twice: 1) to derive the stack distance for each memory access and 2) to count the memory accesses with stack distance larger than the cache size. While this technique seems infeasible in theory, due to non-linearities after the first round of counting, we show that the counting problems are sufficiently linear in practice. Our cache model often computes the results within seconds and contrary to simulation the execution time is mostly problem size independent. Our evaluation measures modeling errors below 0.6% on real hardware. By providing accurate data placement information we enable memory hierarchy aware software development.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {816–829},
numpages = {14},
keywords = {static analysis, performance tool, cache model},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325990,
author = {Gysi, Tobias and Grosser, Tobias and Brandner, Laurin and Hoefler, Torsten},
title = {Replication Package for Article: A Fast Analytical Model of Fully Associative Caches},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325990},
abstract = {
    <p>The artifact contains the sources, benchmarks, and scripts necessary to reproduce the results of the paper "A Fast Analytical Model of Fully Associative Caches".</p>
},
keywords = {cache model, performance tool, static analysis}
}

@inproceedings{10.1145/3314221.3314626,
author = {Sakka, Laith and Sundararajah, Kirshanthan and Newton, Ryan R. and Kulkarni, Milind},
title = {Sound, Fine-Grained Traversal Fusion for Heterogeneous Trees},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314626},
doi = {10.1145/3314221.3314626},
abstract = {Applications in many domains are based on a series of traversals of tree structures, and fusing these traversals together to reduce the total number of passes over the tree is a common, important optimization technique. In applications such as compilers and render trees, these trees are heterogeneous: different nodes of the tree have different types. Unfortunately, prior work for fusing traversals falls short in different ways: they do not handle heterogeneity; they require using domain-specific languages to express an application; they rely on the programmer to aver that fusing traversals is safe, without any soundness guarantee; or they can only perform coarse-grain fusion, leading to missed fusion opportunities. This paper addresses these shortcomings to build a framework for fusing traversals of heterogeneous trees that is automatic, sound, and fine-grained. We show across several case studies that our approach is able to allow programmers to write simple, intuitive traversals, and then automatically fuse them to substantially improve performance.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {830–844},
numpages = {15},
keywords = {Locality, Tree traversals, Fusion},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325973,
author = {Sakka, Laith and Sundararajah, Kirshanthan and Newton, Ryan R. and Kulkarni, Milind},
title = {Grafter: Clang Tool That Perform Sound, Fine-Grained Traversal Fusion for Heterogeneous Trees},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325973},
abstract = {
    <p>Grafter is clang tool that performs source to source transformation, namely traversals fusion. Grafter parse programs written in subset of C++, and write back the transformed code into the same files. The artifact includes the source code of Grafter in addition to the instructions on how to build it and use it. It also includes a pre-build version on a VM, with instructions on how to regenerate the experiments in the initial version of the paper.</p>
},
keywords = {Clang tool., Traversal Fusion}
}

@inproceedings{10.1145/3314221.3314643,
author = {Nguyundefinedn, Ph\'{u}c C. and Gilray, Thomas and Tobin-Hochstadt, Sam and Van Horn, David},
title = {Size-Change Termination as a Contract: Dynamically and Statically Enforcing Termination for Higher-Order Programs},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314643},
doi = {10.1145/3314221.3314643},
abstract = {Termination is an important but undecidable program property, which has led to a large body of work on static methods for conservatively predicting or enforcing termination. One such method is the size-change termination approach of Lee, Jones, and Ben-Amram, which operates in two phases: (1) abstract programs into “size-change graphs,” and (2) check these graphs for the size-change property: the existence of paths that lead to infinite decreasing sequences. We transpose these two phases with an operational semantics that accounts for the run-time enforcement of the size-change property, postponing (or entirely avoiding) program abstraction. This choice has two key consequences: (1) size-change termination can be checked at run-time and (2) termination can be rephrased as a safety property analyzed using existing methods for systematic abstraction. We formulate run-time size-change checks as contracts in the style of Findler and Felleisen. The result compliments existing contracts that enforce partial correctness specifications to obtain contracts for total correctness. Our approach combines the robustness of the size-change principle for termination with the precise information available at run-time. It has tunable overhead and can check for nontermination without the conservativeness necessary in static checking. To obtain a sound and computable termination analysis, we apply existing abstract interpretation techniques directly to the operational semantics, avoiding the need for custom abstractions for termination. The resulting analyzer is competitive with with existing, purpose-built analyzers.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {845–859},
numpages = {15},
keywords = {verification, termination, size-change principle},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325984,
author = {Nguy?n, Ph\'{u}c C. and Gilray, Thomas and Tobin-Hochstadt, Sam and Van Horn, David},
title = {Checkers Used in: Size-Change Termination as a Contract},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325984},
abstract = {
    <p>The dynamic and static checkers for size-change termination, and the tests used in the paper.</p>
},
keywords = {dynamic analysis, static analysis, termination checker}
}

@inproceedings{10.1145/3314221.3314629,
author = {Perry, David M. and Kim, Dohyeong and Samanta, Roopsha and Zhang, Xiangyu},
title = {SemCluster: Clustering of Imperative Programming Assignments Based on Quantitative Semantic Features},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314629},
doi = {10.1145/3314221.3314629},
abstract = {A fundamental challenge in automated reasoning about programming assignments at scale is clustering student submissions based on their underlying algorithms. State-of-the-art clustering techniques are sensitive to control structure variations, cannot cluster buggy solutions with similar correct solutions, and either require expensive pair-wise program analyses or training efforts. We propose a novel technique that can cluster small imperative programs based on their algorithmic essence: (A) how the input space is partitioned into equivalence classes and (B) how the problem is uniquely addressed within individual equivalence classes. We capture these algorithmic aspects as two quantitative semantic program features that are merged into a program's vector representation. Programs are then clustered using their vector representations. The computation of our first semantic feature leverages model counting to identify the number of inputs belonging to an input equivalence class. The computation of our second semantic feature abstracts the program's data flow by tracking the number of occurrences of a unique pair of consecutive values of a variable during its lifetime. The comprehensive evaluation of our tool SemCluster on benchmarks drawn from solutions to small programming assignments shows that SemCluster (1) generates far fewer clusters than other clustering techniques, (2) precisely identifies distinct solution strategies, and (3) boosts the performance of clustering-based program repair, all within a reasonable amount of time.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {860–873},
numpages = {14},
keywords = {Program clustering, Quantitative reasoning, Program analysis},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314610,
author = {Kapus, Timotej and Ish-Shalom, Oren and Itzhaky, Shachar and Rinetzky, Noam and Cadar, Cristian},
title = {Computing Summaries of String Loops in C for Better Testing and Refactoring},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314610},
doi = {10.1145/3314221.3314610},
abstract = {Analysing and comprehending C programs that use strings is hard: using standard library functions for manipulating strings is not enforced and programs often use complex loops for the same purpose. We introduce the notion of memoryless loops that capture some of these string loops and present a counterexample-guided synthesis approach to summarise memoryless loops using C standard library functions, which has applications to testing, optimisation and refactoring.  We prove our summarisation is correct for arbitrary input strings and evaluate it on a database of loops we gathered from thirteen open-source programs. Our approach can summarise over two thirds of memoryless loops in less than five minutes of computation time per loop. We then show that these summaries can be used to (1) improve symbolic execution (2) optimise native code, and (3) refactor code.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {874–888},
numpages = {15},
keywords = {Strings, Refactoring, Optimisation, Synthesis, Loop Summarisation, Symbolic Execution},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325967,
author = {Kapus, Timotej and Ish-Shalom, Oren and Itzhaky, Shachar and Rinetzky, Noam and Cadar, Cristian},
title = {Replication Package for Cumputing Summaries of String Loops},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325967},
abstract = {
    <p>The artefact contains a docker image that has the implementation of loop summarisation described in the paper set up. See the README file for more details.</p>
},
keywords = {Loop Summarisation, Optimisation, Refactoring, Strings, Symbolic Execution, Synthesis}
}

@inproceedings{10.1145/3314221.3314587,
author = {Choi, Jiho and Shull, Thomas and Torrellas, Josep},
title = {Reusable Inline Caching for JavaScript Performance},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314587},
doi = {10.1145/3314221.3314587},
abstract = {JavaScript performance is paramount to a user’s browsing experience. Browser vendors have gone to great lengths to improve JavaScript’s steady-state performance. This has led to sophisticated web applications. However, as users increasingly expect instantaneous page load times, another important goal for JavaScript engines is to attain minimal startup times.  In this paper, we reduce the startup time of JavaScript programs by enhancing the reuse of compilation and optimization information across different executions. Specifically, we propose a new scheme to increase the startup performance of Inline Caching (IC), a key optimization for dynamic type systems. The idea is to represent a substantial portion of the IC information in an execution in a context-independent way, and reuse it in subsequent executions. We call our enhanced IC design Reusable Inline Caching (RIC). We integrate RIC into the state-of-the-art Google V8 JavaScript engine and measure its impact on the initialization time of popular JavaScript libraries. By recycling IC information collected from a previous execution, RIC reduces the average initialization time per library by 17%.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {889–901},
numpages = {13},
keywords = {Dynamic Typing, JavaScript, Scripting Language, Inline Caching},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314592,
author = {Sundararajah, Kirshanthan and Kulkarni, Milind},
title = {Composable, Sound Transformations of Nested Recursion and Loops},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314592},
doi = {10.1145/3314221.3314592},
abstract = {Scheduling transformations reorder a program’s operations to improve locality and/or parallelism. The polyhedral model is a general framework for composing and applying instance-wise scheduling transformations for loop-based programs, but there is no analogous framework for recursive programs. This paper presents an approach for composing and applying scheduling transformations—like inlining, interchange, and code motion—to nested recursive programs. This paper describes the phases of the approach—representing dynamic instances, composing and applying transformations, reasoning about correctness—and shows that these techniques can verify the soundness of composed transformations.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {902–917},
numpages = {16},
keywords = {Dependence Testing, Locality, Recursion, Scheduling Transformations},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314598,
author = {Dhulipala, Laxman and Blelloch, Guy E. and Shun, Julian},
title = {Low-Latency Graph Streaming Using Compressed Purely-Functional Trees},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314598},
doi = {10.1145/3314221.3314598},
abstract = {There has been a growing interest in the graph-streaming setting where a continuous stream of graph updates is mixed with graph queries. In principle, purely-functional trees are an ideal fit for this setting as they enable safe parallelism, lightweight snapshots, and strict serializability for queries. However, directly using them for graph processing leads to significant space overhead and poor cache locality.  This paper presents C-trees, a compressed purely-functional search tree data structure that significantly improves on the space usage and locality of purely-functional trees. We design theoretically-efficient and practical algorithms for performing batch updates to C-trees, and also show that we can store massive dynamic real-world graphs using only a few bytes per edge, thereby achieving space usage close to that of the best static graph processing frameworks.  To study the efficiency and applicability of our data structure, we designed Aspen, a graph-streaming framework that extends the interface of Ligra with operations for updating graphs. We show that Aspen is faster than two state-of-the-art graph-streaming systems, Stinger and LLAMA, while requiring less memory, and is competitive in performance with the state-of-the-art static graph frameworks, Galois, GAP, and Ligra+. With Aspen, we are able to efficiently process the largest publicly-available graph with over two hundred billion edges in the graph-streaming setting using a single commodity multicore server with 1TB of memory.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {918–934},
numpages = {17},
keywords = {streaming graph processing, purely-functional data structures, parallel graph algorithms},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325976,
author = {Dhulipala, Laxman and Blelloch, Guy E. and Shun, Julian},
title = {Artifact for "Low-Latency Graph Streaming Using Compressed Purely-Functional Trees"},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325976},
abstract = {
    <p>This repository provides experiments, scripts, and instructions for reproducing the experiments in our paper, *Low-Latency Graph Streaming Using Compressed Purely-Functional Trees*. Our paper introduces Aspen, a graph-streaming system based on compressed purely-functional trees. Aspen is designed for maintaining a dynamic graph subject to updates by a single writer, while supporting multiple concurrent readers. Due to the fact that the graph is purely-functional, all operations in Aspen are strictly serializable. In the Getting Started Guide, we include functionality to reproduce the main results presented in our paper. In the Step-By-Step Instructions, we also include the codes and instructions used for running experiments on very large graphs (hundreds of billions of edges). Due to the size of these graphs, and the memory footprint requirement on the machine, and the time to download, convert, and process these graphs, we expect that most users will not perform these steps, but we include them for completeness, and to ensure that our results for very large graphs are reproducible. We have made all graphs used in our paper publicly-available to ensure that our results are reproducible and can be built upon.</p>
},
keywords = {dynamic graph processing, graph streaming, parallel graph algorithms}
}

@inproceedings{10.1145/3314221.3314599,
author = {Tang, Xulong and Kandemir, Mahmut Taylan and Karakoy, Mustafa and Arunachalam, Meenakshi},
title = {Co-Optimizing Memory-Level Parallelism and Cache-Level Parallelism},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314599},
doi = {10.1145/3314221.3314599},
abstract = {Minimizing cache misses has been the traditional goal in optimizing cache performance using compiler based techniques. However, continuously increasing dataset sizes combined with large numbers of cache banks and memory banks connected using on-chip networks in emerging manycores/accelerators makes cache hit–miss latency optimization as important as cache miss rate minimization. In this paper, we propose compiler support that optimizes both the latencies of last-level cache (LLC) hits and the latencies of LLC misses. Our approach tries to achieve this goal by improving the parallelism exhibited by LLC hits and LLC misses. More specifically, it tries to maximize both cache-level parallelism (CLP) and memory-level parallelism (MLP). This paper presents different incarnations of our approach, and evaluates them using a set of 12 multithreaded applications. Our results indicate that (i) optimizing MLP first and CLP later brings, on average, 11.31% performance improvement over an approach that already minimizes the number of LLC misses, and (ii) optimizing CLP first and MLP later brings 9.43% performance improvement. In comparison, balancing MLP and CLP brings 17.32% performance improvement on average.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {935–949},
numpages = {15},
keywords = {Manycore systems, data access parallelism},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314600,
author = {Rowe, Reuben N. S. and F\'{e}r\'{e}e, Hugo and Thompson, Simon J. and Owens, Scott},
title = {Characterising Renaming within OCaml’s Module System: Theory and Implementation},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314600},
doi = {10.1145/3314221.3314600},
abstract = {We present an abstract, set-theoretic denotational semantics for a significant subset of OCaml and its module system, allowing to reason about the correctness of renaming value bindings. Our semantics captures information about the binding structure of programs, as well as about which declarations are related by the use of different language constructs (e.g. functors, module types and module constraints). Correct renamings are precisely those that preserve this structure. We show that our abstract semantics is sound with respect to a (domain-theoretic) denotational model of the operational behaviour of programs, and that it allows us to prove various high-level, intuitive properties of renamings. This formal framework has been implemented in a prototype refactoring tool for OCaml that performs renaming.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {950–965},
numpages = {16},
keywords = {refactoring, modules, module types, dependencies, semantics, OCaml, renaming, Adequacy},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.5281/zenodo.2646525,
author = {Rowe, Reuben N. S. and F�r�e, Hugo and Thompson, Simon J. and Owens, Scott},
title = {ROTOR: A Reliable OCaml Tool for OCaml Refactoring - Trustworthy Refactoring for OCaml},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.2646525},
abstract = {
    <p>ROTOR is a refactoring tool for the OCaml language that is written in OCaml.</p>
},
keywords = {OCaml, Refactoring}
}

@inproceedings{10.1145/3314221.3314630,
author = {Kazerounian, Milod and Guria, Sankha Narayan and Vazou, Niki and Foster, Jeffrey S. and Van Horn, David},
title = {Type-Level Computations for Ruby Libraries},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314630},
doi = {10.1145/3314221.3314630},
abstract = {Many researchers have explored ways to bring static typing to dynamic languages. However, to date, such systems are not precise enough when types depend on values, which often arises when using certain Ruby libraries. For example, the type safety of a database query in Ruby on Rails depends on the table and column names used in the query. To address this issue, we introduce CompRDL, a type system for Ruby that allows library method type signatures to include type-level computations (or comp types for short). Combined with singleton types for table and column names, comp types let us give database query methods type signatures that compute a table’s schema to yield very precise type information. Comp types for hash, array, and string libraries can also increase precision and thereby reduce the need for type casts. We formalize CompRDL and prove its type system sound. Rather than type check the bodies of library methods with comp types—those methods may include native code or be complex—CompRDL inserts run-time checks to ensure library methods abide by their computed types. We evaluated CompRDL by writing annotations with type-level computations for several Ruby core libraries and database query APIs. We then used those annotations to type check two popular Ruby libraries and four Ruby on Rails web apps. We found the annotations were relatively compact and could successfully type check 132 methods across our subject programs. Moreover, the use of type-level computations allowed us to check more expressive properties, with fewer manually inserted casts, than was possible without type-level computations. In the process, we found two type errors and a documentation error that were confirmed by the developers. Thus, we believe CompRDL is an important step forward in bringing precise static type checking to dynamic languages.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {966–979},
numpages = {14},
keywords = {Ruby, type-level computations, types, dynamic languages, libraries, database queries},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325964,
author = {Kazerounian, Milod and Guria, Sankha Narayan and Vazou, Niki and Foster, Jeffrey S. and Van Horn, David},
title = {Replication Package for Paper: Type-Level Computations for Ruby Libraries},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325964},
abstract = {
    <p>We provide a virtual machine containing the necessary materials to recreate the data presented in the paper, including: CompRDL, library type annotations, and annotated benchmark apps.</p>
},
keywords = {database queries, dynamic languages, libraries, Ruby, type-level computations, types}
}

@inproceedings{10.1145/3314221.3314617,
author = {Wang, Chao and Enea, Constantin and Mutluergil, Suha Orhun and Petri, Gustavo},
title = {Replication-Aware Linearizability},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314617},
doi = {10.1145/3314221.3314617},
abstract = {Distributed systems often replicate data at multiple locations to achieve availability despite network partitions. These systems accept updates at any replica and propagate them asynchronously to every other replica. Conflict-Free Replicated Data Types (CRDTs) provide a principled approach to the problem of ensuring that replicas are eventually consistent despite the asynchronous delivery of updates.  We address the problem of specifying and verifying CRDTs, introducing a new correctness criterion called Replication-Aware Linearizability. This criterion is inspired by linearizability, the de-facto correctness criterion for (shared-memory) concurrent data structures. We argue that this criterion is both simple to understand, and it fits most known implementations of CRDTs. We provide a proof methodology to show that a CRDT satisfies replication-aware linearizability that we apply on a wide range of implementations. Finally, we show that our criterion can be leveraged to reason modularly about the composition of CRDTs.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {980–993},
numpages = {14},
keywords = {consistency, replicated data types, verification},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314620,
author = {Li, Guangpu and Liu, Haopeng and Chen, Xianglan and Gunawi, Haryadi S. and Lu, Shan},
title = {DFix: Automatically Fixing Timing Bugs in Distributed Systems},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314620},
doi = {10.1145/3314221.3314620},
abstract = {Distributed systems nowadays are the backbone of computing society, and are expected to have high availability. Unfortunately, distributed timing bugs, a type of bugs triggered by non-deterministic timing of messages and node crashes, widely exist. They lead to many production-run failures, and are difficult to reason about and patch. Although recently proposed techniques can automatically detect these bugs, how to automatically and correctly fix them still remains as an open problem. This paper presents DFix, a tool that automatically processes distributed timing bug reports, statically analyzes the buggy system, and produces patches. Our evaluation shows that DFix is effective in fixing real-world distributed timing bugs.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {994–1009},
numpages = {16},
keywords = {Distributed system, Timing, Bug fixing},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314586,
author = {Vasilakis, Nikos and Karel, Ben and Palkhiwala, Yash and Sonchack, John and DeHon, Andr\'{e} and Smith, Jonathan M.},
title = {Ignis: Scaling Distribution-Oblivious Systems with Light-Touch Distribution},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314586},
doi = {10.1145/3314221.3314586},
abstract = {Distributed systems offer notable benefits over their centralized counterparts. Reaping these benefits, however, requires burdensome developer effort to identify and rewrite bottlenecked components. Light-touch distribution is a new approach that converts a legacy system into a distributed one using automated transformations. Transformations operate at the boundaries of bottlenecked modules and are parametrizable by light distribution recipes that guide the intended semantics of the resulting distribution. Transformations and recipes operate at runtime, adapting to load by scaling out only saturated components. Our Ignis prototype shows substantial speedups, attractive elasticity characteristics, and memory gains over full replication, achieved by small and backward-compatible code changes.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1010–1026},
numpages = {17},
keywords = {Distribution, Decomposition, Scale-out, Scalability, Profiling, Transformations, Parallelism, Load detection},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314596,
author = {Churchill, Berkeley and Padon, Oded and Sharma, Rahul and Aiken, Alex},
title = {Semantic Program Alignment for Equivalence Checking},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314596},
doi = {10.1145/3314221.3314596},
abstract = {We introduce a robust semantics-driven technique for program equivalence checking. Given two functions we find a trace alignment over a set of concrete executions of both programs and construct a product program particularly amenable to checking equivalence.  We demonstrate that our algorithm is applicable to challenging equivalence problems beyond the scope of existing techniques. For example, we verify the correctness of the hand-optimized vector implementation of strlen that ships as part of the GNU C Library, as well as the correctness of vectorization optimizations for 56 benchmarks derived from the Test Suite for Vectorizing Compilers.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1027–1040},
numpages = {14},
keywords = {equivalence checking, verification},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.5281/zenodo.2646720,
author = {Churchill, Berkeley and Padon, Oded and Sharma, Rahul and Aiken, Alex},
title = {Semantic Alignment Equivalence Checker},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.2646720},
abstract = {
    <p>An equivalence checker for x86-64 using semantic program alignment, as presented in the 2019 PLDI paper "Semantic Program Alignment for Equivalence Checking".</p>
},
keywords = {equivalence checking, formal verification, x86-64}
}

@inproceedings{10.1145/3314221.3314622,
author = {L\"{o}\"{o}w, Andreas and Kumar, Ramana and Tan, Yong Kiam and Myreen, Magnus O. and Norrish, Michael and Abrahamsson, Oskar and Fox, Anthony},
title = {Verified Compilation on a Verified Processor},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314622},
doi = {10.1145/3314221.3314622},
abstract = {Developing technology for building verified stacks, i.e., computer systems with comprehensive proofs of correctness, is one way the science of programming languages furthers the computing discipline. While there have been successful projects verifying complex, realistic system components, including compilers (software) and processors (hardware), to date these verification efforts have not been compatible to the point of enabling a single end-to-end correctness theorem about running a verified compiler on a verified processor.  In this paper we show how to extend the trustworthy development methodology of the CakeML project, including its verified compiler, with a connection to verified hardware. Our hardware target is Silver, a verified proof-of-concept processor that we introduce here. The result is an approach to producing verified stacks that scales to proving correctness, at the hardware level, of the execution of realistic software including compilers and proof checkers. Alongside our hardware-level theorems, we demonstrate feasibility by hosting and running our verified artefacts on an FPGA board.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1041–1053},
numpages = {13},
keywords = {compiler verification, program verification, verified stack, hardware verification},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314585,
author = {Chajed, Tej and Tassarotti, Joseph and Kaashoek, M. Frans and Zeldovich, Nickolai},
title = {Argosy: Verifying Layered Storage Systems with Recovery Refinement},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314585},
doi = {10.1145/3314221.3314585},
abstract = {Storage systems make persistence guarantees even if the system crashes at any time, which they achieve using recovery procedures that run after a crash. We present Argosy, a framework for machine-checked proofs of storage systems that supports layered recovery implementations with modular proofs. Reasoning about layered recovery procedures is especially challenging because the system can crash in the middle of a more abstract layer’s recovery procedure and must start over with the lowest-level recovery procedure. This paper introduces recovery refinement, a set of conditions that ensure proper implementation of an interface with a recovery procedure. Argosy includes a proof that recovery refinements compose, using Kleene algebra for concise definitions and metatheory. We implemented Crash Hoare Logic, the program logic used by FSCQ, to prove recovery refinement, and demonstrated the whole system by verifying an example of layered recovery featuring a write-ahead log running on top of a disk replication system. The metatheory of the framework, the soundness of the program logic, and these examples are all verified in the Coq proof assistant.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1054–1068},
numpages = {15},
keywords = {Refinement, Kleene Algebra},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.5281/zenodo.2641028,
author = {Chajed, Tej and Tassarotti, Joseph and Kaashoek, M. Frans and Zeldovich, Nickolai},
title = {Argosy: Verifying Layered Storage Systems with Recovery Refinement (Artifact)},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.2641028},
abstract = {
    <p>Artifact for the PLDI 2019 paper "Argosy: Verifying layered storage systems with recovery refinement". Includes the Coq sources for the Argosy framework and the examples we verified using Argosy.</p>
},
keywords = {Coq, Kleene Algebra, Refinement}
}

@inproceedings{10.1145/3314221.3314590,
author = {Gershuni, Elazar and Amit, Nadav and Gurfinkel, Arie and Narodytska, Nina and Navas, Jorge A. and Rinetzky, Noam and Ryzhyk, Leonid and Sagiv, Mooly},
title = {Simple and Precise Static Analysis of Untrusted Linux Kernel Extensions},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314590},
doi = {10.1145/3314221.3314590},
abstract = {Extended Berkeley Packet Filter (eBPF) is a Linux subsystem that allows safely executing untrusted user-defined extensions inside the kernel. It relies on static analysis to protect the kernel against buggy and malicious extensions. As the eBPF ecosystem evolves to support more complex and diverse extensions, the limitations of its current verifier, including high rate of false positives, poor scalability, and lack of support for loops, have become a major barrier for developers.  We design a static analyzer for eBPF within the framework of abstract interpretation. Our choice of abstraction is based on common patterns found in many eBPF programs. We observed that eBPF programs manipulate memory in a rather disciplined way which permits analyzing them successfully with a scalable mixture of very-precise abstraction of certain bounded regions with coarser abstractions of other parts of the memory. We use the Zone domain, a simple domain that tracks differences between pairs of registers and offsets, to achieve precise and scalable analysis. We demonstrate that this abstraction is as precise in practice as more costly abstract domains like Octagon and Polyhedra.  Furthermore, our evaluation, based on hundreds of real-world eBPF programs, shows that the new tool generates no more false alarms than the existing Linux verifier, while it supports a wider class of programs (including programs with loops) and has better asymptotic complexity.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1069–1084},
numpages = {16},
keywords = {ebpf, static analysis, kernel extensions, linux},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.5281/zenodo.2640864,
author = {Gershuni, Elazar and Amit, Nadav and Gurfinkel, Arie and Narodytska, Nina and Navas, Jorge A. and Rinetzky, Noam and Ryzhyk, Leonid and Sagiv, Mooly},
title = {Tool Implementation for Paper: "Simple and Precise Static Analysis of Untrusted Linux Kernel Extensions"},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.2640864},
abstract = {
    <p>C++ implementation of the analyzer described in the paper.</p>
},
keywords = {ebpf, kernel extensions, linux, static analysis}
}

@inproceedings{10.1145/3314221.3314583,
author = {Ruppel, Emily and Lucia, Brandon},
title = {Transactional Concurrency Control for Intermittent, Energy-Harvesting Computing Systems},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314583},
doi = {10.1145/3314221.3314583},
abstract = {Batteryless energy-harvesting devices are computing platforms that operate in environments where batteries are not viable for energy storage. Energy-harvesting devices operate intermittently, only as energy is available. Prior work developed software execution models robust to intermittent power failures but no existing intermittent execution model allows interrupts to update global persistent state without allowing incorrect behavior or requiring complex programming. We present Coati, a system that supports event-driven concurrency via interrupts in an intermittent software execution model. Coati exposes a task-based interface for synchronous computations and an event interface for asynchronous interrupts. Coati supports synchronizing tasks and events using transactions, which allow for multi-task atomic regions that extend across multiple power failures. This work explores two different models for serializing events and tasks that both safely provide intuitive semantics for event-driven intermittent programs. We implement a prototype of Coati as C language extensions and a runtime library. Using energy-harvesting hardware, we evaluate Coati on benchmarks adapted from prior work. We show that Coati prevents failures when interrupts are introduced, while the baseline fails in just seconds. Moreover, Coati operates with a reasonable run time overhead that is often comparable to an idealized baseline.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1085–1100},
numpages = {16},
keywords = {event-driven concurrency, intermittent computing, transactions},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314613,
author = {Maeng, Kiwan and Lucia, Brandon},
title = {Supporting Peripherals in Intermittent Systems with Just-in-Time Checkpoints},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314613},
doi = {10.1145/3314221.3314613},
abstract = {Batteryless energy-harvesting devices have the potential to be the foundation of applications for which batteries are infeasible. Just-In-Time checkpointing supports intermittent execution on energy-harvesting devices by checkpointing processor state right before a power failure. While effective for software execution, Just-In-Time checkpointing remains vulnerable to unrecoverable failures involving peripherals(e.g., sensors and accelerators) because checkpointing during a peripheral operation may lead to inconsistency between peripheral and program state. Additionally, a peripheral operation that uses more energy than a device can buffer never completes, causing non-termination.  This paper presents Samoyed, a Just-In-Time checkpointing system that safely supports peripherals. Samoyed correctly runs user-annotated peripheral functions by selectively disabling checkpoints and undo-logging. Samoyed guarantees progress by energy profiling, dynamic peripheral workload scaling, and a user-provided software fallback routine. Our evaluation shows that Samoyed correctly executes peripheral operations that fail with existing systems, achieving up to 122.9x speedup by using accelerators. Samoyed preserves the performance benefit of Just-In-Time checkpointing, showing 4.11x mean speedup compared to a recent possible alternative. Moreover, Samoyed’s unique ability to profile energy and to dynamically scale large peripheral operations simplifies programming.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1101–1116},
numpages = {16},
keywords = {intermittent computing, energy-harvesting},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3314221.3314649,
author = {Abdulla, Parosh Aziz and Arora, Jatin and Atig, Mohamed Faouzi and Krishna, Shankaranarayanan},
title = {Verification of Programs under the Release-Acquire Semantics},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314649},
doi = {10.1145/3314221.3314649},
abstract = {We address the verification of concurrent programs running under the release-acquire (RA) semantics. We show that the reachability problem is undecidable even in the case where the input program is finite-state. Given this undecidability, we follow the spirit of the work on context-bounded analysis for detecting bugs in programs under the classical SC model, and propose an under-approximate reachability analysis for the case of RA. To this end, we propose a novel notion, called view-switching, and provide a code-to-code translation from an input program under RA to a program under SC. This leads to a reduction, in polynomial time, of the bounded view-switching reachability problem under RA to the bounded context-switching problem under SC. We have implemented a prototype tool VBMC and tested it on a set of benchmarks, demonstrating that many bugs in programs can be found using a small number of view switches.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1117–1132},
numpages = {16},
keywords = {Model-Checking, weak memory models, RA},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.1145/3325988,
author = {Abdulla, Parosh Aziz and Arora, Jatin and Atig, Mohamed Faouzi and Krishna, Shankaranarayanan},
title = {Replication Package for Article: Verification of Programs under Release Acquire Semantics},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3325988},
abstract = {
    <p>The artifact consists of a virtual machine (VM) containing binaries of four model checking tools (VBMC, Cdschecker, Rcmc, and Tracer) along with several scripts in order to reproduce the experimental results in the paper titled  Verification of Concurrent Programs under the Release-Acquire Semantics .</p>
},
keywords = {Bounded Model Checking, Concurrent Program, Release Acquire Semantics, Undecidability, View Bounding}
}

@inproceedings{10.1145/3314221.3314601,
author = {Dasgupta, Sandeep and Park, Daejun and Kasampalis, Theodoros and Adve, Vikram S. and Ro\c{s}u, Grigore},
title = {A Complete Formal Semantics of X86-64 User-Level Instruction Set Architecture},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314601},
doi = {10.1145/3314221.3314601},
abstract = {We present the most complete and thoroughly tested formal semantics of x86-64 to date. Our semantics faithfully formalizes all the non-deprecated, sequential user-level instructions of the x86-64 Haswell instruction set architecture. This totals 3155 instruction variants, corresponding to 774 mnemonics. The semantics is fully executable and has been tested against more than 7,000 instruction-level test cases and the GCC torture test suite. This extensive testing paid off, revealing bugs in both the x86-64 reference manual and other existing semantics. We also illustrate potential applications of our semantics in different formal analyses, and discuss how it can be useful for processor verification.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1133–1148},
numpages = {16},
keywords = {Formal Semantics, x86-64, ISA specification},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@software{10.5281/zenodo.2646617,
author = {Dasgupta, Sandeep and Park, Daejun and Kasampalis, Theodoros and Adve, Vikram S. and Ro�u, Grigore},
title = {Replication Package for the Article "A Complete Formal Semantics of X86-64 User-Level Instruction Set Architecture"},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.2646617},
abstract = {
    <p>We present the most complete and thoroughly tested formal semantics of x86-64 to date. Our semantics faithfully formalizes all the non-deprecated, sequential user-level instructions of the x86-64 Haswell instruction set architecture. This totals 3155 instruction variants, corresponding to 774 mnemonics. The semantics is fully executable and has been tested against more than 7,000 instruction-level test cases and the GCC torture test suite. This extensive testing paid off, revealing bugs in both the x86-64 reference manual and other existing semantics. We also illustrate potential applications of our semantics in different formal analyses, and discuss how it can be useful for processor verification</p>
},
keywords = {Formal Semantics, ISA specification, x86-64}
}

@inproceedings{10.1145/3314221.3314584,
author = {Zhou, Li and Yu, Nengkun and Ying, Mingsheng},
title = {An Applied Quantum Hoare Logic},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314584},
doi = {10.1145/3314221.3314584},
abstract = {We derive a variant of quantum Hoare logic (QHL), called applied quantum Hoare logic (aQHL for short), by: 1. restricting QHL to a special class of preconditions and postconditions, namely projections, which can significantly simplify verification of quantum programs and are much more convenient when used in debugging and testing; and 2. adding several rules for reasoning about robustness of quantum programs, i.e. error bounds of outputs. The effectiveness of aQHL is shown by its applications to verify two sophisticated quantum algorithms: HHL (Harrow-Hassidim-Lloyd) for solving systems of linear equations and qPCA (quantum Principal Component Analysis).},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1149–1162},
numpages = {14},
keywords = {Quantum computation, Hoare logic, robustness, projections, programming languages},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

