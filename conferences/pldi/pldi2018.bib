@software{10.1145/3211981,
author = {Panchekha, Pavel and Geller, Adam T. and Ernst, Michael D. and Tatlock, Zachary and Kamil, Shoaib},
title = {Replication of Evaluation for VizAssert},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3211981},
abstract = {
    <p>Fully reproduces the experiments that are introduced in the paper, including: automatically verifying (or producing counterexamples for) 14 industry-standard assertions on 51 professionally-designed web pages; and testing VizAssert's formalization of line height, margin collapsing, and floating layout against Mozilla Firefox.</p>
},
keywords = {accessibility, CSS, layout, semantics, SMT, usability, verification}
}

@article{10.1145/3296979.3192407,
author = {Panchekha, Pavel and Geller, Adam T. and Ernst, Michael D. and Tatlock, Zachary and Kamil, Shoaib},
title = {Verifying That Web Pages Have Accessible Layout},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192407},
doi = {10.1145/3296979.3192407},
abstract = {Usability and accessibility guidelines aim to make graphical user interfaces accessible to all users, by, say, requiring that text is sufficiently large, interactive controls are visible, and heading size corresponds to importance. These guidelines must hold on the infinitely many possible renderings of a web page generated by differing screen sizes, fonts, and other user preferences. Today, these guidelines are tested by manual inspection of a few renderings, because 1) the guidelines are not expressed in a formal language, 2) the semantics of browser rendering are not well understood, and 3) no tools exist to check all possible renderings of a web page. VizAssert solves these problems. First, it introduces visual logic to precisely specify accessibility properties. Second, it formalizes a large fragment of the browser rendering algorithm using novel finitization reductions. Third, it provides a sound, automated tool for verifying assertions in visual logic.  We encoded 14 assertions drawn from best-practice accessibility and mobile-usability guidelines in visual logic. VizAssert checked them on on 62 professionally designed web pages. It found 64 distinct errors in the web pages, while reporting only 13 false positive warnings.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {1–14},
numpages = {14},
keywords = {usability, SMT, layout, verification, semantics, CSS, accessibility}
}

@inproceedings{10.1145/3192366.3192407,
author = {Panchekha, Pavel and Geller, Adam T. and Ernst, Michael D. and Tatlock, Zachary and Kamil, Shoaib},
title = {Verifying That Web Pages Have Accessible Layout},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192407},
doi = {10.1145/3192366.3192407},
abstract = {Usability and accessibility guidelines aim to make graphical user interfaces accessible to all users, by, say, requiring that text is sufficiently large, interactive controls are visible, and heading size corresponds to importance. These guidelines must hold on the infinitely many possible renderings of a web page generated by differing screen sizes, fonts, and other user preferences. Today, these guidelines are tested by manual inspection of a few renderings, because 1) the guidelines are not expressed in a formal language, 2) the semantics of browser rendering are not well understood, and 3) no tools exist to check all possible renderings of a web page. VizAssert solves these problems. First, it introduces visual logic to precisely specify accessibility properties. Second, it formalizes a large fragment of the browser rendering algorithm using novel finitization reductions. Third, it provides a sound, automated tool for verifying assertions in visual logic.  We encoded 14 assertions drawn from best-practice accessibility and mobile-usability guidelines in visual logic. VizAssert checked them on on 62 professionally designed web pages. It found 64 distinct errors in the web pages, while reporting only 13 false positive warnings.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1–14},
numpages = {14},
keywords = {verification, SMT, layout, accessibility, semantics, usability, CSS},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@software{10.1145/3211982,
author = {Vilk, John and Berger, Emery D.},
title = {Software Artifact for BLeak: Automatically Debugging Memory Leaks in Web Applications},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3211982},
abstract = {
    <p>The artifact consists of a VirtualBox VM image (OVA file) containing the BLeak software and source code, evaluation data, instructions, and all open source evaluation applications and input files.</p>
},
keywords = {BLeak, debugging, JavaScript, leak debugging, leak detection, memory leaks, web applications, web browsers}
}

@article{10.1145/3296979.3192376,
author = {Vilk, John and Berger, Emery D.},
title = {BLeak: Automatically Debugging Memory Leaks in Web Applications},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192376},
doi = {10.1145/3296979.3192376},
abstract = {Despite the presence of garbage collection in managed languages like JavaScript, memory leaks remain a serious problem. In the context of web applications, these leaks are especially pervasive and difficult to debug. Web application memory leaks can take many forms, including failing to dispose of unneeded event listeners, repeatedly injecting iframes and CSS files, and failing to call cleanup routines in third-party libraries. Leaks degrade responsiveness by increasing GC frequency and overhead, and can even lead to browser tab crashes by exhausting available memory. Because previous leak detection approaches designed for conventional C, C++ or Java applications are ineffective in the browser environment, tracking down leaks currently requires intensive manual effort by web developers.  This paper introduces BLeak (Browser Leak debugger), the first system for automatically debugging memory leaks in web applications. BLeak's algorithms leverage the observation that in modern web applications, users often repeatedly return to the same (approximate) visual state (e.g., the inbox view in Gmail). Sustained growth between round trips is a strong indicator of a memory leak. To use BLeak, a developer writes a short script (17-73 LOC on our benchmarks) to drive a web application in round trips to the same visual state. BLeak then automatically generates a list of leaks found along with their root causes, ranked by return on investment. Guided by BLeak, we identify and fix over 50 memory leaks in popular libraries and apps including Airbnb, AngularJS, Google Analytics, Google Maps SDK, and jQuery. BLeak's median precision is 100%; fixing the leaks it identifies reduces heap growth by an average of 94%, saving from 0.5 MB to 8 MB per round trip. We believe BLeak's approach to be broadly applicable beyond web applications, including to GUI applications on desktop and mobile platforms.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {15–29},
numpages = {15},
keywords = {Memory leaks, debugging, JavaScript, leak detection, web development}
}

@inproceedings{10.1145/3192366.3192376,
author = {Vilk, John and Berger, Emery D.},
title = {BLeak: Automatically Debugging Memory Leaks in Web Applications},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192376},
doi = {10.1145/3192366.3192376},
abstract = {Despite the presence of garbage collection in managed languages like JavaScript, memory leaks remain a serious problem. In the context of web applications, these leaks are especially pervasive and difficult to debug. Web application memory leaks can take many forms, including failing to dispose of unneeded event listeners, repeatedly injecting iframes and CSS files, and failing to call cleanup routines in third-party libraries. Leaks degrade responsiveness by increasing GC frequency and overhead, and can even lead to browser tab crashes by exhausting available memory. Because previous leak detection approaches designed for conventional C, C++ or Java applications are ineffective in the browser environment, tracking down leaks currently requires intensive manual effort by web developers.  This paper introduces BLeak (Browser Leak debugger), the first system for automatically debugging memory leaks in web applications. BLeak's algorithms leverage the observation that in modern web applications, users often repeatedly return to the same (approximate) visual state (e.g., the inbox view in Gmail). Sustained growth between round trips is a strong indicator of a memory leak. To use BLeak, a developer writes a short script (17-73 LOC on our benchmarks) to drive a web application in round trips to the same visual state. BLeak then automatically generates a list of leaks found along with their root causes, ranked by return on investment. Guided by BLeak, we identify and fix over 50 memory leaks in popular libraries and apps including Airbnb, AngularJS, Google Analytics, Google Maps SDK, and jQuery. BLeak's median precision is 100%; fixing the leaks it identifies reduces heap growth by an average of 94%, saving from 0.5 MB to 8 MB per round trip. We believe BLeak's approach to be broadly applicable beyond web applications, including to GUI applications on desktop and mobile platforms.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {15–29},
numpages = {15},
keywords = {web development, leak detection, debugging, Memory leaks, JavaScript},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192370,
author = {Baxter, Samuel and Nigam, Rachit and Politz, Joe Gibbs and Krishnamurthi, Shriram and Guha, Arjun},
title = {Putting in All the Stops: Execution Control for JavaScript},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192370},
doi = {10.1145/3296979.3192370},
abstract = {Scores of compilers produce JavaScript, enabling programmers to use many languages on the Web, reuse existing code, and even use Web IDEs. Unfortunately, most compilers inherit the browser's compromised execution model, so long-running programs freeze the browser tab, infinite loops crash IDEs, and so on. The few compilers that avoid these problems suffer poor performance and are difficult to engineer.  This paper presents Stopify, a source-to-source compiler that extends JavaScript with debugging abstractions and blocking operations, and easily integrates with existing compilers. We apply Stopify to ten programming languages and develop a Web IDE that supports stopping, single-stepping, breakpointing, and long-running computations. For nine languages, Stopify requires no or trivial compiler changes. For eight, our IDE is the first that provides these features. Two of our subject languages have compilers with similar features. Stopify's performance is competitive with these compilers and it makes them dramatically simpler.  Stopify's abstractions rely on first-class continuations, which it provides by compiling JavaScript to JavaScript. We also identify sub-languages of JavaScript that compilers implicitly use, and exploit these to improve performance. Finally, Stopify needs to repeatedly interrupt and resume program execution. We use a sampling-based technique to estimate program speed that outperforms other systems.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {30–45},
numpages = {16},
keywords = {continuations, JavaScript, IDEs}
}

@inproceedings{10.1145/3192366.3192370,
author = {Baxter, Samuel and Nigam, Rachit and Politz, Joe Gibbs and Krishnamurthi, Shriram and Guha, Arjun},
title = {Putting in All the Stops: Execution Control for JavaScript},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192370},
doi = {10.1145/3192366.3192370},
abstract = {Scores of compilers produce JavaScript, enabling programmers to use many languages on the Web, reuse existing code, and even use Web IDEs. Unfortunately, most compilers inherit the browser's compromised execution model, so long-running programs freeze the browser tab, infinite loops crash IDEs, and so on. The few compilers that avoid these problems suffer poor performance and are difficult to engineer.  This paper presents Stopify, a source-to-source compiler that extends JavaScript with debugging abstractions and blocking operations, and easily integrates with existing compilers. We apply Stopify to ten programming languages and develop a Web IDE that supports stopping, single-stepping, breakpointing, and long-running computations. For nine languages, Stopify requires no or trivial compiler changes. For eight, our IDE is the first that provides these features. Two of our subject languages have compilers with similar features. Stopify's performance is competitive with these compilers and it makes them dramatically simpler.  Stopify's abstractions rely on first-class continuations, which it provides by compiling JavaScript to JavaScript. We also identify sub-languages of JavaScript that compilers implicitly use, and exploit these to improve performance. Finally, Stopify needs to repeatedly interrupt and resume program execution. We use a sampling-based technique to estimate program speed that outperforms other systems.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {30–45},
numpages = {16},
keywords = {IDEs, continuations, JavaScript},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192367,
author = {Gogte, Vaibhav and Diestelhorst, Stephan and Wang, William and Narayanasamy, Satish and Chen, Peter M. and Wenisch, Thomas F.},
title = {Persistency for Synchronization-Free Regions},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192367},
doi = {10.1145/3296979.3192367},
abstract = {Nascent persistent memory (PM) technologies promise the performance of DRAM with the durability of disk, but how best to integrate them into programming systems remains an open question. Recent work extends language memory models with a persistency model prescribing semantics for updates to PM. These semantics enable programmers to design data structures in PM that are accessed like memory and yet are recoverable upon crash or failure. Alas, we find the semantics and performance of existing approaches unsatisfying. Existing approaches require high-overhead mechanisms, are restricted to certain synchronization constructs, provide incomplete semantics, and/or may recover to state that cannot arise in fault-free execution.  We propose persistency semantics that guarantee failure atomicity of synchronization-free regions (SFRs) - program regions delimited by synchronization operations. Our approach provides clear semantics for the PM state recovery code may observe and extends C++11's "sequential consistency for data-race-free" guarantee to post-failure recovery code. We investigate two designs for failure-atomic SFRs that vary in performance and the degree to which commit of persistent state may lag execution. We demonstrate both approaches in LLVM v3.6.0 and compare to a state-of-the-art baseline to show performance improvement up to 87.5% (65.5% avg).},
journal = {SIGPLAN Not.},
month = {jun},
pages = {46–61},
numpages = {16},
keywords = {Persistent memories, failure-atomicity, persistency models, language memory models, synchronization-free regions}
}

@inproceedings{10.1145/3192366.3192367,
author = {Gogte, Vaibhav and Diestelhorst, Stephan and Wang, William and Narayanasamy, Satish and Chen, Peter M. and Wenisch, Thomas F.},
title = {Persistency for Synchronization-Free Regions},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192367},
doi = {10.1145/3192366.3192367},
abstract = {Nascent persistent memory (PM) technologies promise the performance of DRAM with the durability of disk, but how best to integrate them into programming systems remains an open question. Recent work extends language memory models with a persistency model prescribing semantics for updates to PM. These semantics enable programmers to design data structures in PM that are accessed like memory and yet are recoverable upon crash or failure. Alas, we find the semantics and performance of existing approaches unsatisfying. Existing approaches require high-overhead mechanisms, are restricted to certain synchronization constructs, provide incomplete semantics, and/or may recover to state that cannot arise in fault-free execution.  We propose persistency semantics that guarantee failure atomicity of synchronization-free regions (SFRs) - program regions delimited by synchronization operations. Our approach provides clear semantics for the PM state recovery code may observe and extends C++11's "sequential consistency for data-race-free" guarantee to post-failure recovery code. We investigate two designs for failure-atomic SFRs that vary in performance and the degree to which commit of persistent state may lag execution. We demonstrate both approaches in LLVM v3.6.0 and compare to a state-of-the-art baseline to show performance improvement up to 87.5% (65.5% avg).},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {46–61},
numpages = {16},
keywords = {Persistent memories, persistency models, synchronization-free regions, failure-atomicity, language memory models},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192392,
author = {Akram, Shoaib and Sartor, Jennifer B. and McKinley, Kathryn S. and Eeckhout, Lieven},
title = {Write-Rationing Garbage Collection for Hybrid Memories},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192392},
doi = {10.1145/3296979.3192392},
abstract = {Emerging Non-Volatile Memory (NVM) technologies offer high capacity and energy efficiency compared to DRAM, but suffer from limited write endurance and longer latencies. Prior work seeks the best of both technologies by combining DRAM and NVM in hybrid memories to attain low latency, high capacity, energy efficiency, and durability. Coarsegrained hardware and OS optimizations then spread writes out (wear-leveling) and place highly mutated pages in DRAM to extend NVM lifetimes. Unfortunately even with these coarse-grained methods, popular Java applications exact impractical NVM lifetimes of 4 years or less. This paper shows how to make hybrid memories practical, without changing the programming model, by enhancing garbage collection in managed language runtimes. We find object write behaviors offer two opportunities: (1) 70% of writes occur to newly allocated objects, and (2) 2% of objects capture 81% of writes to mature objects. We introduce writerationing garbage collectors that exploit these fine-grained behaviors. They extend NVM lifetimes by placing highly mutated objects in DRAM and read-mostly objects in NVM. We implement two such systems. (1) Kingsguard-nursery places new allocation in DRAM and survivors in NVM, reducing NVM writes by 5\texttimes{} versus NVM only with wear-leveling. (2) Kingsguard-writers (KG-W) places nursery objects in DRAM and survivors in a DRAM observer space. It monitors all mature object writes and moves unwritten mature objects from DRAM to NVM. Because most mature objects are unwritten, KG-W exploits NVM capacity while increasing NVM lifetimes by 11\texttimes{}. It reduces the energy-delay product by 32% over DRAM-only and 29% over NVM-only. This work opens up new avenues for making hybrid memories practical.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {62–77},
numpages = {16},
keywords = {Managed runtimes, Non-volatile memory (NVM), Garbage collection, Hybrid DRAM-NVM memories}
}

@inproceedings{10.1145/3192366.3192392,
author = {Akram, Shoaib and Sartor, Jennifer B. and McKinley, Kathryn S. and Eeckhout, Lieven},
title = {Write-Rationing Garbage Collection for Hybrid Memories},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192392},
doi = {10.1145/3192366.3192392},
abstract = {Emerging Non-Volatile Memory (NVM) technologies offer high capacity and energy efficiency compared to DRAM, but suffer from limited write endurance and longer latencies. Prior work seeks the best of both technologies by combining DRAM and NVM in hybrid memories to attain low latency, high capacity, energy efficiency, and durability. Coarsegrained hardware and OS optimizations then spread writes out (wear-leveling) and place highly mutated pages in DRAM to extend NVM lifetimes. Unfortunately even with these coarse-grained methods, popular Java applications exact impractical NVM lifetimes of 4 years or less. This paper shows how to make hybrid memories practical, without changing the programming model, by enhancing garbage collection in managed language runtimes. We find object write behaviors offer two opportunities: (1) 70% of writes occur to newly allocated objects, and (2) 2% of objects capture 81% of writes to mature objects. We introduce writerationing garbage collectors that exploit these fine-grained behaviors. They extend NVM lifetimes by placing highly mutated objects in DRAM and read-mostly objects in NVM. We implement two such systems. (1) Kingsguard-nursery places new allocation in DRAM and survivors in NVM, reducing NVM writes by 5\texttimes{} versus NVM only with wear-leveling. (2) Kingsguard-writers (KG-W) places nursery objects in DRAM and survivors in a DRAM observer space. It monitors all mature object writes and moves unwritten mature objects from DRAM to NVM. Because most mature objects are unwritten, KG-W exploits NVM capacity while increasing NVM lifetimes by 11\texttimes{}. It reduces the energy-delay product by 32% over DRAM-only and 29% over NVM-only. This work opens up new avenues for making hybrid memories practical.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {62–77},
numpages = {16},
keywords = {Managed runtimes, Hybrid DRAM-NVM memories, Garbage collection, Non-volatile memory (NVM)},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192371,
author = {Lin, Chit-Kwan and Wild, Andreas and Chinya, Gautham N. and Lin, Tsung-Han and Davies, Mike and Wang, Hong},
title = {Mapping Spiking Neural Networks onto a Manycore Neuromorphic Architecture},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192371},
doi = {10.1145/3296979.3192371},
abstract = {We present a compiler for Loihi, a novel manycore neuromorphic processor that features a programmable, on-chip learning engine for training and executing spiking neural networks (SNNs). An SNN is distinguished from other neural networks in that (1) its independent computing units, or "neurons", communicate with others only through spike messages; and (2) each neuron evaluates local learning rules, which are functions of spike arrival and departure timings, to modify its local state. The collective neuronal state dynamics of an SNN form a nonlinear dynamical system that can be cast as an unconventional model of computation. To realize such an SNN on Loihi requires each constituent neuron to locally store and independently update its own spike timing information. However, each Loihi core has limited resources for this purpose and these must be shared by neurons assigned to the same core. In this work, we present a compiler for Loihi that maps the neurons of an SNN onto and across Loihi's cores efficiently. We show that a poor neuron-to-core mapping can incur significant energy costs and address this with a greedy algorithm that compiles SNNs onto Loihi in a power-efficient manner. In so doing, we highlight the need for further development of compilers for this new, emerging class of architectures.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {78–89},
numpages = {12},
keywords = {neuromorphic computing, small-world networks, spiking neural networks, compilers, neuromorphic architectures}
}

@inproceedings{10.1145/3192366.3192371,
author = {Lin, Chit-Kwan and Wild, Andreas and Chinya, Gautham N. and Lin, Tsung-Han and Davies, Mike and Wang, Hong},
title = {Mapping Spiking Neural Networks onto a Manycore Neuromorphic Architecture},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192371},
doi = {10.1145/3192366.3192371},
abstract = {We present a compiler for Loihi, a novel manycore neuromorphic processor that features a programmable, on-chip learning engine for training and executing spiking neural networks (SNNs). An SNN is distinguished from other neural networks in that (1) its independent computing units, or "neurons", communicate with others only through spike messages; and (2) each neuron evaluates local learning rules, which are functions of spike arrival and departure timings, to modify its local state. The collective neuronal state dynamics of an SNN form a nonlinear dynamical system that can be cast as an unconventional model of computation. To realize such an SNN on Loihi requires each constituent neuron to locally store and independently update its own spike timing information. However, each Loihi core has limited resources for this purpose and these must be shared by neurons assigned to the same core. In this work, we present a compiler for Loihi that maps the neurons of an SNN onto and across Loihi's cores efficiently. We show that a poor neuron-to-core mapping can incur significant energy costs and address this with a greedy algorithm that compiles SNNs onto Loihi in a power-efficient manner. In so doing, we highlight the need for further development of compilers for this new, emerging class of architectures.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {78–89},
numpages = {12},
keywords = {compilers, neuromorphic computing, spiking neural networks, neuromorphic architectures, small-world networks},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@software{10.1145/3211984,
author = {Brutschy, Lucas and Dimitrov, Dimitar and M\"{u}ller, Peter and Vechev, Martin},
title = {C4 Tool Source Code},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3211984},
abstract = {
    <p>This is the source code for the C4 tool that we developed and reported in our "Static Serializability Analysis for Causal Consistency" PLDI '18 paper. For more information, check out the project home page at http://ecracer.inf.ethz.ch/.</p>
},
keywords = {Atomic Visibility, Causal Consistency, Serializability, Static Analysis}
}

@article{10.1145/3296979.3192415,
author = {Brutschy, Lucas and Dimitrov, Dimitar and M\"{u}ller, Peter and Vechev, Martin},
title = {Static Serializability Analysis for Causal Consistency},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192415},
doi = {10.1145/3296979.3192415},
abstract = {Many distributed databases provide only weak consistency guarantees to reduce synchronization overhead and remain available under network partitions. However, this leads to behaviors not possible under stronger guarantees. Such behaviors can easily defy programmer intuition and lead to errors that are notoriously hard to detect.  In this paper, we propose a static analysis for detecting non-serializable behaviors of applications running on top of causally-consistent databases. Our technique is based on a novel, local serializability criterion and combines a generalization of graph-based techniques from the database literature with another, complementary analysis technique that encodes our serializability criterion into first-order logic formulas to be checked by an SMT solver. This analysis is more expensive yet more precise and produces concrete counter-examples.  We implemented our methods and evaluated them on a number of applications from two different domains: cloud-backed mobile applications and clients of a distributed database. Our experiments demonstrate that our analysis is able to detect harmful serializability violations while producing only a small number of false alarms.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {90–104},
numpages = {15},
keywords = {serializability, causal consistency, static analysis}
}

@inproceedings{10.1145/3192366.3192415,
author = {Brutschy, Lucas and Dimitrov, Dimitar and M\"{u}ller, Peter and Vechev, Martin},
title = {Static Serializability Analysis for Causal Consistency},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192415},
doi = {10.1145/3192366.3192415},
abstract = {Many distributed databases provide only weak consistency guarantees to reduce synchronization overhead and remain available under network partitions. However, this leads to behaviors not possible under stronger guarantees. Such behaviors can easily defy programmer intuition and lead to errors that are notoriously hard to detect.  In this paper, we propose a static analysis for detecting non-serializable behaviors of applications running on top of causally-consistent databases. Our technique is based on a novel, local serializability criterion and combines a generalization of graph-based techniques from the database literature with another, complementary analysis technique that encodes our serializability criterion into first-order logic formulas to be checked by an SMT solver. This analysis is more expensive yet more precise and produces concrete counter-examples.  We implemented our methods and evaluated them on a number of applications from two different domains: cloud-backed mobile applications and clients of a distributed database. Our experiments demonstrate that our analysis is able to detect harmful serializability violations while producing only a small number of false alarms.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {90–104},
numpages = {15},
keywords = {serializability, causal consistency, static analysis},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@software{10.5281/zenodo.1218718,
author = {Liu, Peizun and Wahl, Thomas},
title = {CUBA: Interprocedural Context-Unbounded Analysis of Concurrent Programs (Artifact)},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.1218718},
abstract = {
    <p>This is the artifact package for the article: CUBA: Interprocedural Context-Unbounded Analysis of Concurrent Programs. It includes: 1. The tool implemented in the article. In particular, -- the source code, -- executable binaries, -- an installation guide, -- documentation. 2. A brief introduction to the syntax used in our input programs. 3. A brief tutorial. 4. A set of benchmarks used and a guide to run the experiments.</p>
},
keywords = {Concurrent Program, Context Bound, Interprocedural Analysis, Recursion, Stack}
}

@article{10.1145/3296979.3192419,
author = {Liu, Peizun and Wahl, Thomas},
title = {CUBA: Interprocedural Context-UnBounded Analysis of Concurrent Programs},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192419},
doi = {10.1145/3296979.3192419},
abstract = {A classical result by Ramalingam about synchronization-sensitive interprocedural program analysis implies that reachability for concurrent threads running recursive procedures is undecidable. A technique proposed by Qadeer and Rehof, to bound the number of context switches allowed between the threads, leads to an incomplete solution that is, however, believed to catch “most bugs” in practice. The question whether the technique can also prove the absence of bugs at least in some cases has remained largely open. In this paper we introduce a broad verification methodology for resource-parameterized programs that observes how changes to the resource parameter affect the behavior of the program. Applied to the context-unbounded analysis problem (CUBA), the methodology results in partial verification techniques for procedural concurrent programs. Our solutions may not terminate, but are able to both refute and prove context-unbounded safety for concurrent recursive threads. We demonstrate the effectiveness of our method using a variety of examples, the safe of which cannot be proved safe by earlier, context-bounded methods.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {105–119},
numpages = {15},
keywords = {Concurrent Program, Recursion, Stack, Context Bound, Interprocedural Analysis}
}

@inproceedings{10.1145/3192366.3192419,
author = {Liu, Peizun and Wahl, Thomas},
title = {CUBA: Interprocedural Context-UnBounded Analysis of Concurrent Programs},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192419},
doi = {10.1145/3192366.3192419},
abstract = {A classical result by Ramalingam about synchronization-sensitive interprocedural program analysis implies that reachability for concurrent threads running recursive procedures is undecidable. A technique proposed by Qadeer and Rehof, to bound the number of context switches allowed between the threads, leads to an incomplete solution that is, however, believed to catch “most bugs” in practice. The question whether the technique can also prove the absence of bugs at least in some cases has remained largely open. In this paper we introduce a broad verification methodology for resource-parameterized programs that observes how changes to the resource parameter affect the behavior of the program. Applied to the context-unbounded analysis problem (CUBA), the methodology results in partial verification techniques for procedural concurrent programs. Our solutions may not terminate, but are able to both refute and prove context-unbounded safety for concurrent recursive threads. We demonstrate the effectiveness of our method using a variety of examples, the safe of which cannot be proved safe by earlier, context-bounded methods.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {105–119},
numpages = {15},
keywords = {Interprocedural Analysis, Recursion, Concurrent Program, Context Bound, Stack},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192395,
author = {Ferles, Kostas and Van Geffen, Jacob and Dillig, Isil and Smaragdakis, Yannis},
title = {Symbolic Reasoning for Automatic Signal Placement},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192395},
doi = {10.1145/3296979.3192395},
abstract = {Explicit signaling between threads is a perennial cause of bugs in concurrent programs. While there are several run-time techniques to automatically notify threads upon the availability of some shared resource, such techniques are not widely-adopted due to their run-time overhead. This paper proposes a new solution based on static analysis for automatically generating a performant explicit-signal program from its corresponding implicit-signal implementation. The key idea is to generate verification conditions that allow us to minimize the number of required signals and unnecessary context switches, while guaranteeing semantic equivalence between the source and target programs. We have implemented our method in a tool called Expresso and evaluate it on challenging benchmarks from prior papers and open-source software. Expresso-generated code significantly outperforms past automatic signaling mechanisms (avg. 1.56x speedup) and closely matches the performance of hand-optimized explicit-signal code.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {120–134},
numpages = {15},
keywords = {monitor invariant, symbolic reasoning, Implicit signal monitors, verification conditions, abductive reasoning, concurrent programming}
}

@inproceedings{10.1145/3192366.3192395,
author = {Ferles, Kostas and Van Geffen, Jacob and Dillig, Isil and Smaragdakis, Yannis},
title = {Symbolic Reasoning for Automatic Signal Placement},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192395},
doi = {10.1145/3192366.3192395},
abstract = {Explicit signaling between threads is a perennial cause of bugs in concurrent programs. While there are several run-time techniques to automatically notify threads upon the availability of some shared resource, such techniques are not widely-adopted due to their run-time overhead. This paper proposes a new solution based on static analysis for automatically generating a performant explicit-signal program from its corresponding implicit-signal implementation. The key idea is to generate verification conditions that allow us to minimize the number of required signals and unnecessary context switches, while guaranteeing semantic equivalence between the source and target programs. We have implemented our method in a tool called Expresso and evaluate it on challenging benchmarks from prior papers and open-source software. Expresso-generated code significantly outperforms past automatic signaling mechanisms (avg. 1.56x speedup) and closely matches the performance of hand-optimized explicit-signal code.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {120–134},
numpages = {15},
keywords = {Implicit signal monitors, verification conditions, abductive reasoning, symbolic reasoning, concurrent programming, monitor invariant},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192405,
author = {Chen, Yu-Fang and Heizmann, Matthias and Leng\'{a}l, Ond\v{r}ej and Li, Yong and Tsai, Ming-Hsien and Turrini, Andrea and Zhang, Lijun},
title = {Advanced Automata-Based Algorithms for Program Termination Checking},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192405},
doi = {10.1145/3296979.3192405},
abstract = {In 2014, Heizmann et al. proposed a novel framework for program termination analysis. The analysis starts with a termination proof of a sample path. The path is generalized to a B\"{u}chi automaton (BA) whose language (by construction) represents a set of terminating paths. All these paths can be safely removed from the program. The removal of paths is done using automata difference, implemented via BA complementation and intersection. The analysis constructs in this way a set of BAs that jointly "cover" the behavior of the program, thus proving its termination. An implementation of the approach in Ultimate Automizer won the 1st place in the Termination category of SV-COMP 2017.  In this paper, we exploit advanced automata-based algorithms and propose several non-trivial improvements of the framework. To alleviate the complementation computation for BAs---one of the most expensive operations in the framework---, we propose a multi-stage generalization construction. We start with generalizations producing subclasses of BAs (such as deterministic BAs) for which efficient complementation algorithms are known, and proceed to more general classes only if necessary. Particularly, we focus on the quite expressive subclass of semideterministic BAs and provide an improved complementation algorithm for this class. Our experimental evaluation shows that the proposed approach significantly improves the power of termination checking within the Ultimate Automizer framework.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {135–150},
numpages = {16},
keywords = {B\"{u}chi Automata Complementation and Language Difference, Program Termination}
}

@inproceedings{10.1145/3192366.3192405,
author = {Chen, Yu-Fang and Heizmann, Matthias and Leng\'{a}l, Ond\v{r}ej and Li, Yong and Tsai, Ming-Hsien and Turrini, Andrea and Zhang, Lijun},
title = {Advanced Automata-Based Algorithms for Program Termination Checking},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192405},
doi = {10.1145/3192366.3192405},
abstract = {In 2014, Heizmann et al. proposed a novel framework for program termination analysis. The analysis starts with a termination proof of a sample path. The path is generalized to a B\"{u}chi automaton (BA) whose language (by construction) represents a set of terminating paths. All these paths can be safely removed from the program. The removal of paths is done using automata difference, implemented via BA complementation and intersection. The analysis constructs in this way a set of BAs that jointly "cover" the behavior of the program, thus proving its termination. An implementation of the approach in Ultimate Automizer won the 1st place in the Termination category of SV-COMP 2017.  In this paper, we exploit advanced automata-based algorithms and propose several non-trivial improvements of the framework. To alleviate the complementation computation for BAs---one of the most expensive operations in the framework---, we propose a multi-stage generalization construction. We start with generalizations producing subclasses of BAs (such as deterministic BAs) for which efficient complementation algorithms are known, and proceed to more general classes only if necessary. Particularly, we focus on the quite expressive subclass of semideterministic BAs and provide an improved complementation algorithm for this class. Our experimental evaluation shows that the proposed approach significantly improves the power of termination checking within the Ultimate Automizer framework.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {135–150},
numpages = {16},
keywords = {Program Termination, B\"{u}chi Automata Complementation and Language Difference},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192374,
author = {Ottoni, Guilherme},
title = {HHVM JIT: A Profile-Guided, Region-Based Compiler for PHP and Hack},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192374},
doi = {10.1145/3296979.3192374},
abstract = {Dynamic languages such as PHP, JavaScript, Python, and Ruby have been gaining popularity over the last two decades. A very popular domain for these languages is web development, including server-side development of large-scale websites. As a result, improving the performance of these languages has become more important. Efficiently compiling programs in these languages is challenging, and many popular dynamic languages still lack efficient production-quality implementations. This paper describes the design of the second generation of the HHVM JIT and how it addresses the challenges to efficiently execute PHP and Hack programs. This new design uses profiling to build an aggressive region-based JIT compiler. We discuss the benefits of this approach compared to the more popular method-based and trace-based approaches to compile dynamic languages. Our evaluation running a very large PHP-based code base, the Facebook website, demonstrates the effectiveness of the new JIT design.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {151–165},
numpages = {15},
keywords = {Hack, PHP, web server applications, code optimizations, dynamic languages, profile-guided optimizations, region-based compilation}
}

@inproceedings{10.1145/3192366.3192374,
author = {Ottoni, Guilherme},
title = {HHVM JIT: A Profile-Guided, Region-Based Compiler for PHP and Hack},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192374},
doi = {10.1145/3192366.3192374},
abstract = {Dynamic languages such as PHP, JavaScript, Python, and Ruby have been gaining popularity over the last two decades. A very popular domain for these languages is web development, including server-side development of large-scale websites. As a result, improving the performance of these languages has become more important. Efficiently compiling programs in these languages is challenging, and many popular dynamic languages still lack efficient production-quality implementations. This paper describes the design of the second generation of the HHVM JIT and how it addresses the challenges to efficiently execute PHP and Hack programs. This new design uses profiling to build an aggressive region-based JIT compiler. We discuss the benefits of this approach compared to the more popular method-based and trace-based approaches to compile dynamic languages. Our evaluation running a very large PHP-based code base, the Facebook website, demonstrates the effectiveness of the new JIT design.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {151–165},
numpages = {15},
keywords = {Hack, code optimizations, profile-guided optimizations, PHP, web server applications, dynamic languages, region-based compilation},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192396,
author = {D'Elia, Daniele Cono and Demetrescu, Camil},
title = {On-Stack Replacement, Distilled},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192396},
doi = {10.1145/3296979.3192396},
abstract = {On-stack replacement (OSR) is essential technology for adaptive optimization, allowing changes to code actively executing in a managed runtime. The engineering aspects of OSR are well-known among VM architects, with several implementations available to date. However, OSR is yet to be explored as a general means to transfer execution between related program versions, which can pave the road to unprecedented applications that stretch beyond VMs. We aim at filling this gap with a constructive and provably correct OSR framework, allowing a class of general-purpose transformation functions to yield a special-purpose replacement. We describe and evaluate an implementation of our technique in LLVM. As a novel application of OSR, we present a feasibility study on debugging of optimized code, showing how our techniques can be used to fix variables holding incorrect values at breakpoints due to optimizations.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {166–180},
numpages = {15},
keywords = {deoptimization, dynamic compilers, debugging}
}

@inproceedings{10.1145/3192366.3192396,
author = {D'Elia, Daniele Cono and Demetrescu, Camil},
title = {On-Stack Replacement, Distilled},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192396},
doi = {10.1145/3192366.3192396},
abstract = {On-stack replacement (OSR) is essential technology for adaptive optimization, allowing changes to code actively executing in a managed runtime. The engineering aspects of OSR are well-known among VM architects, with several implementations available to date. However, OSR is yet to be explored as a general means to transfer execution between related program versions, which can pave the road to unprecedented applications that stretch beyond VMs. We aim at filling this gap with a constructive and provably correct OSR framework, allowing a class of general-purpose transformation functions to yield a special-purpose replacement. We describe and evaluate an implementation of our technique in LLVM. As a novel application of OSR, we present a feasibility study on debugging of optimized code, showing how our techniques can be used to fix variables holding incorrect values at breakpoints due to optimizations.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {166–180},
numpages = {15},
keywords = {deoptimization, dynamic compilers, debugging},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192388,
author = {Duck, Gregory J. and Yap, Roland H. C.},
title = {EffectiveSan: Type and Memory Error Detection Using Dynamically Typed C/C++},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192388},
doi = {10.1145/3296979.3192388},
abstract = {Low-level programming languages with weak/static type systems, such as C and C++, are vulnerable to errors relating to the misuse of memory at runtime, such as (sub-)object bounds overflows, (re)use-after-free, and type confusion. Such errors account for many security and other undefined behavior bugs for programs written in these languages. In this paper, we introduce the notion of dynamically typed C/C++, which aims to detect such errors by dynamically checking the "effective type" of each object before use at runtime. We also present an implementation of dynamically typed C/C++ in the form of the Effective Type Sanitizer (EffectiveSan). EffectiveSan enforces type and memory safety using a combination of low-fat pointers, type meta data and type/bounds check instrumentation. We evaluate EffectiveSan against the SPEC2006 benchmark suite and the Firefox web browser, and detect several new type and memory errors. We also show that EffectiveSan achieves high compatibility and reasonable overheads for the given error coverage. Finally, we highlight that EffectiveSan is one of only a few tools that can detect sub-object bounds errors, and uses a novel approach (dynamic type checking) to do so.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {181–195},
numpages = {15},
keywords = {low-fat pointers, dynamic types, bounds checking, Type errors, C++, C, use-after-free errors, type checking, (sub-)object bounds errors, type confusion, sanitizers, memory errors}
}

@inproceedings{10.1145/3192366.3192388,
author = {Duck, Gregory J. and Yap, Roland H. C.},
title = {EffectiveSan: Type and Memory Error Detection Using Dynamically Typed C/C++},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192388},
doi = {10.1145/3192366.3192388},
abstract = {Low-level programming languages with weak/static type systems, such as C and C++, are vulnerable to errors relating to the misuse of memory at runtime, such as (sub-)object bounds overflows, (re)use-after-free, and type confusion. Such errors account for many security and other undefined behavior bugs for programs written in these languages. In this paper, we introduce the notion of dynamically typed C/C++, which aims to detect such errors by dynamically checking the "effective type" of each object before use at runtime. We also present an implementation of dynamically typed C/C++ in the form of the Effective Type Sanitizer (EffectiveSan). EffectiveSan enforces type and memory safety using a combination of low-fat pointers, type meta data and type/bounds check instrumentation. We evaluate EffectiveSan against the SPEC2006 benchmark suite and the Firefox web browser, and detect several new type and memory errors. We also show that EffectiveSan achieves high compatibility and reasonable overheads for the given error coverage. Finally, we highlight that EffectiveSan is one of only a few tools that can detect sub-object bounds errors, and uses a novel approach (dynamic type checking) to do so.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {181–195},
numpages = {15},
keywords = {type confusion, bounds checking, type checking, Type errors, sanitizers, C++, memory errors, C, low-fat pointers, (sub-)object bounds errors, dynamic types, use-after-free errors},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192378,
author = {Cai, Cheng and Zhang, Qirun and Zuo, Zhiqiang and Nguyen, Khanh and Xu, Guoqing and Su, Zhendong},
title = {Calling-to-Reference Context Translation via Constraint-Guided CFL-Reachability},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192378},
doi = {10.1145/3296979.3192378},
abstract = {A calling context is an important piece of information used widely to help developers understand program executions (e.g., for debugging). While calling contexts offer useful control information, information regarding data involved in a bug (e.g., what data structure holds a leaking object), in many cases, can bring developers closer to the bug's root cause. Such data information, often exhibited as heap reference paths, has already been needed by many tools.  The only way for a dynamic analysis to record complete reference paths is to perform heap dumping, which incurs huge runtime overhead and renders the analysis impractical. This paper presents a novel static analysis that can precisely infer, from a calling context of a method that contains a use (e.g., read or write) of an object, the heap reference paths leading to the object at the time the use occurs. Since calling context recording is much less expensive, our technique provides benefits for all dynamic techniques that need heap information, significantly reducing their overhead.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {196–210},
numpages = {15},
keywords = {dynamic analysis, Static analysis, heap dump}
}

@inproceedings{10.1145/3192366.3192378,
author = {Cai, Cheng and Zhang, Qirun and Zuo, Zhiqiang and Nguyen, Khanh and Xu, Guoqing and Su, Zhendong},
title = {Calling-to-Reference Context Translation via Constraint-Guided CFL-Reachability},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192378},
doi = {10.1145/3192366.3192378},
abstract = {A calling context is an important piece of information used widely to help developers understand program executions (e.g., for debugging). While calling contexts offer useful control information, information regarding data involved in a bug (e.g., what data structure holds a leaking object), in many cases, can bring developers closer to the bug's root cause. Such data information, often exhibited as heap reference paths, has already been needed by many tools.  The only way for a dynamic analysis to record complete reference paths is to perform heap dumping, which incurs huge runtime overhead and renders the analysis impractical. This paper presents a novel static analysis that can precisely infer, from a calling context of a method that contains a use (e.g., read or write) of an object, the heap reference paths leading to the object at the time the use occurs. Since calling context recording is much less expensive, our technique provides benefits for all dynamic techniques that need heap information, significantly reducing their overhead.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {196–210},
numpages = {15},
keywords = {heap dump, Static analysis, dynamic analysis},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192373,
author = {Chong, Nathan and Sorensen, Tyler and Wickerson, John},
title = {The Semantics of Transactions and Weak Memory in X86, Power, ARM, and C++},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192373},
doi = {10.1145/3296979.3192373},
abstract = {Weak memory models provide a complex, system-centric semantics for concurrent programs, while transactional memory (TM) provides a simpler, programmer-centric semantics. Both have been studied in detail, but their combined semantics is not well understood. This is problematic because such widely-used architectures and languages as x86, Power, and C++ all support TM, and all have weak memory models.  Our work aims to clarify the interplay between weak memory and TM by extending existing axiomatic weak memory models (x86, Power, ARMv8, and C++) with new rules for TM. Our formal models are backed by automated tooling that enables (1) the synthesis of tests for validating our models against existing implementations and (2) the model-checking of TM-related transformations, such as lock elision and compiling C++ transactions to hardware. A key finding is that a proposed TM extension to ARMv8 currently being considered within ARM Research is incompatible with lock elision without sacrificing portability or performance.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {211–225},
numpages = {15},
keywords = {Shared Memory Concurrency, Weak Memory, Program Synthesis, Transactional Memory}
}

@inproceedings{10.1145/3192366.3192373,
author = {Chong, Nathan and Sorensen, Tyler and Wickerson, John},
title = {The Semantics of Transactions and Weak Memory in X86, Power, ARM, and C++},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192373},
doi = {10.1145/3192366.3192373},
abstract = {Weak memory models provide a complex, system-centric semantics for concurrent programs, while transactional memory (TM) provides a simpler, programmer-centric semantics. Both have been studied in detail, but their combined semantics is not well understood. This is problematic because such widely-used architectures and languages as x86, Power, and C++ all support TM, and all have weak memory models.  Our work aims to clarify the interplay between weak memory and TM by extending existing axiomatic weak memory models (x86, Power, ARMv8, and C++) with new rules for TM. Our formal models are backed by automated tooling that enables (1) the synthesis of tests for validating our models against existing implementations and (2) the model-checking of TM-related transformations, such as lock elision and compiling C++ transactions to hardware. A key finding is that a proposed TM extension to ARMv8 currently being considered within ARM Research is incompatible with lock elision without sacrificing portability or performance.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {211–225},
numpages = {15},
keywords = {Program Synthesis, Transactional Memory, Shared Memory Concurrency, Weak Memory},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192375,
author = {Milano, Matthew and Myers, Andrew C.},
title = {MixT: A Language for Mixing Consistency in Geodistributed Transactions},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192375},
doi = {10.1145/3296979.3192375},
abstract = {Programming concurrent, distributed systems is hard—especially when these systems mutate shared, persistent state replicated at geographic scale. To enable high availability and scalability, a new class of weakly consistent data stores has become popular. However, some data needs strong consistency. To manipulate both weakly and strongly consistent data in a single transaction, we introduce a new abstraction: mixed-consistency transactions, embodied in a new embedded language, MixT. Programmers explicitly associate consistency models with remote storage sites; each atomic, isolated transaction can access a mixture of data with different consistency models. Compile-time information-flow checking, applied to consistency models, ensures that these models are mixed safely and enables the compiler to automatically partition transactions. New run-time mechanisms ensure that consistency models can also be mixed safely, even when the data used by a transaction resides on separate, mutually unaware stores. Performance measurements show that despite their stronger guarantees, mixed-consistency transactions retain much of the speed of weak consistency, significantly outperforming traditional serializable transactions.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {226–241},
numpages = {16},
keywords = {Consistency, Transactions, Information Flow}
}

@inproceedings{10.1145/3192366.3192375,
author = {Milano, Matthew and Myers, Andrew C.},
title = {MixT: A Language for Mixing Consistency in Geodistributed Transactions},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192375},
doi = {10.1145/3192366.3192375},
abstract = {Programming concurrent, distributed systems is hard—especially when these systems mutate shared, persistent state replicated at geographic scale. To enable high availability and scalability, a new class of weakly consistent data stores has become popular. However, some data needs strong consistency. To manipulate both weakly and strongly consistent data in a single transaction, we introduce a new abstraction: mixed-consistency transactions, embodied in a new embedded language, MixT. Programmers explicitly associate consistency models with remote storage sites; each atomic, isolated transaction can access a mixture of data with different consistency models. Compile-time information-flow checking, applied to consistency models, ensures that these models are mixed safely and enables the compiler to automatically partition transactions. New run-time mechanisms ensure that consistency models can also be mixed safely, even when the data used by a transaction resides on separate, mutually unaware stores. Performance measurements show that despite their stronger guarantees, mixed-consistency transactions retain much of the speed of weak consistency, significantly outperforming traditional serializable transactions.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {226–241},
numpages = {16},
keywords = {Information Flow, Transactions, Consistency},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192421,
author = {Dolan, Stephen and Sivaramakrishnan, KC and Madhavapeddy, Anil},
title = {Bounding Data Races in Space and Time},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192421},
doi = {10.1145/3296979.3192421},
abstract = {We propose a new semantics for shared-memory parallel programs that gives strong guarantees even in the presence of data races. Our local data race freedom property guarantees that all data-race-free portions of programs exhibit sequential semantics. We provide a straightforward operational semantics and an equivalent axiomatic model, and evaluate an implementation for the OCaml programming language. Our evaluation demonstrates that it is possible to balance a comprehensible memory model with a reasonable (no overhead on x86, ~0.6% on ARM) sequential performance trade-off in a mainstream programming language.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {242–255},
numpages = {14},
keywords = {operational semantics, weak memory models}
}

@inproceedings{10.1145/3192366.3192421,
author = {Dolan, Stephen and Sivaramakrishnan, KC and Madhavapeddy, Anil},
title = {Bounding Data Races in Space and Time},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192421},
doi = {10.1145/3192366.3192421},
abstract = {We propose a new semantics for shared-memory parallel programs that gives strong guarantees even in the presence of data races. Our local data race freedom property guarantees that all data-race-free portions of programs exhibit sequential semantics. We provide a straightforward operational semantics and an equivalent axiomatic model, and evaluate an implementation for the OCaml programming language. Our evaluation demonstrates that it is possible to balance a comprehensible memory model with a reasonable (no overhead on x86, ~0.6% on ARM) sequential performance trade-off in a mainstream programming language.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {242–255},
numpages = {14},
keywords = {operational semantics, weak memory models},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@software{10.1145/3211988,
author = {Sanchez-Stern, Alex and Panchekha, Pavel and Lerner, Sorin and Tatlock, Zachary},
title = {Herbgrind Sources &nbsp;at Time Of Submission},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3211988},
abstract = {
    <p>A source archive of Herbgrind, and the version of Herbie used in the paper eval. A more up-to-date version of Herbgrind can be found at github.com/uwplse/herbgrind.git</p>
},
keywords = {debugging, dynamic analysis, floating point}
}

@article{10.1145/3296979.3192411,
author = {Sanchez-Stern, Alex and Panchekha, Pavel and Lerner, Sorin and Tatlock, Zachary},
title = {Finding Root Causes of Floating Point Error},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192411},
doi = {10.1145/3296979.3192411},
abstract = {Floating-point arithmetic plays a central role in science, engineering, and finance by enabling developers to approximate real arithmetic. To address numerical issues in large floating-point applications, developers must identify root causes, which is difficult because floating-point errors are generally non-local, non-compositional, and non-uniform.  This paper presents Herbgrind, a tool to help developers identify and address root causes in numerical code written in low-level languages like C/C++ and Fortran. Herbgrind dynamically tracks dependencies between operations and program outputs to avoid false positives and abstracts erroneous computations to simplified program fragments whose improvement can reduce output error. We perform several case studies applying Herbgrind to large, expert-crafted numerical programs and show that it scales to applications spanning hundreds of thousands of lines, correctly handling the low-level details of modern floating point hardware and mathematical libraries and tracking error across function boundaries and through the heap.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {256–269},
numpages = {14},
keywords = {dynamic analysis, floating point, debugging}
}

@inproceedings{10.1145/3192366.3192411,
author = {Sanchez-Stern, Alex and Panchekha, Pavel and Lerner, Sorin and Tatlock, Zachary},
title = {Finding Root Causes of Floating Point Error},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192411},
doi = {10.1145/3192366.3192411},
abstract = {Floating-point arithmetic plays a central role in science, engineering, and finance by enabling developers to approximate real arithmetic. To address numerical issues in large floating-point applications, developers must identify root causes, which is difficult because floating-point errors are generally non-local, non-compositional, and non-uniform.  This paper presents Herbgrind, a tool to help developers identify and address root causes in numerical code written in low-level languages like C/C++ and Fortran. Herbgrind dynamically tracks dependencies between operations and program outputs to avoid false positives and abstracts erroneous computations to simplified program fragments whose improvement can reduce output error. We perform several case studies applying Herbgrind to large, expert-crafted numerical programs and show that it scales to applications spanning hundreds of thousands of lines, correctly handling the low-level details of modern floating point hardware and mathematical libraries and tracking error across function boundaries and through the heap.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {256–269},
numpages = {14},
keywords = {dynamic analysis, debugging, floating point},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192369,
author = {Adams, Ulf},
title = {Ry\={u}: Fast Float-to-String Conversion},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192369},
doi = {10.1145/3296979.3192369},
abstract = {We present Ry\={u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. Ry\={u} is simpler and approximately three times faster than the previously fastest implementation.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {270–282},
numpages = {13},
keywords = {float, performance, string}
}

@inproceedings{10.1145/3192366.3192369,
author = {Adams, Ulf},
title = {Ry\={u}: Fast Float-to-String Conversion},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192369},
doi = {10.1145/3192366.3192369},
abstract = {We present Ry\={u}, a new routine to convert binary floating point numbers to their decimal representations using only fixed-size integer operations, and prove its correctness. Ry\={u} is simpler and approximately three times faster than the previously fastest implementation.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {270–282},
numpages = {13},
keywords = {float, string, performance},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@software{10.1145/3211990,
author = {Steindorfer, Michael J. and Vinju, Jurgen J.},
title = {Replication Package for Article "To-Many or To-One? All-in-One! Efficient Purely Functional Multi-Maps with Type-Heterogeneous Hash-Tries"},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3211990},
abstract = {
    <p># Getting Started Guide The evaluation of our PLDI'18 paper entitled _To-many or To-one__?__ All-in-one! --- Efficient Purely Functional Multi-Maps with Type-Heterogeneous Hash-Tries_ consists of microbenchmarks and a case study in program analysis, both benchmarks execute fully automated. ## Requirements We assume basic familiarity with UNIX terminals and command line tools. The artifact was tested under UNIX operating systems (i.e., Ubuntu Linux 16.04.3.LTS and Apple macOS). The artifact requires the following resources to run: * Command line tools: * java (version 8), * maven, * make, * ant, * R and RScript * Internet connection (for automatically downloading dependencies). The execution time benchmarks are configured to use heap sizes of 8GB, whereas the memory measurments use 16GB. Thus computers or virtual machiens with at least 8GB RAM are recommended. For ease of use, we created a virtual machine with tools, benchmarks, and data setup. As basis we used the **Ubuntu Linux 16.04.3.LTS** distribution, installed in a [**VirtualBox**](https://www.virtualbox.org) virtual machine (version 5.2.6) with the **Oracle VM VirtualBox Extension Pack** installed. Go to [www.virtualbox.org](www.virtualbox.org) for downloading the software and installation instructions. The virtual machine has the following packages installed in order to satisfy our requirements: * `sudo apt install git` * `sudo apt install default-jdk` * `sudo apt install maven` * `sudo apt install openssh-server` * `sudo apt install r-base r-base-dev` ## SSH access to Virtual Machine For more convenient usage of the artifact, the authors are asked to setup remote access via terminal according to the stackoverflow answer [How to SSH to a VirtualBox guest externally through a host__?__](https://stackoverflow.com/questions/5906441/how-to-ssh-to-a-virtualbox-guest-externally-through-a-host). Afterwards, users of the artifact can log into the virtual machine with the following command line: &gt; ssh -p 3022 axiom@localhost When prompted for a password, use the password "axiom" (i.e., the same as the username that is already provided in the commannd line). ## Executing the Benchmarks and Data Post-Processing Pipeline Thh execution of benchmark referred to in the evaluation Sections 4, 5, and 6 of the paper, are entriely automated. The data post-processing of the data used in Sections 4 and 5 (cf. Figures 4, 5 and 6) are automated as well. Getting started with reproducing our resutls requires the execution of following commands in a console/terminal after being logged into our virtual machine: Moving into the artifacts directory: &gt; cd pldi18-artifact Setting up and compiling the artifacts: &gt; make prepare (To manually inspect what the **make** commands do, have a look at *pldi18-artifact/Makefile*.) ### (Quickly) Evaluating Archived Data concerning Sections 4 and 5 Next, one can re-generate the boxplots of the Figures 4, 5 and 6 (page 8 of the paper) based the logs that we obtained when executing the benchmarks ourselves. Our cached results are contained in the folder *data/20170417_1554*. The folder contains the following files: * **results.all-20170417_1554.log**: comma-separated values (CSV) file containing microbenchmark results of runtimes of individual operations * **map_sizes_heterogeneous_exponential_32bit_20170417_1554**: CSV file containing memory footprints in a 32-bit JVM setting. * **map_sizes_heterogeneous_exponential_64bit_20170417_1554**: CSV file containing memory footprints in a 64-bit JVM setting. * **hamt-benchmark-results-20170417_1554.tgz**: an archieve containing the files mentioned above verbose console output from running the benchmarks. The folder addionally contains three PDFs that were generated from the data files referenced above. The files correspond directly to the boxplots of Figures 4, 5, and 6 of page 8 of the paper. The boxplots are named **all-benchmarks-vf_champ_multimap_hhamt_by_vf_(scala|clojure|champ_map_as_multimap-map)-boxplot-speedup.pdf** and were generated with the following command, which should finish in a few seconds: &gt; make postprocessing_cached The reviewer may delete the three PDFs and re-generate them by with the command. An R script that is located under *benchmark/resources/r/benchmarks.r* performs the data post-processing and generation of the plots. ### (Relatively Quickly) Re-executing the whole Pipeline with a Reduced Dataset concerning Sections 4 and 5 The following commands re-execute the benchmarks for a reduced data set (with not statistically significant results) for the data structure sizes 16, 2048 and 1048576: Running reduced set of microbenchmarks: &gt; make run_microbenchmarks_short Benchmarking should approximately be finished in 15 to 30 minutes. After the benchmarks finished, the *data* directory should contain a new timestamped sub-folder with the benchmark results. After executing the following post-processing command, the folder should once again contain three PDFs / figures with boxplots: Running result analysis and post-processing: &gt; make postprocessing Note, that the results are neither statistically significant, nor do the cover a representative set of data structures size or data points in general. Nevertheless, the shape the boxplots may be similar to the figures in the paper. ### (Extremely Long) Re-executing the whole Pipeline with the Full Dataset concerning Sections 4 and 5 Running reduced set of microbenchmarks: &gt; make run_microbenchmarks Running result analysis and post-processing: &gt; make postprocessing Benchmarking the full data set can take up to two days of processing and should be performed on a dedicated machine without load or extraneous processes running in order to yield reliable results (see paper section about experimental setup and related work). ### (Relatively Quickly) Re-executing the Static Analysis Case Study concerning Section 6 Running case study benchmarks: &gt; make run_static_analysis_case_study Executing the case study benchmark I guess takes approximatley 1 to 2 hours. The benchmark results should be shown in tabular form in the terminal at the end of the benchmark run, and also serialized to disk (to a CSV file named *results.all-real-world-$TIMESTAMP.log*). Post-processing the data of this benchmark was not automated, and instead done by hand. However, it should be observable that the benchmark results for CHAMP and AXIOM are roughly the same. Note that abbreviations used in the benchmark setup do not match the names in the paper, i.e., CHART is used for CHAMP, and VF_CHAMP_MULTIMAP_HHAMT is used for AXIOM. The results cover the first three columns of Table 1. Colums 3-6 of Table 1 were also extracted manually with an instrumented version of the benchmark (cf. file *benchmark/src/main/java/dom/multimap/DominatorsSetMultimap_Default_Instrumented.java*). ## Other Relevant Items of our Artifact Our AXIOM hash trie implementations can be found under *code/capsule-experimental/src/main/java/io/usethesource/capsule/experimental/multimap/TrieSetMultimap_HHAMT.java*, for people interested in manually inspecting the implementation. The packages *benchmark/src/main/scala/io/usethesource/criterion/impl/persistent/scala* and *benchmark/src/main/java/io/usethesource/criterion/impl/persistent/clojure* contain simple interface facades that enables cross-library benchmarks under a common API. The benchmark implementations can be found in the *benchmark* project. File *benchmark/src/main/java/dom/multimap/DominatorsSetMultimap_Default.java* implements the real-word experiment (Section 6 of the paper), and file *benchmark/src/main/java/dom/multimap/DominatorsSetMultimap_Default_Instrumented.java* was used to extracting further statistics (colums 4-6 of Table 1). File *JmhSetMultimapBenchmarks.java* measures the runtimes of individual operations, whereas *CalculateHeterogeneousFootprints.java* performs footprint measurements (cf. page 8, Figures 4, 5, and 6). Note that the Java benchmark classes contain default parameters for their invocation, however the actual parameters are set in *runMicrobenchmarks.sh* and *runStaticProgramAnalysisCaseStudy.sh*.</p>
},
keywords = {Data structures, functional programming, graph, hashtable, JVM., many-to-many relation, multi-map, optimization, performance, persistent data structures}
}

@article{10.1145/3296979.3192420,
author = {Steindorfer, Michael J. and Vinju, Jurgen J.},
title = {To-Many or to-One? All-in-One! Efficient Purely Functional Multi-Maps with Type-Heterogeneous Hash-Tries},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192420},
doi = {10.1145/3296979.3192420},
abstract = {An immutable multi-map is a many-to-many map data structure with expected fast insert and lookup operations. This data structure is used for applications processing graphs or many-to-many relations as applied in compilers, runtimes of programming languages, or in static analysis of object-oriented systems. Collection data structures are assumed to carefully balance execution time of operations with memory consumption characteristics and need to scale gracefully from a few elements to multiple gigabytes at least. When processing larger in-memory data sets the overhead of the data structure encoding itself becomes a memory usage bottleneck, dominating the overall performance.  In this paper we propose AXIOM, a novel hash-trie data structure that allows for a highly efficient and type-safe multi-map encoding by distinguishing inlined values of singleton sets from nested sets of multi-mappings. AXIOM strictly generalizes over previous hash-trie data structures by supporting the processing of fine-grained type-heterogeneous content on the implementation level (while API and language support for type-heterogeneity are not scope of this paper). We detail the design and optimizations of AXIOM and further compare it against state-of-the-art immutable maps and multi-maps in Java, Scala and Clojure. We isolate key differences using microbenchmarks and validate the resulting conclusions on a case study in static analysis. AXIOM reduces the key-value storage overhead by 1.87x; with specializing and inlining across collection boundaries it improves by 5.1x.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {283–295},
numpages = {13},
keywords = {many-to-many relation, graph, multi-map, optimization, hashtable, performance, Data structures, persistent data structures, JVM, functional programming}
}

@inproceedings{10.1145/3192366.3192420,
author = {Steindorfer, Michael J. and Vinju, Jurgen J.},
title = {To-Many or to-One? All-in-One! Efficient Purely Functional Multi-Maps with Type-Heterogeneous Hash-Tries},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192420},
doi = {10.1145/3192366.3192420},
abstract = {An immutable multi-map is a many-to-many map data structure with expected fast insert and lookup operations. This data structure is used for applications processing graphs or many-to-many relations as applied in compilers, runtimes of programming languages, or in static analysis of object-oriented systems. Collection data structures are assumed to carefully balance execution time of operations with memory consumption characteristics and need to scale gracefully from a few elements to multiple gigabytes at least. When processing larger in-memory data sets the overhead of the data structure encoding itself becomes a memory usage bottleneck, dominating the overall performance.  In this paper we propose AXIOM, a novel hash-trie data structure that allows for a highly efficient and type-safe multi-map encoding by distinguishing inlined values of singleton sets from nested sets of multi-mappings. AXIOM strictly generalizes over previous hash-trie data structures by supporting the processing of fine-grained type-heterogeneous content on the implementation level (while API and language support for type-heterogeneity are not scope of this paper). We detail the design and optimizations of AXIOM and further compare it against state-of-the-art immutable maps and multi-maps in Java, Scala and Clojure. We isolate key differences using microbenchmarks and validate the resulting conclusions on a case study in static analysis. AXIOM reduces the key-value storage overhead by 1.87x; with specializing and inlining across collection boundaries it improves by 5.1x.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {283–295},
numpages = {13},
keywords = {performance, JVM, multi-map, hashtable, functional programming, optimization, persistent data structures, Data structures, many-to-many relation, graph},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192379,
author = {Koeplinger, David and Feldman, Matthew and Prabhakar, Raghu and Zhang, Yaqi and Hadjis, Stefan and Fiszel, Ruben and Zhao, Tian and Nardi, Luigi and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Spatial: A Language and Compiler for Application Accelerators},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192379},
doi = {10.1145/3296979.3192379},
abstract = {Industry is increasingly turning to reconfigurable architectures like FPGAs and CGRAs for improved performance and energy efficiency. Unfortunately, adoption of these architectures has been limited by their programming models. HDLs lack abstractions for productivity and are difficult to target from higher level languages. HLS tools are more productive, but offer an ad-hoc mix of software and hardware abstractions which make performance optimizations difficult.  In this work, we describe a new domain-specific language and compiler called Spatial for higher level descriptions of application accelerators. We describe Spatial's hardware-centric abstractions for both programmer productivity and design performance, and summarize the compiler passes required to support these abstractions, including pipeline scheduling, automatic memory banking, and automated design tuning driven by active machine learning. We demonstrate the language's ability to target FPGAs and CGRAs from common source code. We show that applications written in Spatial are, on average, 42% shorter and achieve a mean speedup of 2.9x over SDAccel HLS when targeting a Xilinx UltraScale+ VU9P FPGA on an Amazon EC2 F1 instance.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {296–311},
numpages = {16},
keywords = {domain-specific languages, CGRAs, reconfigurable architectures, high-level synthesis, hardware accelerators, compilers, FPGAs}
}

@inproceedings{10.1145/3192366.3192379,
author = {Koeplinger, David and Feldman, Matthew and Prabhakar, Raghu and Zhang, Yaqi and Hadjis, Stefan and Fiszel, Ruben and Zhao, Tian and Nardi, Luigi and Pedram, Ardavan and Kozyrakis, Christos and Olukotun, Kunle},
title = {Spatial: A Language and Compiler for Application Accelerators},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192379},
doi = {10.1145/3192366.3192379},
abstract = {Industry is increasingly turning to reconfigurable architectures like FPGAs and CGRAs for improved performance and energy efficiency. Unfortunately, adoption of these architectures has been limited by their programming models. HDLs lack abstractions for productivity and are difficult to target from higher level languages. HLS tools are more productive, but offer an ad-hoc mix of software and hardware abstractions which make performance optimizations difficult.  In this work, we describe a new domain-specific language and compiler called Spatial for higher level descriptions of application accelerators. We describe Spatial's hardware-centric abstractions for both programmer productivity and design performance, and summarize the compiler passes required to support these abstractions, including pipeline scheduling, automatic memory banking, and automated design tuning driven by active machine learning. We demonstrate the language's ability to target FPGAs and CGRAs from common source code. We show that applications written in Spatial are, on average, 42% shorter and achieve a mean speedup of 2.9x over SDAccel HLS when targeting a Xilinx UltraScale+ VU9P FPGA on an Amazon EC2 F1 instance.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {296–311},
numpages = {16},
keywords = {high-level synthesis, reconfigurable architectures, domain-specific languages, hardware accelerators, CGRAs, FPGAs, compilers},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192386,
author = {Kislal, Orhan and Kotra, Jagadish and Tang, Xulong and Kandemir, Mahmut Taylan and Jung, Myoungsoo},
title = {Enhancing Computation-to-Core Assignment with Physical Location Information},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192386},
doi = {10.1145/3296979.3192386},
abstract = {Going beyond a certain number of cores in modern architectures requires an on-chip network more scalable than conventional buses. However, employing an on-chip network in a manycore system (to improve scalability) makes the latencies of the data accesses issued by a core non-uniform. This non-uniformity can play a significant role in shaping the overall application performance. This work presents a novel compiler strategy which involves exposing architecture information to the compiler to enable an optimized computation-to-core mapping. Specifically, we propose a compiler-guided scheme that takes into account the relative positions of (and distances between) cores, last-level caches (LLCs) and memory controllers (MCs) in a manycore system, and generates a mapping of computations to cores with the goal of minimizing the on-chip network traffic. The experimental data collected using a set of 21 multi-threaded applications reveal that, on an average, our approach reduces the on-chip network latency in a 6\texttimes{}6 manycore system by 38.4% in the case of private LLCs, and 43.8% in the case of shared LLCs. These improvements translate to the corresponding execution time improvements of 10.9% and 12.7% for the private LLC and shared LLC based systems, respectively.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {312–327},
numpages = {16},
keywords = {Computation Mapping, Multicore Architectures, Compiler}
}

@inproceedings{10.1145/3192366.3192386,
author = {Kislal, Orhan and Kotra, Jagadish and Tang, Xulong and Kandemir, Mahmut Taylan and Jung, Myoungsoo},
title = {Enhancing Computation-to-Core Assignment with Physical Location Information},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192386},
doi = {10.1145/3192366.3192386},
abstract = {Going beyond a certain number of cores in modern architectures requires an on-chip network more scalable than conventional buses. However, employing an on-chip network in a manycore system (to improve scalability) makes the latencies of the data accesses issued by a core non-uniform. This non-uniformity can play a significant role in shaping the overall application performance. This work presents a novel compiler strategy which involves exposing architecture information to the compiler to enable an optimized computation-to-core mapping. Specifically, we propose a compiler-guided scheme that takes into account the relative positions of (and distances between) cores, last-level caches (LLCs) and memory controllers (MCs) in a manycore system, and generates a mapping of computations to cores with the goal of minimizing the on-chip network traffic. The experimental data collected using a set of 21 multi-threaded applications reveal that, on an average, our approach reduces the on-chip network latency in a 6\texttimes{}6 manycore system by 38.4% in the case of private LLCs, and 43.8% in the case of shared LLCs. These improvements translate to the corresponding execution time improvements of 10.9% and 12.7% for the private LLC and shared LLC based systems, respectively.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {312–327},
numpages = {16},
keywords = {Compiler, Multicore Architectures, Computation Mapping},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192393,
author = {Tran, Kim-Anh and Jimborean, Alexandra and Carlson, Trevor E. and Koukos, Konstantinos and Sj\"{a}lander, Magnus and Kaxiras, Stefanos},
title = {SWOOP: Software-Hardware Co-Design for Non-Speculative, Execute-Ahead, in-Order Cores},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192393},
doi = {10.1145/3296979.3192393},
abstract = {Increasing demands for energy efficiency constrain emerging hardware. These new hardware trends challenge the established assumptions in code generation and force us to rethink existing software optimization techniques. We propose a cross-layer redesign of the way compilers and the underlying microarchitecture are built and interact, to achieve both performance and high energy efficiency. In this paper, we address one of the main performance bottlenecks — last-level cache misses — through a software-hardware co-design. Our approach is able to hide memory latency and attain increased memory and instruction level parallelism by orchestrating a non-speculative, execute-ahead paradigm in software (SWOOP). While out-of-order (OoO) architectures attempt to hide memory latency by dynamically reordering instructions, they do so through expensive, power-hungry, speculative mechanisms.We aim to shift this complexity into software, and we build upon compilation techniques inherited from VLIW, software pipelining, modulo scheduling, decoupled access-execution, and software prefetching. In contrast to previous approaches we do not rely on either software or hardware speculation that can be detrimental to efficiency. Our SWOOP compiler is enhanced with lightweight architectural support, thus being able to transform applications that include highly complex control-flow and indirect memory accesses.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {328–343},
numpages = {16},
keywords = {memory level parallelism, compilers, hardware-software co-design}
}

@inproceedings{10.1145/3192366.3192393,
author = {Tran, Kim-Anh and Jimborean, Alexandra and Carlson, Trevor E. and Koukos, Konstantinos and Sj\"{a}lander, Magnus and Kaxiras, Stefanos},
title = {SWOOP: Software-Hardware Co-Design for Non-Speculative, Execute-Ahead, in-Order Cores},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192393},
doi = {10.1145/3192366.3192393},
abstract = {Increasing demands for energy efficiency constrain emerging hardware. These new hardware trends challenge the established assumptions in code generation and force us to rethink existing software optimization techniques. We propose a cross-layer redesign of the way compilers and the underlying microarchitecture are built and interact, to achieve both performance and high energy efficiency. In this paper, we address one of the main performance bottlenecks — last-level cache misses — through a software-hardware co-design. Our approach is able to hide memory latency and attain increased memory and instruction level parallelism by orchestrating a non-speculative, execute-ahead paradigm in software (SWOOP). While out-of-order (OoO) architectures attempt to hide memory latency by dynamically reordering instructions, they do so through expensive, power-hungry, speculative mechanisms.We aim to shift this complexity into software, and we build upon compilation techniques inherited from VLIW, software pipelining, modulo scheduling, decoupled access-execution, and software prefetching. In contrast to previous approaches we do not rely on either software or hardware speculation that can be detrimental to efficiency. Our SWOOP compiler is enhanced with lightweight architectural support, thus being able to transform applications that include highly complex control-flow and indirect memory accesses.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {328–343},
numpages = {16},
keywords = {memory level parallelism, compilers, hardware-software co-design},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192380,
author = {Liu, Hongyu and Silvestro, Sam and Wang, Wei and Tian, Chen and Liu, Tongping},
title = {IReplayer: In-Situ and Identical Record-and-Replay for Multithreaded Applications},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192380},
doi = {10.1145/3296979.3192380},
abstract = {Reproducing executions of multithreaded programs is very challenging due to many intrinsic and external non-deterministic factors. Existing RnR systems achieve significant progress in terms of performance overhead, but none targets the in-situ setting, in which replay occurs within the same process as the recording process. Also, most existing work cannot achieve identical replay, which may prevent the reproduction of some errors.  This paper presents iReplayer, which aims to identically replay multithreaded programs in the original process (under the "in-situ" setting). The novel in-situ and identical replay of iReplayer makes it more likely to reproduce errors, and allows it to directly employ debugging mechanisms (e.g. watchpoints) to aid failure diagnosis. Currently, iReplayer only incurs 3% performance overhead on average, which allows it to be always enabled in the production environment. iReplayer enables a range of possibilities, and this paper presents three examples: two automatic tools for detecting buffer overflows and use-after-free bugs, and one interactive debugging tool that is integrated with GDB.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {344–358},
numpages = {15},
keywords = {Record-and-Replay, In-situ Replay, Multithreaded Debugging, Identical Replay}
}

@inproceedings{10.1145/3192366.3192380,
author = {Liu, Hongyu and Silvestro, Sam and Wang, Wei and Tian, Chen and Liu, Tongping},
title = {IReplayer: In-Situ and Identical Record-and-Replay for Multithreaded Applications},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192380},
doi = {10.1145/3192366.3192380},
abstract = {Reproducing executions of multithreaded programs is very challenging due to many intrinsic and external non-deterministic factors. Existing RnR systems achieve significant progress in terms of performance overhead, but none targets the in-situ setting, in which replay occurs within the same process as the recording process. Also, most existing work cannot achieve identical replay, which may prevent the reproduction of some errors.  This paper presents iReplayer, which aims to identically replay multithreaded programs in the original process (under the "in-situ" setting). The novel in-situ and identical replay of iReplayer makes it more likely to reproduce errors, and allows it to directly employ debugging mechanisms (e.g. watchpoints) to aid failure diagnosis. Currently, iReplayer only incurs 3% performance overhead on average, which allows it to be always enabled in the production environment. iReplayer enables a range of possibilities, and this paper presents three examples: two automatic tools for detecting buffer overflows and use-after-free bugs, and one interactive debugging tool that is integrated with GDB.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {344–358},
numpages = {15},
keywords = {In-situ Replay, Multithreaded Debugging, Record-and-Replay, Identical Replay},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192390,
author = {Liu, Bozhen and Huang, Jeff},
title = {D4: Fast Concurrency Debugging with Parallel Differential Analysis},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192390},
doi = {10.1145/3296979.3192390},
abstract = {We present D4, a fast concurrency analysis framework that detects concurrency bugs (e.g., data races and deadlocks) interactively in the programming phase. As developers add, modify, and remove statements, the code changes are sent to D4 to detect concurrency bugs in real time, which in turn provides immediate feedback to the developer of the new bugs. The cornerstone of D4 includes a novel system design and two novel parallel differential algorithms that embrace both change and parallelization for fundamental static analyses of concurrent programs. Both algorithms react to program changes by memoizing the analysis results and only recomputing the impact of a change in parallel. Our evaluation on an extensive collection of large real-world applications shows that D4 efficiently pinpoints concurrency bugs within 100ms on average after a code change, several orders of magnitude faster than both the exhaustive analysis and the state-of-the-art incremental techniques.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {359–373},
numpages = {15},
keywords = {Real Time, Differential Pointer Analysis, Deadlocks, Data Races, Static Happens-Before Analysis, Concurrency Debugging, Parallel Differential Analysis}
}

@inproceedings{10.1145/3192366.3192390,
author = {Liu, Bozhen and Huang, Jeff},
title = {D4: Fast Concurrency Debugging with Parallel Differential Analysis},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192390},
doi = {10.1145/3192366.3192390},
abstract = {We present D4, a fast concurrency analysis framework that detects concurrency bugs (e.g., data races and deadlocks) interactively in the programming phase. As developers add, modify, and remove statements, the code changes are sent to D4 to detect concurrency bugs in real time, which in turn provides immediate feedback to the developer of the new bugs. The cornerstone of D4 includes a novel system design and two novel parallel differential algorithms that embrace both change and parallelization for fundamental static analyses of concurrent programs. Both algorithms react to program changes by memoizing the analysis results and only recomputing the impact of a change in parallel. Our evaluation on an extensive collection of large real-world applications shows that D4 efficiently pinpoints concurrency bugs within 100ms on average after a code change, several orders of magnitude faster than both the exhaustive analysis and the state-of-the-art incremental techniques.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {359–373},
numpages = {15},
keywords = {Static Happens-Before Analysis, Differential Pointer Analysis, Concurrency Debugging, Real Time, Deadlocks, Parallel Differential Analysis, Data Races},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192385,
author = {Roemer, Jake and Gen\c{c}, Kaan and Bond, Michael D.},
title = {High-Coverage, Unbounded Sound Predictive Race Detection},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192385},
doi = {10.1145/3296979.3192385},
abstract = {Dynamic program analysis can predict data races knowable from an observed execution, but existing predictive analyses either miss races or cannot analyze full program executions. This paper presents Vindicator, a novel, sound (no false races) predictive approach that finds more data races than existing predictive approaches. Vindicator achieves high coverage by using a new, efficient analysis that finds all possible predictable races but may detect false races. Vindicator ensures soundness using a novel algorithm that checks each potential race to determine whether it is a true predictable race. An evaluation using large Java programs shows that Vindicator finds hard-to-detect predictable races that existing sound predictive analyses miss, at a comparable performance cost.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {374–389},
numpages = {16},
keywords = {Data race detection, dynamic predictive analysis}
}

@inproceedings{10.1145/3192366.3192385,
author = {Roemer, Jake and Gen\c{c}, Kaan and Bond, Michael D.},
title = {High-Coverage, Unbounded Sound Predictive Race Detection},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192385},
doi = {10.1145/3192366.3192385},
abstract = {Dynamic program analysis can predict data races knowable from an observed execution, but existing predictive analyses either miss races or cannot analyze full program executions. This paper presents Vindicator, a novel, sound (no false races) predictive approach that finds more data races than existing predictive approaches. Vindicator achieves high coverage by using a new, efficient analysis that finds all possible predictable races but may detect false races. Vindicator ensures soundness using a novel algorithm that checks each potential race to determine whether it is a true predictable race. An evaluation using large Java programs shows that Vindicator finds hard-to-detect predictable races that existing sound predictive analyses miss, at a comparable performance cost.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {374–389},
numpages = {16},
keywords = {dynamic predictive analysis, Data race detection},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192368,
author = {Peng, Yuanfeng and Grover, Vinod and Devietti, Joseph},
title = {CURD: A Dynamic CUDA Race Detector},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192368},
doi = {10.1145/3296979.3192368},
abstract = {As GPUs have become an integral part of nearly every pro- cessor, GPU programming has become increasingly popular. GPU programming requires a combination of extreme levels of parallelism and low-level programming, making it easy for concurrency bugs such as data races to arise. These con- currency bugs can be extremely subtle and di cult to debug due to the massive numbers of threads running concurrently on a modern GPU. While some tools exist to detect data races in GPU pro- grams, they are often prohibitively slow or focused only on a small class of data races in shared memory. Compared to prior work, our race detector, CURD, can detect data races precisely on both shared and global memory, selects an appropriate race detection algorithm based on the synchronization used in a program, and utilizes efficient compiler instrumentation to reduce performance overheads. Across 53 benchmarks, we find that using CURD incurs an aver- age slowdown of just 2.88x over native execution. CURD is 2.1x faster than Nvidia’s CUDA-Racecheck race detector, de- spite detecting a much broader class of races. CURD finds 35 races across our benchmarks, including bugs in established benchmark suites and in sample programs from Nvidia.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {390–403},
numpages = {14},
keywords = {GPUs, data race detection, Nvidia CUDA}
}

@inproceedings{10.1145/3192366.3192368,
author = {Peng, Yuanfeng and Grover, Vinod and Devietti, Joseph},
title = {CURD: A Dynamic CUDA Race Detector},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192368},
doi = {10.1145/3192366.3192368},
abstract = {As GPUs have become an integral part of nearly every pro- cessor, GPU programming has become increasingly popular. GPU programming requires a combination of extreme levels of parallelism and low-level programming, making it easy for concurrency bugs such as data races to arise. These con- currency bugs can be extremely subtle and di cult to debug due to the massive numbers of threads running concurrently on a modern GPU. While some tools exist to detect data races in GPU pro- grams, they are often prohibitively slow or focused only on a small class of data races in shared memory. Compared to prior work, our race detector, CURD, can detect data races precisely on both shared and global memory, selects an appropriate race detection algorithm based on the synchronization used in a program, and utilizes efficient compiler instrumentation to reduce performance overheads. Across 53 benchmarks, we find that using CURD incurs an aver- age slowdown of just 2.88x over native execution. CURD is 2.1x faster than Nvidia’s CUDA-Racecheck race detector, de- spite detecting a much broader class of races. CURD finds 35 races across our benchmarks, including bugs in established benchmark suites and in sample programs from Nvidia.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {390–403},
numpages = {14},
keywords = {Nvidia CUDA, data race detection, GPUs},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192412,
author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
title = {A General Path-Based Representation for Predicting Program Properties},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192412},
doi = {10.1145/3296979.3192412},
abstract = {Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming, and increase programmer productivity. A major challenge when learning from programs is how to represent programs in a way that facilitates effective learning. We present a general path-based representation for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree (AST). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens. We show that this representation is general and can: (i) cover different prediction tasks, (ii) drive different learning algorithms (for both generative and discriminative models), and (iii) work across different programming languages. We evaluate our approach on the tasks of predicting variable names, method names, and full types. We use our representation to drive both CRF-based and word2vec-based learning, for programs of four languages: JavaScript, Java, Python and C#. Our evaluation shows that our approach obtains better results than task-specific handcrafted representations across different tasks and programming languages.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {404–419},
numpages = {16},
keywords = {Machine Learning, Learning Representations, Big Code, Programming Languages}
}

@inproceedings{10.1145/3192366.3192412,
author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
title = {A General Path-Based Representation for Predicting Program Properties},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192412},
doi = {10.1145/3192366.3192412},
abstract = {Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming, and increase programmer productivity. A major challenge when learning from programs is how to represent programs in a way that facilitates effective learning. We present a general path-based representation for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree (AST). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens. We show that this representation is general and can: (i) cover different prediction tasks, (ii) drive different learning algorithms (for both generative and discriminative models), and (iii) work across different programming languages. We evaluate our approach on the tasks of predicting variable names, method names, and full types. We use our representation to drive both CRF-based and word2vec-based learning, for programs of four languages: JavaScript, Java, Python and C#. Our evaluation shows that our approach obtains better results than task-specific handcrafted representations across different tasks and programming languages.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {404–419},
numpages = {16},
keywords = {Learning Representations, Programming Languages, Big Code, Machine Learning},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192382,
author = {Feng, Yu and Martins, Ruben and Bastani, Osbert and Dillig, Isil},
title = {Program Synthesis Using Conflict-Driven Learning},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192382},
doi = {10.1145/3296979.3192382},
abstract = {We propose a new conflict-driven program synthesis technique that is capable of learning from past mistakes. Given a spurious program that violates the desired specification, our synthesis algorithm identifies the root cause of the conflict and learns new lemmas that can prevent similar mistakes in the future. Specifically, we introduce the notion of equivalence modulo conflict and show how this idea can be used to learn useful lemmas that allow the synthesizer to prune large parts of the search space. We have implemented a general-purpose CDCL-style program synthesizer called Neo and evaluate it in two different application domains, namely data wrangling in R and functional programming over lists. Our experiments demonstrate the substantial benefits of conflict-driven learning and show that Neo outperforms two state-of-the-art synthesis tools, Morpheus and Deepcoder, that target these respective domains.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {420–435},
numpages = {16},
keywords = {automated reasoning, conflict-driven learning, program synthesis}
}

@inproceedings{10.1145/3192366.3192382,
author = {Feng, Yu and Martins, Ruben and Bastani, Osbert and Dillig, Isil},
title = {Program Synthesis Using Conflict-Driven Learning},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192382},
doi = {10.1145/3192366.3192382},
abstract = {We propose a new conflict-driven program synthesis technique that is capable of learning from past mistakes. Given a spurious program that violates the desired specification, our synthesis algorithm identifies the root cause of the conflict and learns new lemmas that can prevent similar mistakes in the future. Specifically, we introduce the notion of equivalence modulo conflict and show how this idea can be used to learn useful lemmas that allow the synthesizer to prune large parts of the search space. We have implemented a general-purpose CDCL-style program synthesizer called Neo and evaluate it in two different application domains, namely data wrangling in R and functional programming over lists. Our experiments demonstrate the substantial benefits of conflict-driven learning and show that Neo outperforms two state-of-the-art synthesis tools, Morpheus and Deepcoder, that target these respective domains.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {420–435},
numpages = {16},
keywords = {conflict-driven learning, program synthesis, automated reasoning},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@software{10.1145/3211992,
author = {Lee, Woosuk and Heo, Kihong and Alur, Rajeev and Naik, Mayur},
title = {Replication Package for PLDI'18 of Euphony},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3211992},
abstract = {
    <p>This is the replication package for our paper titled "Accelerating Search-Based Program Synthesis using Learned Probabilistic Models". The package includes a virtual machine image along with instructions how to use our tool and reproduce the claims in our paper. Using this package, one can reproduce Table 4, 5, 6, 7 and Figure 8 in the paper.</p>
},
keywords = {Domain-specific languages, Statistical methods, Synthesis, Transfer learning}
}

@article{10.1145/3296979.3192410,
author = {Lee, Woosuk and Heo, Kihong and Alur, Rajeev and Naik, Mayur},
title = {Accelerating Search-Based Program Synthesis Using Learned Probabilistic Models},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192410},
doi = {10.1145/3296979.3192410},
abstract = {A key challenge in program synthesis concerns how to efficiently search for the desired program in the space of possible programs. We propose a general approach to accelerate search-based program synthesis by biasing the search towards likely programs. Our approach targets a standard formulation, syntax-guided synthesis (SyGuS), by extending the grammar of possible programs with a probabilistic model dictating the likelihood of each program. We develop a weighted search algorithm to efficiently enumerate programs in order of their likelihood. We also propose a method based on transfer learning that enables to effectively learn a powerful model, called probabilistic higher-order grammar, from known solutions in a domain. We have implemented our approach in a tool called Euphony and evaluate it on SyGuS benchmark problems from a variety of domains. We show that Euphony can learn good models using easily obtainable solutions, and achieves significant performance gains over existing general-purpose as well as domain-specific synthesizers.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {436–449},
numpages = {14},
keywords = {Domain-specific languages, Statistical methods, Synthesis, Transfer learning}
}

@inproceedings{10.1145/3192366.3192410,
author = {Lee, Woosuk and Heo, Kihong and Alur, Rajeev and Naik, Mayur},
title = {Accelerating Search-Based Program Synthesis Using Learned Probabilistic Models},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192410},
doi = {10.1145/3192366.3192410},
abstract = {A key challenge in program synthesis concerns how to efficiently search for the desired program in the space of possible programs. We propose a general approach to accelerate search-based program synthesis by biasing the search towards likely programs. Our approach targets a standard formulation, syntax-guided synthesis (SyGuS), by extending the grammar of possible programs with a probabilistic model dictating the likelihood of each program. We develop a weighted search algorithm to efficiently enumerate programs in order of their likelihood. We also propose a method based on transfer learning that enables to effectively learn a powerful model, called probabilistic higher-order grammar, from known solutions in a domain. We have implemented our approach in a tool called Euphony and evaluate it on SyGuS benchmark problems from a variety of domains. We show that Euphony can learn good models using easily obtainable solutions, and achieves significant performance gains over existing general-purpose as well as domain-specific synthesizers.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {436–449},
numpages = {14},
keywords = {Statistical methods, Domain-specific languages, Transfer learning, Synthesis},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192403,
author = {Paletov, Rumen and Tsankov, Petar and Raychev, Veselin and Vechev, Martin},
title = {Inferring Crypto API Rules from Code Changes},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192403},
doi = {10.1145/3296979.3192403},
abstract = {Creating and maintaining an up-to-date set of security rules that match misuses of crypto APIs is challenging, as crypto APIs constantly evolve over time with new cryptographic primitives and settings, making existing ones obsolete.  To address this challenge, we present a new approach to extract security fixes from thousands of code changes. Our approach consists of: (i) identifying code changes, which often capture security fixes, (ii) an abstraction that filters irrelevant code changes (such as refactorings), and (iii) a clustering analysis that reveals commonalities between semantic code changes and helps in eliciting security rules.  We applied our approach to the Java Crypto API and showed that it is effective: (i) our abstraction effectively filters non-semantic code changes (over 99% of all changes) without removing security fixes, and (ii) over 80% of the code changes are security fixes identifying security rules. Based on our results, we identified 13 rules, including new ones not supported by existing security checkers.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {450–464},
numpages = {15},
keywords = {Misuse of Cryptography, Learning, Security}
}

@inproceedings{10.1145/3192366.3192403,
author = {Paletov, Rumen and Tsankov, Petar and Raychev, Veselin and Vechev, Martin},
title = {Inferring Crypto API Rules from Code Changes},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192403},
doi = {10.1145/3192366.3192403},
abstract = {Creating and maintaining an up-to-date set of security rules that match misuses of crypto APIs is challenging, as crypto APIs constantly evolve over time with new cryptographic primitives and settings, making existing ones obsolete.  To address this challenge, we present a new approach to extract security fixes from thousands of code changes. Our approach consists of: (i) identifying code changes, which often capture security fixes, (ii) an abstraction that filters irrelevant code changes (such as refactorings), and (iii) a clustering analysis that reveals commonalities between semantic code changes and helps in eliciting security rules.  We applied our approach to the Java Crypto API and showed that it is effective: (i) our abstraction effectively filters non-semantic code changes (over 99% of all changes) without removing security fixes, and (ii) over 80% of the code changes are security fixes identifying security rules. Based on our results, we identified 13 rules, including new ones not supported by existing security checkers.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {450–464},
numpages = {15},
keywords = {Security, Learning, Misuse of Cryptography},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192387,
author = {Gulwani, Sumit and Radi\v{c}ek, Ivan and Zuleger, Florian},
title = {Automated Clustering and Program Repair for Introductory Programming Assignments},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192387},
doi = {10.1145/3296979.3192387},
abstract = {Providing feedback on programming assignments is a tedious task for the instructor, and even impossible in large Massive Open Online Courses with thousands of students. Previous research has suggested that program repair techniques can be used to generate feedback in programming education. In this paper, we present a novel fully automated program repair algorithm for introductory programming assignments. The key idea of the technique, which enables automation and scalability, is to use the existing correct student solutions to repair the incorrect attempts. We evaluate the approach in two experiments: (I) We evaluate the number, size and quality of the generated repairs on 4,293 incorrect student attempts from an existing MOOC. We find that our approach can repair 97% of student attempts, while 81% of those are small repairs of good quality. (II) We conduct a preliminary user study on performance and repair usefulness in an interactive teaching setting. We obtain promising initial results (the average usefulness grade 3.4 on a scale from 1 to 5), and conclude that our approach can be used in an interactive setting.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {465–480},
numpages = {16},
keywords = {MOOC, clustering, program repair, dynamic analysis, programming education}
}

@inproceedings{10.1145/3192366.3192387,
author = {Gulwani, Sumit and Radi\v{c}ek, Ivan and Zuleger, Florian},
title = {Automated Clustering and Program Repair for Introductory Programming Assignments},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192387},
doi = {10.1145/3192366.3192387},
abstract = {Providing feedback on programming assignments is a tedious task for the instructor, and even impossible in large Massive Open Online Courses with thousands of students. Previous research has suggested that program repair techniques can be used to generate feedback in programming education. In this paper, we present a novel fully automated program repair algorithm for introductory programming assignments. The key idea of the technique, which enables automation and scalability, is to use the existing correct student solutions to repair the incorrect attempts. We evaluate the approach in two experiments: (I) We evaluate the number, size and quality of the generated repairs on 4,293 incorrect student attempts from an existing MOOC. We find that our approach can repair 97% of student attempts, while 81% of those are small repairs of good quality. (II) We conduct a preliminary user study on performance and repair usefulness in an interactive teaching setting. We obtain promising initial results (the average usefulness grade 3.4 on a scale from 1 to 5), and conclude that our approach can be used in an interactive setting.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {465–480},
numpages = {16},
keywords = {clustering, programming education, program repair, MOOC, dynamic analysis},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192384,
author = {Wang, Ke and Singh, Rishabh and Su, Zhendong},
title = {Search, Align, and Repair: Data-Driven Feedback Generation for Introductory Programming Exercises},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192384},
doi = {10.1145/3296979.3192384},
abstract = {This paper introduces the “Search, Align, and Repair” data-driven program repair framework to automate feedback generation for introductory programming exercises. Distinct from existing techniques, our goal is to develop an efficient, fully automated, and problem-agnostic technique for large or MOOC-scale introductory programming courses. We leverage the large amount of available student submissions in such settings and develop new algorithms for identifying similar programs, aligning correct and incorrect programs, and repairing incorrect programs by finding minimal fixes. We have implemented our technique in the Sarfgen system and evaluated it on thousands of real student attempts from the Microsoft-DEV204.1x edX course and the Microsoft CodeHunt platform. Our results show that Sarfgen can, within two seconds on average, generate concise, useful feedback for 89.7% of the incorrect student submissions. It has been integrated with the Microsoft-DEV204.1X edX class and deployed for production use.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {481–495},
numpages = {15},
keywords = {Computer-Aided Education, Automatic Grading, Program Analysis}
}

@inproceedings{10.1145/3192366.3192384,
author = {Wang, Ke and Singh, Rishabh and Su, Zhendong},
title = {Search, Align, and Repair: Data-Driven Feedback Generation for Introductory Programming Exercises},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192384},
doi = {10.1145/3192366.3192384},
abstract = {This paper introduces the “Search, Align, and Repair” data-driven program repair framework to automate feedback generation for introductory programming exercises. Distinct from existing techniques, our goal is to develop an efficient, fully automated, and problem-agnostic technique for large or MOOC-scale introductory programming courses. We leverage the large amount of available student submissions in such settings and develop new algorithms for identifying similar programs, aligning correct and incorrect programs, and repairing incorrect programs by finding minimal fixes. We have implemented our technique in the Sarfgen system and evaluated it on thousands of real student attempts from the Microsoft-DEV204.1x edX course and the Microsoft CodeHunt platform. Our results show that Sarfgen can, within two seconds on average, generate concise, useful feedback for 89.7% of the incorrect student submissions. It has been integrated with the Microsoft-DEV204.1X edX class and deployed for production use.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {481–495},
numpages = {15},
keywords = {Computer-Aided Education, Automatic Grading, Program Analysis},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192394,
author = {Ngo, Van Chan and Carbonneaux, Quentin and Hoffmann, Jan},
title = {Bounded Expectations: Resource Analysis for Probabilistic Programs},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192394},
doi = {10.1145/3296979.3192394},
abstract = {This paper presents a new static analysis for deriving upper bounds on the expected resource consumption of probabilistic programs. The analysis is fully automatic and derives symbolic bounds that are multivariate polynomials in the inputs. The new technique combines manual state-of-the-art reasoning techniques for probabilistic programs with an effective method for automatic resource-bound analysis of deterministic programs. It can be seen as both, an extension of automatic amortized resource analysis (AARA) to probabilistic programs and an automation of manual reasoning for probabilistic programs that is based on weakest preconditions. An advantage of the technique is that it combines the clarity and compositionality of a weakest-precondition calculus with the efficient automation of AARA. As a result, bound inference can be reduced to off-the-shelf LP solving in many cases and automatically-derived bounds can be interactively extended with standard program logics if the automation fails. Building on existing work, the soundness of the analysis is proved with respect to an operational semantics that is based on Markov decision processes. The effectiveness of the technique is demonstrated with a prototype implementation that is used to automatically analyze 39 challenging probabilistic programs and randomized algorithms. Experiments indicate that the derived constant factors in the bounds are very precise and even optimal for some programs.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {496–512},
numpages = {17},
keywords = {Probabilistic programming, Static analysis, Resource bound analysis}
}

@inproceedings{10.1145/3192366.3192394,
author = {Ngo, Van Chan and Carbonneaux, Quentin and Hoffmann, Jan},
title = {Bounded Expectations: Resource Analysis for Probabilistic Programs},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192394},
doi = {10.1145/3192366.3192394},
abstract = {This paper presents a new static analysis for deriving upper bounds on the expected resource consumption of probabilistic programs. The analysis is fully automatic and derives symbolic bounds that are multivariate polynomials in the inputs. The new technique combines manual state-of-the-art reasoning techniques for probabilistic programs with an effective method for automatic resource-bound analysis of deterministic programs. It can be seen as both, an extension of automatic amortized resource analysis (AARA) to probabilistic programs and an automation of manual reasoning for probabilistic programs that is based on weakest preconditions. An advantage of the technique is that it combines the clarity and compositionality of a weakest-precondition calculus with the efficient automation of AARA. As a result, bound inference can be reduced to off-the-shelf LP solving in many cases and automatically-derived bounds can be interactively extended with standard program logics if the automation fails. Building on existing work, the soundness of the analysis is proved with respect to an operational semantics that is based on Markov decision processes. The effectiveness of the technique is demonstrated with a prototype implementation that is used to automatically analyze 39 challenging probabilistic programs and randomized algorithms. Experiments indicate that the derived constant factors in the bounds are very precise and even optimal for some programs.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {496–512},
numpages = {17},
keywords = {Static analysis, Probabilistic programming, Resource bound analysis},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@software{10.1145/3211994,
author = {Wang, Di and Hoffmann, Jan and Reps, Thomas},
title = {PMAF: An Algebraic Framework for Static Analysis of Probabilistic Programs},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3211994},
abstract = {
    <p>PMAF is an algebraic framework for static analysis of probabilistic programs. PMAF takes an interpretation of a pre-Markov algebra (PMA) and a program as its input, and then performs the analysis specified by the algebra on the program automatically via a fixpoint computation. We have instantiated PMAF on three PMAs, obtaining tolls for: Bayesian inference, the Markov decision problem, and linear expectation-invariant analysis.</p>
},
keywords = {Expectation invariant, Pre-Markov algebra, Probabilistic program, Program analysis}
}

@article{10.1145/3296979.3192408,
author = {Wang, Di and Hoffmann, Jan and Reps, Thomas},
title = {PMAF: An Algebraic Framework for Static Analysis of Probabilistic Programs},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192408},
doi = {10.1145/3296979.3192408},
abstract = {Automatically establishing that a probabilistic program satisfies some property ϕ is a challenging problem. While a sampling-based approach—which involves running the program repeatedly—can suggest that ϕ holds, to establish that the program satisfies ϕ, analysis techniques must be used. Despite recent successes, probabilistic static analyses are still more difficult to design and implement than their deterministic counterparts. This paper presents a framework, called PMAF, for designing, implementing, and proving the correctness of static analyses of probabilistic programs with challenging features such as recursion, unstructured control-flow, divergence, nondeterminism, and continuous distributions. PMAF introduces pre-Markov algebras to factor out common parts of different analyses. To perform interprocedural analysis and to create procedure summaries, PMAF extends ideas from non-probabilistic interprocedural dataflow analysis to the probabilistic setting. One novelty is that PMAF is based on a semantics formulated in terms of a control-flow hyper-graph for each procedure, rather than a standard control-flow graph. To evaluate its effectiveness, PMAF has been used to reformulate and implement existing intraprocedural analyses for Bayesian-inference and the Markov decision problem, by creating corresponding interprocedural analyses. Additionally, PMAF has been used to implement a new interprocedural linear expectation-invariant analysis. Experiments with benchmark programs for the three analyses demonstrate that the approach is practical.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {513–528},
numpages = {16},
keywords = {pre-Markov algebra, expectation invariant, Program analysis, probabilistic programming}
}

@inproceedings{10.1145/3192366.3192408,
author = {Wang, Di and Hoffmann, Jan and Reps, Thomas},
title = {PMAF: An Algebraic Framework for Static Analysis of Probabilistic Programs},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192408},
doi = {10.1145/3192366.3192408},
abstract = {Automatically establishing that a probabilistic program satisfies some property ϕ is a challenging problem. While a sampling-based approach—which involves running the program repeatedly—can suggest that ϕ holds, to establish that the program satisfies ϕ, analysis techniques must be used. Despite recent successes, probabilistic static analyses are still more difficult to design and implement than their deterministic counterparts. This paper presents a framework, called PMAF, for designing, implementing, and proving the correctness of static analyses of probabilistic programs with challenging features such as recursion, unstructured control-flow, divergence, nondeterminism, and continuous distributions. PMAF introduces pre-Markov algebras to factor out common parts of different analyses. To perform interprocedural analysis and to create procedure summaries, PMAF extends ideas from non-probabilistic interprocedural dataflow analysis to the probabilistic setting. One novelty is that PMAF is based on a semantics formulated in terms of a control-flow hyper-graph for each procedure, rather than a standard control-flow graph. To evaluate its effectiveness, PMAF has been used to reformulate and implement existing intraprocedural analyses for Bayesian-inference and the Markov decision problem, by creating corresponding interprocedural analyses. Additionally, PMAF has been used to implement a new interprocedural linear expectation-invariant analysis. Experiments with benchmark programs for the three analyses demonstrate that the approach is practical.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {513–528},
numpages = {16},
keywords = {probabilistic programming, pre-Markov algebra, expectation invariant, Program analysis},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192401,
author = {Acharya, Aravind and Bondhugula, Uday and Cohen, Albert},
title = {Polyhedral Auto-Transformation with No Integer Linear Programming},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192401},
doi = {10.1145/3296979.3192401},
abstract = {State-of-the-art algorithms used in automatic polyhedral transformation for parallelization and locality optimization typically rely on Integer Linear Programming (ILP). This poses a scalability issue when scaling to tens or hundreds of statements, and may be disconcerting in production compiler settings. In this work, we consider relaxing integrality in the ILP formulation of the Pluto algorithm, a popular algorithm used to find good affine transformations. We show that the rational solutions obtained from the relaxed LP formulation can easily be scaled to valid integral ones to obtain desired solutions, although with some caveats. We first present formal results connecting the solution of the relaxed LP to the original Pluto ILP. We then show that there are difficulties in realizing the above theoretical results in practice, and propose an alternate approach to overcome those while still leveraging linear programming. Our new approach obtains dramatic compile-time speedups for a range of large benchmarks. While achieving these compile-time improvements, we show that the performance of the transformed code is not sacrificed. Our approach to automatic transformation provides a mean compilation time improvement of 5.6\texttimes{} over state-of-the-art on relevant challenging benchmarks from the NAS PB, SPEC CPU 2006, and PolyBench suites. We also came across situations where prior frameworks failed to find a transformation in a reasonable amount of time, while our new approach did so instantaneously.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {529–542},
numpages = {14},
keywords = {Polyhedral transformation, Pluto algorithm}
}

@inproceedings{10.1145/3192366.3192401,
author = {Acharya, Aravind and Bondhugula, Uday and Cohen, Albert},
title = {Polyhedral Auto-Transformation with No Integer Linear Programming},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192401},
doi = {10.1145/3192366.3192401},
abstract = {State-of-the-art algorithms used in automatic polyhedral transformation for parallelization and locality optimization typically rely on Integer Linear Programming (ILP). This poses a scalability issue when scaling to tens or hundreds of statements, and may be disconcerting in production compiler settings. In this work, we consider relaxing integrality in the ILP formulation of the Pluto algorithm, a popular algorithm used to find good affine transformations. We show that the rational solutions obtained from the relaxed LP formulation can easily be scaled to valid integral ones to obtain desired solutions, although with some caveats. We first present formal results connecting the solution of the relaxed LP to the original Pluto ILP. We then show that there are difficulties in realizing the above theoretical results in practice, and propose an alternate approach to overcome those while still leveraging linear programming. Our new approach obtains dramatic compile-time speedups for a range of large benchmarks. While achieving these compile-time improvements, we show that the performance of the transformed code is not sacrificed. Our approach to automatic transformation provides a mean compilation time improvement of 5.6\texttimes{} over state-of-the-art on relevant challenging benchmarks from the NAS PB, SPEC CPU 2006, and PolyBench suites. We also came across situations where prior frameworks failed to find a transformation in a reasonable amount of time, while our new approach did so instantaneously.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {529–542},
numpages = {14},
keywords = {Polyhedral transformation, Pluto algorithm},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192413,
author = {Moll, Simon and Hack, Sebastian},
title = {Partial Control-Flow Linearization},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192413},
doi = {10.1145/3296979.3192413},
abstract = {If-conversion is a fundamental technique for vectorization. It accounts for the fact that in a SIMD program, several targets of a branch might be executed because of divergence. Especially for irregular data-parallel workloads, it is crucial to avoid if-converting non-divergent branches to increase SIMD utilization. In this paper, we present partial linearization, a simple and efficient if-conversion algorithm that overcomes several limitations of existing if-conversion techniques. In contrast to prior work, it has provable guarantees on which non-divergent branches are retained and will never duplicate code or insert additional branches. We show how our algorithm can be used in a classic loop vectorizer as well as to implement data-parallel languages such as ISPC or OpenCL. Furthermore, we implement prior vectorizer optimizations on top of partial linearization in a more general way. We evaluate the implementation of our algorithm in LLVM on a range of irregular data analytics kernels, a neutronics simulation benchmark and NAB, a molecular dynamics benchmark from SPEC2017 on AVX2, AVX512, and ARM Advanced SIMD machines and report speedups of up to 146 % over ICC, GCC and Clang O3.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {543–556},
numpages = {14},
keywords = {SPMD, Compiler optimizations, SIMD}
}

@inproceedings{10.1145/3192366.3192413,
author = {Moll, Simon and Hack, Sebastian},
title = {Partial Control-Flow Linearization},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192413},
doi = {10.1145/3192366.3192413},
abstract = {If-conversion is a fundamental technique for vectorization. It accounts for the fact that in a SIMD program, several targets of a branch might be executed because of divergence. Especially for irregular data-parallel workloads, it is crucial to avoid if-converting non-divergent branches to increase SIMD utilization. In this paper, we present partial linearization, a simple and efficient if-conversion algorithm that overcomes several limitations of existing if-conversion techniques. In contrast to prior work, it has provable guarantees on which non-divergent branches are retained and will never duplicate code or insert additional branches. We show how our algorithm can be used in a classic loop vectorizer as well as to implement data-parallel languages such as ISPC or OpenCL. Furthermore, we implement prior vectorizer optimizations on top of partial linearization in a more general way. We evaluate the implementation of our algorithm in LLVM on a range of irregular data analytics kernels, a neutronics simulation benchmark and NAB, a molecular dynamics benchmark from SPEC2017 on AVX2, AVX512, and ARM Advanced SIMD machines and report speedups of up to 146 % over ICC, GCC and Clang O3.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {543–556},
numpages = {14},
keywords = {Compiler optimizations, SPMD, SIMD},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@software{10.5281/zenodo.1218771,
author = {Chen, Dong and Liu, Fangzhou and Ding, Chen and Pai, Sreepathi},
title = {Software Artifact for Locality Analysis through Static Parallel Sampling},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.1218771},
abstract = {
    <p>We provide the Static Parallel Sampling (SPS) artifact that contains code, testing shell scripts and Python scripts for plotting result. We extracted the minimal code needed from the newest version of the Static Parallel Sampling tools and provide a Dockerfile to generate one working testing environment. This given artifact can reproduce all our results in Figures 5-7 (Miss ratio curve, overhead and parallel execution) in the evaluation section of the paper.</p>
},
keywords = {locality, program specialization, Static analysis}
}

@article{10.1145/3296979.3192402,
author = {Chen, Dong and Liu, Fangzhou and Ding, Chen and Pai, Sreepathi},
title = {Locality Analysis through Static Parallel Sampling},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192402},
doi = {10.1145/3296979.3192402},
abstract = {Locality analysis is important since accessing memory is much slower than computing. Compile-time locality analysis can provide detailed program-level feedback for compilers or runtime systems faster than trace-based locality analysis.  In this paper, we describe a new approach to locality analysis based on static parallel sampling. A compiler analyzes loop-based code and generates sampler code which is run to measure locality. Our approach can predict precise cache line granularity miss ratio curves for complex loops with non-linear array references and even branches. The precision and overhead of static sampling are evaluated using PolyBench and a bit-reversal loop. Our result shows that by randomly sampling 2% of loop iterations, a compiler can construct almost exact miss ratio curves as trace based analysis. Sampling 0.5% and 1% iterations can achieve good precision and efficiency with an average 0.6% to 1% the time of tracing respectively. Our analysis can also be parallelized. The analysis may assist program optimization techniques such as tiling, program co-location, cache hint selection and help to analyze write locality and parallel locality.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {557–570},
numpages = {14},
keywords = {Static analysis, locality, program specialization}
}

@inproceedings{10.1145/3192366.3192402,
author = {Chen, Dong and Liu, Fangzhou and Ding, Chen and Pai, Sreepathi},
title = {Locality Analysis through Static Parallel Sampling},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192402},
doi = {10.1145/3192366.3192402},
abstract = {Locality analysis is important since accessing memory is much slower than computing. Compile-time locality analysis can provide detailed program-level feedback for compilers or runtime systems faster than trace-based locality analysis.  In this paper, we describe a new approach to locality analysis based on static parallel sampling. A compiler analyzes loop-based code and generates sampler code which is run to measure locality. Our approach can predict precise cache line granularity miss ratio curves for complex loops with non-linear array references and even branches. The precision and overhead of static sampling are evaluated using PolyBench and a bit-reversal loop. Our result shows that by randomly sampling 2% of loop iterations, a compiler can construct almost exact miss ratio curves as trace based analysis. Sampling 0.5% and 1% iterations can achieve good precision and efficiency with an average 0.6% to 1% the time of tracing respectively. Our analysis can also be parallelized. The analysis may assist program optimization techniques such as tiling, program co-location, cache hint selection and help to analyze write locality and parallel locality.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {557–570},
numpages = {14},
keywords = {Static analysis, locality, program specialization},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192399,
author = {Cusumano-Towner, Marco and Bichsel, Benjamin and Gehr, Timon and Vechev, Martin and Mansinghka, Vikash K.},
title = {Incremental Inference for Probabilistic Programs},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192399},
doi = {10.1145/3296979.3192399},
abstract = {We present a novel approach for approximate sampling in probabilistic programs based on incremental inference. The key idea is to adapt the samples for a program P into samples for a program Q, thereby avoiding the expensive sampling computation for program Q. To enable incremental inference in probabilistic programming, our work: (i) introduces the concept of a trace translator which adapts samples from P into samples of Q, (ii) phrases this translation approach in the context of sequential Monte Carlo (SMC), which gives theoretical guarantees that the adapted samples converge to the distribution induced by Q, and (iii) shows how to obtain a concrete trace translator by establishing a correspondence between the random choices of the two probabilistic programs. We implemented our approach in two different probabilistic programming systems and showed that, compared to methods that sample the program Q from scratch, incremental inference can lead to orders of magnitude increase in efficiency, depending on how closely related P and Q are.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {571–585},
numpages = {15},
keywords = {Probabilistic programming, sequential Monte Carlo, incremental computation}
}

@inproceedings{10.1145/3192366.3192399,
author = {Cusumano-Towner, Marco and Bichsel, Benjamin and Gehr, Timon and Vechev, Martin and Mansinghka, Vikash K.},
title = {Incremental Inference for Probabilistic Programs},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192399},
doi = {10.1145/3192366.3192399},
abstract = {We present a novel approach for approximate sampling in probabilistic programs based on incremental inference. The key idea is to adapt the samples for a program P into samples for a program Q, thereby avoiding the expensive sampling computation for program Q. To enable incremental inference in probabilistic programming, our work: (i) introduces the concept of a trace translator which adapts samples from P into samples of Q, (ii) phrases this translation approach in the context of sequential Monte Carlo (SMC), which gives theoretical guarantees that the adapted samples converge to the distribution induced by Q, and (iii) shows how to obtain a concrete trace translator by establishing a correspondence between the random choices of the two probabilistic programs. We implemented our approach in two different probabilistic programming systems and showed that, compared to methods that sample the program Q from scratch, incremental inference can lead to orders of magnitude increase in efficiency, depending on how closely related P and Q are.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {571–585},
numpages = {15},
keywords = {incremental computation, sequential Monte Carlo, Probabilistic programming},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@software{10.1145/3211997,
author = {Gehr, Timon and Misailovic, Sasa and Tsankov, Petar and Vanbever, Laurent and Wiesmann, Pascal and Vechev, Martin},
title = {Artifact for the PLDI'18 Paper "Bayonet: Probabilistic Inference for Networks"},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3211997},
abstract = {
    <p>This is the version of Bayonet that was used to generate the results in the paper "Bayonet: Probabilistic Inference for Networks" Find the current version of Bayonet at: http://bayonet.ethz.ch</p>
},
keywords = {Computer Networks, Probabilistic Programming}
}

@article{10.1145/3296979.3192400,
author = {Gehr, Timon and Misailovic, Sasa and Tsankov, Petar and Vanbever, Laurent and Wiesmann, Pascal and Vechev, Martin},
title = {Bayonet: Probabilistic Inference for Networks},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192400},
doi = {10.1145/3296979.3192400},
abstract = {Network operators often need to ensure that important probabilistic properties are met, such as that the probability of network congestion is below a certain threshold. Ensuring such properties is challenging and requires both a suitable language for probabilistic networks and an automated procedure for answering probabilistic inference queries.  We present Bayonet, a novel approach that consists of: (i) a probabilistic network programming language and (ii) a system that performs probabilistic inference on Bayonet programs. The key insight behind Bayonet is to phrase the problem of probabilistic network reasoning as inference in existing probabilistic languages. As a result, Bayonet directly leverages existing probabilistic inference systems and offers a flexible and expressive interface to operators.  We present a detailed evaluation of Bayonet on common network scenarios, such as network congestion, reliability of packet delivery, and others. Our results indicate that Bayonet can express such practical scenarios and answer queries for realistic topology sizes (with up to 30 nodes).},
journal = {SIGPLAN Not.},
month = {jun},
pages = {586–602},
numpages = {17},
keywords = {Computer Networks, Probabilistic Programming}
}

@inproceedings{10.1145/3192366.3192400,
author = {Gehr, Timon and Misailovic, Sasa and Tsankov, Petar and Vanbever, Laurent and Wiesmann, Pascal and Vechev, Martin},
title = {Bayonet: Probabilistic Inference for Networks},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192400},
doi = {10.1145/3192366.3192400},
abstract = {Network operators often need to ensure that important probabilistic properties are met, such as that the probability of network congestion is below a certain threshold. Ensuring such properties is challenging and requires both a suitable language for probabilistic networks and an automated procedure for answering probabilistic inference queries.  We present Bayonet, a novel approach that consists of: (i) a probabilistic network programming language and (ii) a system that performs probabilistic inference on Bayonet programs. The key insight behind Bayonet is to phrase the problem of probabilistic network reasoning as inference in existing probabilistic languages. As a result, Bayonet directly leverages existing probabilistic inference systems and offers a flexible and expressive interface to operators.  We present a detailed evaluation of Bayonet on common network scenarios, such as network congestion, reliability of packet delivery, and others. Our results indicate that Bayonet can express such practical scenarios and answer queries for realistic topology sizes (with up to 30 nodes).},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {586–602},
numpages = {17},
keywords = {Probabilistic Programming, Computer Networks},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192409,
author = {Mansinghka, Vikash K. and Schaechtle, Ulrich and Handa, Shivam and Radul, Alexey and Chen, Yutian and Rinard, Martin},
title = {Probabilistic Programming with Programmable Inference},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192409},
doi = {10.1145/3296979.3192409},
abstract = {We introduce inference metaprogramming for probabilistic programming languages, including new language constructs, a formalism, and the rst demonstration of e ectiveness in practice. Instead of relying on rigid black-box inference algorithms hard-coded into the language implementation as in previous probabilistic programming languages, infer- ence metaprogramming enables developers to 1) dynamically decompose inference problems into subproblems, 2) apply in- ference tactics to subproblems, 3) alternate between incorpo- rating new data and performing inference over existing data, and 4) explore multiple execution traces of the probabilis- tic program at once. Implemented tactics include gradient- based optimization, Markov chain Monte Carlo, variational inference, and sequental Monte Carlo techniques. Inference metaprogramming enables the concise expression of proba- bilistic models and inference algorithms across diverse elds, such as computer vision, data science, and robotics, within a single probabilistic programming language.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {603–616},
numpages = {14},
keywords = {Semantics, Probabilistic Programming, Inference}
}

@inproceedings{10.1145/3192366.3192409,
author = {Mansinghka, Vikash K. and Schaechtle, Ulrich and Handa, Shivam and Radul, Alexey and Chen, Yutian and Rinard, Martin},
title = {Probabilistic Programming with Programmable Inference},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192409},
doi = {10.1145/3192366.3192409},
abstract = {We introduce inference metaprogramming for probabilistic programming languages, including new language constructs, a formalism, and the rst demonstration of e ectiveness in practice. Instead of relying on rigid black-box inference algorithms hard-coded into the language implementation as in previous probabilistic programming languages, infer- ence metaprogramming enables developers to 1) dynamically decompose inference problems into subproblems, 2) apply in- ference tactics to subproblems, 3) alternate between incorpo- rating new data and performing inference over existing data, and 4) explore multiple execution traces of the probabilis- tic program at once. Implemented tactics include gradient- based optimization, Markov chain Monte Carlo, variational inference, and sequental Monte Carlo techniques. Inference metaprogramming enables the concise expression of proba- bilistic models and inference algorithms across diverse elds, such as computer vision, data science, and robotics, within a single probabilistic programming language.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {603–616},
numpages = {14},
keywords = {Inference, Probabilistic Programming, Semantics},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192406,
author = {Bohrer, Brandon and Tan, Yong Kiam and Mitsch, Stefan and Myreen, Magnus O. and Platzer, Andr\'{e}},
title = {VeriPhy: Verified Controller Executables from Verified Cyber-Physical System Models},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192406},
doi = {10.1145/3296979.3192406},
abstract = {We present VeriPhy, a verified pipeline which automatically transforms verified high-level models of safety-critical cyber-physical systems (CPSs) in differential dynamic logic (dL) to verified controller executables. VeriPhy proves that all safety results are preserved end-to-end as it bridges abstraction gaps, including: i) the gap between mathematical reals in physical models and machine arithmetic in the implementation, ii) the gap between real physics and its differential-equation models, and iii) the gap between nondeterministic controller models and machine code. VeriPhy reduces CPS safety to the faithfulness of the physical environment, which is checked at runtime by synthesized, verified monitors. We use three provers in this effort: KeYmaera X, HOL4, and Isabelle/HOL. To minimize the trusted base, we cross-verify KeYmaeraX in Isabelle/HOL. We evaluate the resulting controller and monitors on commodity robotics hardware.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {617–630},
numpages = {14},
keywords = {hybrid systems, formal verification, verified executables, verified compilation, cyber-physical systems}
}

@inproceedings{10.1145/3192366.3192406,
author = {Bohrer, Brandon and Tan, Yong Kiam and Mitsch, Stefan and Myreen, Magnus O. and Platzer, Andr\'{e}},
title = {VeriPhy: Verified Controller Executables from Verified Cyber-Physical System Models},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192406},
doi = {10.1145/3192366.3192406},
abstract = {We present VeriPhy, a verified pipeline which automatically transforms verified high-level models of safety-critical cyber-physical systems (CPSs) in differential dynamic logic (dL) to verified controller executables. VeriPhy proves that all safety results are preserved end-to-end as it bridges abstraction gaps, including: i) the gap between mathematical reals in physical models and machine arithmetic in the implementation, ii) the gap between real physics and its differential-equation models, and iii) the gap between nondeterministic controller models and machine code. VeriPhy reduces CPS safety to the faithfulness of the physical environment, which is checked at runtime by synthesized, verified monitors. We use three provers in this effort: KeYmaera X, HOL4, and Isabelle/HOL. To minimize the trusted base, we cross-verify KeYmaeraX in Isabelle/HOL. We evaluate the resulting controller and monitors on commodity robotics hardware.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {617–630},
numpages = {14},
keywords = {cyber-physical systems, formal verification, hybrid systems, verified compilation, verified executables},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192377,
author = {Kang, Jeehoon and Kim, Yoonseung and Song, Youngju and Lee, Juneyoung and Park, Sanghoon and Shin, Mark Dongyeon and Kim, Yonghyun and Cho, Sungkeun and Choi, Joonwon and Hur, Chung-Kil and Yi, Kwangkeun},
title = {Crellvm: Verified Credible Compilation for LLVM},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192377},
doi = {10.1145/3296979.3192377},
abstract = {Production compilers such as GCC and LLVM are large complex software systems, for which achieving a high level of reliability is hard. Although testing is an effective method for finding bugs, it alone cannot guarantee a high level of reliability. To provide a higher level of reliability, many approaches that examine compilers' internal logics have been proposed. However, none of them have been successfully applied to major optimizations of production compilers.  This paper presents Crellvm: a verified credible compilation framework for LLVM, which can be used as a systematic way of providing a high level of reliability for major optimizations in LLVM. Specifically, we augment an LLVM optimizer to generate translation results together with their correctness proofs, which can then be checked by a proof checker formally verified in Coq. As case studies, we applied our approach to two major optimizations of LLVM: register promotion mem2reg and global value numbering gvn, having found four new miscompilation bugs (two in each).},
journal = {SIGPLAN Not.},
month = {jun},
pages = {631–645},
numpages = {15},
keywords = {compiler verification, LLVM, Coq, translation validation, relational Hoare logic, credible compilation}
}

@inproceedings{10.1145/3192366.3192377,
author = {Kang, Jeehoon and Kim, Yoonseung and Song, Youngju and Lee, Juneyoung and Park, Sanghoon and Shin, Mark Dongyeon and Kim, Yonghyun and Cho, Sungkeun and Choi, Joonwon and Hur, Chung-Kil and Yi, Kwangkeun},
title = {Crellvm: Verified Credible Compilation for LLVM},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192377},
doi = {10.1145/3192366.3192377},
abstract = {Production compilers such as GCC and LLVM are large complex software systems, for which achieving a high level of reliability is hard. Although testing is an effective method for finding bugs, it alone cannot guarantee a high level of reliability. To provide a higher level of reliability, many approaches that examine compilers' internal logics have been proposed. However, none of them have been successfully applied to major optimizations of production compilers.  This paper presents Crellvm: a verified credible compilation framework for LLVM, which can be used as a systematic way of providing a high level of reliability for major optimizations in LLVM. Specifically, we augment an LLVM optimizer to generate translation results together with their correctness proofs, which can then be checked by a proof checker formally verified in Coq. As case studies, we applied our approach to two major optimizations of LLVM: register promotion mem2reg and global value numbering gvn, having found four new miscompilation bugs (two in each).},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {631–645},
numpages = {15},
keywords = {translation validation, LLVM, relational Hoare logic, credible compilation, Coq, compiler verification},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192381,
author = {Gu, Ronghui and Shao, Zhong and Kim, Jieung and Wu, Xiongnan (Newman) and Koenig, J\'{e}r\'{e}mie and Sj\"{o}berg, Vilhelm and Chen, Hao and Costanzo, David and Ramananandro, Tahina},
title = {Certified Concurrent Abstraction Layers},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192381},
doi = {10.1145/3296979.3192381},
abstract = {Concurrent abstraction layers are ubiquitous in modern computer systems because of the pervasiveness of multithreaded programming and multicore hardware. Abstraction layers are used to hide the implementation details (e.g., fine-grained synchronization) and reduce the complex dependencies among components at different levels of abstraction. Despite their obvious importance, concurrent abstraction layers have not been treated formally. This severely limits the applicability of layer-based techniques and makes it difficult to scale verification across multiple concurrent layers.  In this paper, we present CCAL---a fully mechanized programming toolkit developed under the CertiKOS project---for specifying, composing, compiling, and linking certified concurrent abstraction layers. CCAL consists of three technical novelties: a new game-theoretical, strategy-based compositional semantic model for concurrency (and its associated program verifiers), a set of formal linking theorems for composing multithreaded and multicore concurrent layers, and a new CompCertX compiler that supports certified thread-safe compilation and linking. The CCAL toolkit is implemented in Coq and supports layered concurrent programming in both C and assembly. It has been successfully applied to build a fully certified concurrent OS kernel with fine-grained locking.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {646–661},
numpages = {16},
keywords = {certified OS kernels, modularity, concurrency, certified compilers, verification, abstraction layer}
}

@inproceedings{10.1145/3192366.3192381,
author = {Gu, Ronghui and Shao, Zhong and Kim, Jieung and Wu, Xiongnan (Newman) and Koenig, J\'{e}r\'{e}mie and Sj\"{o}berg, Vilhelm and Chen, Hao and Costanzo, David and Ramananandro, Tahina},
title = {Certified Concurrent Abstraction Layers},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192381},
doi = {10.1145/3192366.3192381},
abstract = {Concurrent abstraction layers are ubiquitous in modern computer systems because of the pervasiveness of multithreaded programming and multicore hardware. Abstraction layers are used to hide the implementation details (e.g., fine-grained synchronization) and reduce the complex dependencies among components at different levels of abstraction. Despite their obvious importance, concurrent abstraction layers have not been treated formally. This severely limits the applicability of layer-based techniques and makes it difficult to scale verification across multiple concurrent layers.  In this paper, we present CCAL---a fully mechanized programming toolkit developed under the CertiKOS project---for specifying, composing, compiling, and linking certified concurrent abstraction layers. CCAL consists of three technical novelties: a new game-theoretical, strategy-based compositional semantic model for concurrency (and its associated program verifiers), a set of formal linking theorems for composing multithreaded and multicore concurrent layers, and a new CompCertX compiler that supports certified thread-safe compilation and linking. The CCAL toolkit is implemented in Coq and supports layered concurrent programming in both C and assembly. It has been successfully applied to build a fully certified concurrent OS kernel with fine-grained locking.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {646–661},
numpages = {16},
keywords = {certified OS kernels, abstraction layer, verification, modularity, concurrency, certified compilers},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192414,
author = {Taube, Marcelo and Losa, Giuliano and McMillan, Kenneth L. and Padon, Oded and Sagiv, Mooly and Shoham, Sharon and Wilcox, James R. and Woos, Doug},
title = {Modularity for Decidability of Deductive Verification with Applications to Distributed Systems},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192414},
doi = {10.1145/3296979.3192414},
abstract = {Proof automation can substantially increase productivity in formal verification of complex systems. However, unpredictablility of automated provers in handling quantified formulas presents a major hurdle to usability of these tools. We propose to solve this problem not by improving the provers, but by using a modular proof methodology that allows us to produce decidable verification conditions. Decidability greatly improves predictability of proof automation, resulting in a more practical verification approach. We apply this methodology to develop verified implementations of distributed protocols, demonstrating its effectiveness.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {662–677},
numpages = {16},
keywords = {Distributed systems, Paxos, Formal verification, Decidable logic, Raft, Ivy, Modularity}
}

@inproceedings{10.1145/3192366.3192414,
author = {Taube, Marcelo and Losa, Giuliano and McMillan, Kenneth L. and Padon, Oded and Sagiv, Mooly and Shoham, Sharon and Wilcox, James R. and Woos, Doug},
title = {Modularity for Decidability of Deductive Verification with Applications to Distributed Systems},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192414},
doi = {10.1145/3192366.3192414},
abstract = {Proof automation can substantially increase productivity in formal verification of complex systems. However, unpredictablility of automated provers in handling quantified formulas presents a major hurdle to usability of these tools. We propose to solve this problem not by improving the provers, but by using a modular proof methodology that allows us to produce decidable verification conditions. Decidability greatly improves predictability of proof automation, resulting in a more practical verification approach. We apply this methodology to develop verified implementations of distributed protocols, demonstrating its effectiveness.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {662–677},
numpages = {16},
keywords = {Decidable logic, Distributed systems, Ivy, Modularity, Raft, Paxos, Formal verification},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192383,
author = {Bastani, Osbert and Sharma, Rahul and Aiken, Alex and Liang, Percy},
title = {Active Learning of Points-to Specifications},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192383},
doi = {10.1145/3296979.3192383},
abstract = {When analyzing programs, large libraries pose significant challenges to static points-to analysis. A popular solution is to have a human analyst provide points-to specifications that summarize relevant behaviors of library code, which can substantially improve precision and handle missing code such as native code. We propose Atlas, a tool that automatically infers points-to specifications. Atlas synthesizes unit tests that exercise the library code, and then infers points-to specifications based on observations from these executions. Atlas automatically infers specifications for the Java standard library, and produces better results for a client static information flow analysis on a benchmark of 46 Android apps compared to using existing handwritten specifications.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {678–692},
numpages = {15},
keywords = {specification inference, static points-to analysis}
}

@inproceedings{10.1145/3192366.3192383,
author = {Bastani, Osbert and Sharma, Rahul and Aiken, Alex and Liang, Percy},
title = {Active Learning of Points-to Specifications},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192383},
doi = {10.1145/3192366.3192383},
abstract = {When analyzing programs, large libraries pose significant challenges to static points-to analysis. A popular solution is to have a human analyst provide points-to specifications that summarize relevant behaviors of library code, which can substantially improve precision and handle missing code such as native code. We propose Atlas, a tool that automatically infers points-to specifications. Atlas synthesizes unit tests that exercise the library code, and then infers points-to specifications based on observations from these executions. Atlas automatically infers specifications for the Java standard library, and produces better results for a client static information flow analysis on a benchmark of 46 Android apps compared to using existing handwritten specifications.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {678–692},
numpages = {15},
keywords = {static points-to analysis, specification inference},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192418,
author = {Shi, Qingkai and Xiao, Xiao and Wu, Rongxin and Zhou, Jinguo and Fan, Gang and Zhang, Charles},
title = {Pinpoint: Fast and Precise Sparse Value Flow Analysis for Million Lines of Code},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192418},
doi = {10.1145/3296979.3192418},
abstract = {When dealing with millions of lines of code, we still cannot have the cake and eat it: sparse value-flow analysis is powerful in checking source-sink problems, but existing work cannot escape from the “pointer trap” – a precise points-to analysis limits its scalability and an imprecise one seriously undermines its precision. We present Pinpoint, a holistic approach that decomposes the cost of high-precision points-to analysis by precisely discovering local data dependence and delaying the expensive inter-procedural analysis through memorization. Such memorization enables the on-demand slicing of only the necessary inter-procedural data dependence and path feasibility queries, which are then solved by a costly SMT solver. Experiments show that Pinpoint can check programs such as MySQL (around 2 million lines of code) within 1.5 hours. The overall false positive rate is also very low (14.3% - 23.6%). Pinpoint has discovered over forty real bugs in mature and extensively checked open source systems. And the implementation of Pinpoint and all experimental results are freely available.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {693–706},
numpages = {14},
keywords = {error detection, Sparse program analysis, path-sensitive analysis}
}

@inproceedings{10.1145/3192366.3192418,
author = {Shi, Qingkai and Xiao, Xiao and Wu, Rongxin and Zhou, Jinguo and Fan, Gang and Zhang, Charles},
title = {Pinpoint: Fast and Precise Sparse Value Flow Analysis for Million Lines of Code},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192418},
doi = {10.1145/3192366.3192418},
abstract = {When dealing with millions of lines of code, we still cannot have the cake and eat it: sparse value-flow analysis is powerful in checking source-sink problems, but existing work cannot escape from the “pointer trap” – a precise points-to analysis limits its scalability and an imprecise one seriously undermines its precision. We present Pinpoint, a holistic approach that decomposes the cost of high-precision points-to analysis by precisely discovering local data dependence and delaying the expensive inter-procedural analysis through memorization. Such memorization enables the on-demand slicing of only the necessary inter-procedural data dependence and path feasibility queries, which are then solved by a costly SMT solver. Experiments show that Pinpoint can check programs such as MySQL (around 2 million lines of code) within 1.5 hours. The overall false positive rate is also very low (14.3% - 23.6%). Pinpoint has discovered over forty real bugs in mature and extensively checked open source systems. And the implementation of Pinpoint and all experimental results are freely available.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {693–706},
numpages = {14},
keywords = {path-sensitive analysis, error detection, Sparse program analysis},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@software{10.1145/3212003,
author = {Zhu, He and Magill, Stephen and Jagannathan, Suresh},
title = {Artifact for Paper: A Data-Driven CHC Solver (PLDI 2018)},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3212003},
abstract = {
    <p>LinearArbitrary-SeaHorn: A CHC solver for LLVM-based languages</p>
},
keywords = {Constrained Horn Clauses (CHCs), Data-Driven Analysis, Invariant Inference, Program Verification}
}

@article{10.1145/3296979.3192416,
author = {Zhu, He and Magill, Stephen and Jagannathan, Suresh},
title = {A Data-Driven CHC Solver},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192416},
doi = {10.1145/3296979.3192416},
abstract = {We present a data-driven technique to solve Constrained Horn Clauses (CHCs) that encode verification conditions of programs containing unconstrained loops and recursions. Our CHC solver neither constrains the search space from which a predicate's components are inferred (e.g., by constraining the number of variables or the values of coefficients used to specify an invariant), nor fixes the shape of the predicate itself (e.g., by bounding the number and kind of logical connectives). Instead, our approach is based on a novel machine learning-inspired tool chain that synthesizes CHC solutions in terms of arbitrary Boolean combinations of unrestricted atomic predicates. A CEGAR-based verification loop inside the solver progressively samples representative positive and negative data from recursive CHCs, which is fed to the machine learning tool chain. Our solver is implemented as an LLVM pass in the SeaHorn verification framework and has been used to successfully verify a large number of nontrivial and challenging C programs from the literature and well-known benchmark suites (e.g., SV-COMP).},
journal = {SIGPLAN Not.},
month = {jun},
pages = {707–721},
numpages = {15},
keywords = {Invariant Inference, Data-Driven Analysis, Program Verification, Constrained Horn Clauses (CHCs)}
}

@inproceedings{10.1145/3192366.3192416,
author = {Zhu, He and Magill, Stephen and Jagannathan, Suresh},
title = {A Data-Driven CHC Solver},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192416},
doi = {10.1145/3192366.3192416},
abstract = {We present a data-driven technique to solve Constrained Horn Clauses (CHCs) that encode verification conditions of programs containing unconstrained loops and recursions. Our CHC solver neither constrains the search space from which a predicate's components are inferred (e.g., by constraining the number of variables or the values of coefficients used to specify an invariant), nor fixes the shape of the predicate itself (e.g., by bounding the number and kind of logical connectives). Instead, our approach is based on a novel machine learning-inspired tool chain that synthesizes CHC solutions in terms of arbitrary Boolean combinations of unrestricted atomic predicates. A CEGAR-based verification loop inside the solver progressively samples representative positive and negative data from recursive CHCs, which is fed to the machine learning tool chain. Our solver is implemented as an LLVM pass in the SeaHorn verification framework and has been used to successfully verify a large number of nontrivial and challenging C programs from the literature and well-known benchmark suites (e.g., SV-COMP).},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {707–721},
numpages = {15},
keywords = {Constrained Horn Clauses (CHCs), Data-Driven Analysis, Invariant Inference, Program Verification},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192417,
author = {Raghothaman, Mukund and Kulkarni, Sulekha and Heo, Kihong and Naik, Mayur},
title = {User-Guided Program Reasoning Using Bayesian Inference},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192417},
doi = {10.1145/3296979.3192417},
abstract = {Program analyses necessarily make approximations that often lead them to report true alarms interspersed with many false alarms. We propose a new approach to leverage user feedback to guide program analyses towards true alarms and away from false alarms. Our approach associates each alarm with a confidence value by performing Bayesian inference on a probabilistic model derived from the analysis rules. In each iteration, the user inspects the alarm with the highest confidence and labels its ground truth, and the approach recomputes the confidences of the remaining alarms given this feedback. It thereby maximizes the return on the effort by the user in inspecting each alarm. We have implemented our approach in a tool named Bingo for program analyses expressed in Datalog. Experiments with real users and two sophisticated analyses---a static datarace analysis for Java programs and a static taint analysis for Android apps---show significant improvements on a range of metrics, including false alarm rates and number of bugs found.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {722–735},
numpages = {14},
keywords = {Bayesian inference, Static analysis, belief networks, alarm ranking}
}

@inproceedings{10.1145/3192366.3192417,
author = {Raghothaman, Mukund and Kulkarni, Sulekha and Heo, Kihong and Naik, Mayur},
title = {User-Guided Program Reasoning Using Bayesian Inference},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192417},
doi = {10.1145/3192366.3192417},
abstract = {Program analyses necessarily make approximations that often lead them to report true alarms interspersed with many false alarms. We propose a new approach to leverage user feedback to guide program analyses towards true alarms and away from false alarms. Our approach associates each alarm with a confidence value by performing Bayesian inference on a probabilistic model derived from the analysis rules. In each iteration, the user inspects the alarm with the highest confidence and labels its ground truth, and the approach recomputes the confidences of the remaining alarms given this feedback. It thereby maximizes the return on the effort by the user in inspecting each alarm. We have implemented our approach in a tool named Bingo for program analyses expressed in Datalog. Experiments with real users and two sophisticated analyses---a static datarace analysis for Java programs and a static taint analysis for Android apps---show significant improvements on a range of metrics, including false alarm rates and number of bugs found.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {722–735},
numpages = {14},
keywords = {Bayesian inference, Static analysis, alarm ranking, belief networks},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192397,
author = {Hong, Changwan and Sukumaran-Rajam, Aravind and Kim, Jinsung and Rawat, Prashant Singh and Krishnamoorthy, Sriram and Pouchet, Louis-No\"{e}l and Rastello, Fabrice and Sadayappan, P.},
title = {GPU Code Optimization Using Abstract Kernel Emulation and Sensitivity Analysis},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192397},
doi = {10.1145/3296979.3192397},
abstract = {In this paper, we develop an approach to GPU kernel optimization by focusing on identification of bottleneck resources and determining optimization parameters that can alleviate the bottleneck. Performance modeling for GPUs is done by abstract kernel emulation along with latency/gap modeling of resources. Sensitivity analysis with respect to resource latency/gap parameters is used to predict the bottleneck resource for a given kernel's execution. The utility of the bottleneck analysis is demonstrated in two contexts: 1) Coupling the new bottleneck-driven optimization strategy with the OpenTuner auto-tuner: experimental results on all kernels from the Rodinia suite and GPU tensor contraction kernels from the NWChem computational chemistry suite demonstrate effectiveness. 2) Manual code optimization: two case studies illustrate the use of the bottleneck analysis to iteratively improve the performance of code from state-of-the-art domain-specific code generators.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {736–751},
numpages = {16},
keywords = {sensitivity analysis, GPU, Performance modeling, abstract emulation, bottleneck analysis}
}

@inproceedings{10.1145/3192366.3192397,
author = {Hong, Changwan and Sukumaran-Rajam, Aravind and Kim, Jinsung and Rawat, Prashant Singh and Krishnamoorthy, Sriram and Pouchet, Louis-No\"{e}l and Rastello, Fabrice and Sadayappan, P.},
title = {GPU Code Optimization Using Abstract Kernel Emulation and Sensitivity Analysis},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192397},
doi = {10.1145/3192366.3192397},
abstract = {In this paper, we develop an approach to GPU kernel optimization by focusing on identification of bottleneck resources and determining optimization parameters that can alleviate the bottleneck. Performance modeling for GPUs is done by abstract kernel emulation along with latency/gap modeling of resources. Sensitivity analysis with respect to resource latency/gap parameters is used to predict the bottleneck resource for a given kernel's execution. The utility of the bottleneck analysis is demonstrated in two contexts: 1) Coupling the new bottleneck-driven optimization strategy with the OpenTuner auto-tuner: experimental results on all kernels from the Rodinia suite and GPU tensor contraction kernels from the NWChem computational chemistry suite demonstrate effectiveness. 2) Manual code optimization: two case studies illustrate the use of the bottleneck analysis to iteratively improve the performance of code from state-of-the-art domain-specific code generators.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {736–751},
numpages = {16},
keywords = {bottleneck analysis, GPU, abstract emulation, Performance modeling, sensitivity analysis},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192404,
author = {Dathathri, Roshan and Gill, Gurbinder and Hoang, Loc and Dang, Hoang-Vu and Brooks, Alex and Dryden, Nikoli and Snir, Marc and Pingali, Keshav},
title = {Gluon: A Communication-Optimizing Substrate for Distributed Heterogeneous Graph Analytics},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192404},
doi = {10.1145/3296979.3192404},
abstract = {This paper introduces a new approach to building distributed-memory graph analytics systems that exploits heterogeneity in processor types (CPU and GPU), partitioning policies, and programming models. The key to this approach is Gluon, a communication-optimizing substrate. Programmers write applications in a shared-memory programming system of their choice and interface these applications with Gluon using a lightweight API. Gluon enables these programs to run on heterogeneous clusters and optimizes communication in a novel way by exploiting structural and temporal invariants of graph partitioning policies. To demonstrate Gluon’s ability to support different programming models, we interfaced Gluon with the Galois and Ligra shared-memory graph analytics systems to produce distributed-memory versions of these systems named D-Galois and D-Ligra, respectively. To demonstrate Gluon’s ability to support heterogeneous processors, we interfaced Gluon with IrGL, a state-of-the-art single-GPU system for graph analytics, to produce D-IrGL, the first multi-GPU distributed-memory graph analytics system. Our experiments were done on CPU clusters with up to 256 hosts and roughly 70,000 threads and on multi-GPU clusters with up to 64 GPUs. The communication optimizations in Gluon improve end-to-end application execution time by ∼2.6\texttimes{} on the average. D-Galois and D-IrGL scale well and are faster than Gemini, the state-of-the-art distributed CPU graph analytics system, by factors of ∼3.9\texttimes{} and ∼4.9\texttimes{}, respectively, on the average.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {752–768},
numpages = {17},
keywords = {Distributed-memory graph analytics, communication optimizations, heterogeneous architectures, big data, GPUs}
}

@inproceedings{10.1145/3192366.3192404,
author = {Dathathri, Roshan and Gill, Gurbinder and Hoang, Loc and Dang, Hoang-Vu and Brooks, Alex and Dryden, Nikoli and Snir, Marc and Pingali, Keshav},
title = {Gluon: A Communication-Optimizing Substrate for Distributed Heterogeneous Graph Analytics},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192404},
doi = {10.1145/3192366.3192404},
abstract = {This paper introduces a new approach to building distributed-memory graph analytics systems that exploits heterogeneity in processor types (CPU and GPU), partitioning policies, and programming models. The key to this approach is Gluon, a communication-optimizing substrate. Programmers write applications in a shared-memory programming system of their choice and interface these applications with Gluon using a lightweight API. Gluon enables these programs to run on heterogeneous clusters and optimizes communication in a novel way by exploiting structural and temporal invariants of graph partitioning policies. To demonstrate Gluon’s ability to support different programming models, we interfaced Gluon with the Galois and Ligra shared-memory graph analytics systems to produce distributed-memory versions of these systems named D-Galois and D-Ligra, respectively. To demonstrate Gluon’s ability to support heterogeneous processors, we interfaced Gluon with IrGL, a state-of-the-art single-GPU system for graph analytics, to produce D-IrGL, the first multi-GPU distributed-memory graph analytics system. Our experiments were done on CPU clusters with up to 256 hosts and roughly 70,000 threads and on multi-GPU clusters with up to 64 GPUs. The communication optimizations in Gluon improve end-to-end application execution time by ∼2.6\texttimes{} on the average. D-Galois and D-IrGL scale well and are faster than Gemini, the state-of-the-art distributed CPU graph analytics system, by factors of ∼3.9\texttimes{} and ∼4.9\texttimes{}, respectively, on the average.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {752–768},
numpages = {17},
keywords = {Distributed-memory graph analytics, big data, heterogeneous architectures, GPUs, communication optimizations},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192391,
author = {Acar, Umut A. and Chargu\'{e}raud, Arthur and Guatto, Adrien and Rainey, Mike and Sieczkowski, Filip},
title = {Heartbeat Scheduling: Provable Efficiency for Nested Parallelism},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192391},
doi = {10.1145/3296979.3192391},
abstract = {A classic problem in parallel computing is to take a high-level parallel program written, for example, in nested-parallel style with fork-join constructs and run it efficiently on a real machine. The problem could be considered solved in theory, but not in practice, because the overheads of creating and managing parallel threads can overwhelm their benefits. Developing efficient parallel codes therefore usually requires extensive tuning and optimizations to reduce parallelism just to a point where the overheads become acceptable.  In this paper, we present a scheduling technique that delivers provably efficient results for arbitrary nested-parallel programs, without the tuning needed for controlling parallelism overheads. The basic idea behind our technique is to create threads only at a beat (which we refer to as the "heartbeat") and make sure to do useful work in between. We specify our heartbeat scheduler using an abstract-machine semantics and provide mechanized proofs that the scheduler guarantees low overheads for all nested parallel programs. We present a prototype C++ implementation and an evaluation that shows that Heartbeat competes well with manually optimized Cilk Plus codes, without requiring manual tuning.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {769–782},
numpages = {14},
keywords = {granularity control, parallel programming languages}
}

@inproceedings{10.1145/3192366.3192391,
author = {Acar, Umut A. and Chargu\'{e}raud, Arthur and Guatto, Adrien and Rainey, Mike and Sieczkowski, Filip},
title = {Heartbeat Scheduling: Provable Efficiency for Nested Parallelism},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192391},
doi = {10.1145/3192366.3192391},
abstract = {A classic problem in parallel computing is to take a high-level parallel program written, for example, in nested-parallel style with fork-join constructs and run it efficiently on a real machine. The problem could be considered solved in theory, but not in practice, because the overheads of creating and managing parallel threads can overwhelm their benefits. Developing efficient parallel codes therefore usually requires extensive tuning and optimizations to reduce parallelism just to a point where the overheads become acceptable.  In this paper, we present a scheduling technique that delivers provably efficient results for arbitrary nested-parallel programs, without the tuning needed for controlling parallelism overheads. The basic idea behind our technique is to create threads only at a beat (which we refer to as the "heartbeat") and make sure to do useful work in between. We specify our heartbeat scheduler using an abstract-machine semantics and provide mechanized proofs that the scheduler guarantees low overheads for all nested parallel programs. We present a prototype C++ implementation and an evaluation that shows that Heartbeat competes well with manually optimized Cilk Plus codes, without requiring manual tuning.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {769–782},
numpages = {14},
keywords = {granularity control, parallel programming languages},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192389,
author = {Serrano, Alejandro and Hage, Jurriaan and Vytiniotis, Dimitrios and Peyton Jones, Simon},
title = {Guarded Impredicative Polymorphism},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192389},
doi = {10.1145/3296979.3192389},
abstract = {The design space for type systems that support impredicative instantiation is extremely complicated. One needs to strike a balance between expressiveness, simplicity for both the end programmer and the type system implementor, and how easily the system can be integrated with other advanced type system concepts. In this paper, we propose a new point in the design space, which we call guarded impredicativity. Its key idea is that impredicative instantiation in an application is allowed for type variables that occur under a type constructor. The resulting type system has a clean declarative specification — making it easy for programmers to predict what will type and what will not —, allows for a smooth integration with GHC’s OutsideIn(X) constraint solving framework, while giving up very little in terms of expressiveness compared to systems like HMF, HML, FPH and MLF. We give a sound and complete inference algorithm, and prove a principal type property for our system.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {783–796},
numpages = {14},
keywords = {Type systems, constraint-based inference, impredicative polymorphism}
}

@inproceedings{10.1145/3192366.3192389,
author = {Serrano, Alejandro and Hage, Jurriaan and Vytiniotis, Dimitrios and Peyton Jones, Simon},
title = {Guarded Impredicative Polymorphism},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192389},
doi = {10.1145/3192366.3192389},
abstract = {The design space for type systems that support impredicative instantiation is extremely complicated. One needs to strike a balance between expressiveness, simplicity for both the end programmer and the type system implementor, and how easily the system can be integrated with other advanced type system concepts. In this paper, we propose a new point in the design space, which we call guarded impredicativity. Its key idea is that impredicative instantiation in an application is allowed for type variables that occur under a type constructor. The resulting type system has a clean declarative specification — making it easy for programmers to predict what will type and what will not —, allows for a smooth integration with GHC’s OutsideIn(X) constraint solving framework, while giving up very little in terms of expressiveness compared to systems like HMF, HML, FPH and MLF. We give a sound and complete inference algorithm, and prove a principal type property for our system.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {783–796},
numpages = {14},
keywords = {impredicative polymorphism, Type systems, constraint-based inference},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192372,
author = {Bowman, William J. and Ahmed, Amal},
title = {Typed Closure Conversion for the Calculus of Constructions},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192372},
doi = {10.1145/3296979.3192372},
abstract = {Dependently typed languages such as Coq are used to specify and verify the full functional correctness of source programs. Type-preserving compilation can be used to preserve these specifications and proofs of correctness through compilation into the generated target-language programs. Unfortunately, type-preserving compilation of dependent types is hard. In essence, the problem is that dependent type systems are designed around high-level compositional abstractions to decide type checking, but compilation interferes with the type-system rules for reasoning about run-time terms. We develop a type-preserving closure-conversion translation from the Calculus of Constructions (CC) with strong dependent pairs (Σ types)—a subset of the core language of Coq—to a type-safe, dependently typed compiler intermediate language named CC-CC. The central challenge in this work is how to translate the source type-system rules for reasoning about functions into target type-system rules for reasoning about closures. To justify these rules, we prove soundness of CC-CC by giving a model in CC. In addition to type preservation, we prove correctness of separate compilation.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {797–811},
numpages = {15},
keywords = {closure conversion, type theory, type-preserving compilation, Dependent types}
}

@inproceedings{10.1145/3192366.3192372,
author = {Bowman, William J. and Ahmed, Amal},
title = {Typed Closure Conversion for the Calculus of Constructions},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192372},
doi = {10.1145/3192366.3192372},
abstract = {Dependently typed languages such as Coq are used to specify and verify the full functional correctness of source programs. Type-preserving compilation can be used to preserve these specifications and proofs of correctness through compilation into the generated target-language programs. Unfortunately, type-preserving compilation of dependent types is hard. In essence, the problem is that dependent type systems are designed around high-level compositional abstractions to decide type checking, but compilation interferes with the type-system rules for reasoning about run-time terms. We develop a type-preserving closure-conversion translation from the Calculus of Constructions (CC) with strong dependent pairs (Σ types)—a subset of the core language of Coq—to a type-safe, dependently typed compiler intermediate language named CC-CC. The central challenge in this work is how to translate the source type-system rules for reasoning about functions into target type-system rules for reasoning about closures. To justify these rules, we prove soundness of CC-CC by giving a model in CC. In addition to type preservation, we prove correctness of separate compilation.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {797–811},
numpages = {15},
keywords = {Dependent types, type-preserving compilation, type theory, closure conversion},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@software{10.1145/3212005,
author = {Pombrio, Justin and Krishnamurthi, Shriram},
title = {PLDI 2018 Artifact for Inferring Type Rules for Syntactic Sugar},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3212005},
abstract = {
    <p>This is the artifact for the paper "Inferring Type Rules for Syntactic Sugar" by Justin Pombrio and Shriram Krishnamurthi. Type systems and syntactic sugar are both valuable to programmers, but sometimes at odds. While sugar is a valuable mechanism for implementing realistic languages, the expansion process obscures program source structure. As a result, type errors can reference terms the programmers did not write (and even constructs they do not know), baffling them. The language developer must also manually construct type rules for the sugars, to give a typed account of the surface language. We address these problems by presenting a process for automatically reconstructing type rules for the surface language using rules for the core. We have implemented this theory, and show several interesting case studies.</p>
},
keywords = {Macros, Programming Languages, Resugaring, Syntactic Sugar, Type Systems}
}

@article{10.1145/3296979.3192398,
author = {Pombrio, Justin and Krishnamurthi, Shriram},
title = {Inferring Type Rules for Syntactic Sugar},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192398},
doi = {10.1145/3296979.3192398},
abstract = {Type systems and syntactic sugar are both valuable to programmers, but sometimes at odds. While sugar is a valuable mechanism for implementing realistic languages, the expansion process obscures program source structure. As a result, type errors can reference terms the programmers did not write (and even constructs they do not know), baffling them. The language developer must also manually construct type rules for the sugars, to give a typed account of the surface language. We address these problems by presenting a process for automatically reconstructing type rules for the surface language using rules for the core. We have implemented this theory, and show several interesting case studies.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {812–825},
numpages = {14},
keywords = {Type Systems, Programming Languages, Syntactic Sugar, Macros, Resugaring}
}

@inproceedings{10.1145/3192366.3192398,
author = {Pombrio, Justin and Krishnamurthi, Shriram},
title = {Inferring Type Rules for Syntactic Sugar},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192398},
doi = {10.1145/3192366.3192398},
abstract = {Type systems and syntactic sugar are both valuable to programmers, but sometimes at odds. While sugar is a valuable mechanism for implementing realistic languages, the expansion process obscures program source structure. As a result, type errors can reference terms the programmers did not write (and even constructs they do not know), baffling them. The language developer must also manually construct type rules for the sugars, to give a typed account of the surface language. We address these problems by presenting a process for automatically reconstructing type rules for the surface language using rules for the core. We have implemented this theory, and show several interesting case studies.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {812–825},
numpages = {14},
keywords = {Programming Languages, Syntactic Sugar, Macros, Resugaring, Type Systems},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

