@inproceedings{10.1145/3385412.3385967,
author = {Miltner, Anders and Padhi, Saswat and Millstein, Todd and Walker, David},
title = {Data-Driven Inference of Representation Invariants},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385967},
doi = {10.1145/3385412.3385967},
abstract = {A representation invariant is a property that holds of all values of abstract type produced by a module. Representation invariants play important roles in software engineering and program verification. In this paper, we develop a counterexample-driven algorithm for inferring a representation invariant that is sufficient to imply a desired specification for a module. The key novelty is a type-directed notion of visible inductiveness, which ensures that the algorithm makes progress toward its goal as it alternates between weakening and strengthening candidate invariants. The algorithm is parameterized by an example-based synthesis engine and a verifier, and we prove that it is sound and complete for first-order modules over finite types, assuming that the synthesizer and verifier are as well. We implement these ideas in a tool called Hanoi, which synthesizes representation invariants for recursive data types. Hanoi not only handles invariants for first-order code, but higher-order code as well. In its back end, Hanoi uses an enumerative synthesizer called Myth and an enumerative testing tool as a verifier. Because Hanoi uses testing for verification, it is not sound, though our empirical evaluation shows that it is successful on the benchmarks we investigated.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1–15},
numpages = {15},
keywords = {Type-Directed Synthesis, Abstract Data Types, Logical Relations},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395638,
author = {Miltner, Anders and Padhi, Saswat and Millstein, Todd and Walker, David},
title = {Replication Package for Artifact: Data-Driven Inference of Representation Invariants},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395638},
abstract = {
    <p>This archive contains the code and benchmarks for the paper Data-Driven Inference of Representation Invariants.</p>
<p>Instructions for installation and build, instructions for reproduction of the results, and a description of the structure of the repository, are all available in the file $/README.pdf.</p>

},
keywords = {Abstract Data Types, Logical Relations, Type-Directed Synthesis}
}

@inproceedings{10.1145/3385412.3386005,
author = {Sakkas, Georgios and Endres, Madeline and Cosman, Benjamin and Weimer, Westley and Jhala, Ranjit},
title = {Type Error Feedback via Analytic Program Repair},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386005},
doi = {10.1145/3385412.3386005},
abstract = {We introduce Analytic Program Repair, a data-driven strategy for providing feedback for type-errors via repairs for the erroneous program. Our strategy is based on insight that similar errors have similar repairs. Thus, we show how to use a training dataset of pairs of ill-typed programs and their fixed versions to: (1)&nbsp;learn a collection of candidate repair templates by abstracting and partitioning the edits made in the training set into a representative set of templates; (2)&nbsp;predict the appropriate template from a given error, by training multi-class classifiers on the repair templates used in the training set; (3)&nbsp;synthesize a concrete repair from the template by enumerating and ranking correct (e.g. well-typed) terms matching the predicted template. We have implemented our approach in Rite: a type error reporting tool for OCaml programs. We present an evaluation of the accuracy and efficiency of Rite on a corpus of 4,500 ill-typed Ocaml programs drawn from two instances of an introductory programming course, and a user-study of the quality of the generated error messages that shows the locations and final repair quality to be better than the state-of-the-art tool in a statistically-significant manner.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {16–30},
numpages = {15},
keywords = {Machine Learning, Program Repair, Program Synthesis, Type Error Feedback},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395640,
author = {Sakkas, Georgios and Endres, Madeline and Cosman, Benjamin and Weimer, Westley and Jhala, Ranjit},
title = {Replication Package for Article: Type Error Feedback via Analytic Program Repair},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395640},
abstract = {
    <p>The artifact contains all the necessary code and data in order to reproduce the results from the paper "Type Error Feedback via Analytic Program Repair" as appeared in PLDI 2020. Instructions for installing and running the artifact can be found in the provided README file.</p>

},
keywords = {Machine Learning, Program Repair, Program Synthesis, Type Error Feedback}
}

@inproceedings{10.1145/3385412.3386012,
author = {Nandi, Chandrakana and Willsey, Max and Anderson, Adam and Wilcox, James R. and Darulova, Eva and Grossman, Dan and Tatlock, Zachary},
title = {Synthesizing Structured CAD Models with Equality Saturation and Inverse Transformations},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386012},
doi = {10.1145/3385412.3386012},
abstract = {Recent program synthesis techniques help users customize CAD models(e.g., for 3D printing) by decompiling low-level triangle meshes to Constructive Solid Geometry (CSG) expressions. Without loops or functions, editing CSG can require many coordinated changes, and existing mesh decompilers use heuristics that can obfuscate high-level structure.  This paper proposes a second decompilation stage to robustly "shrink" unstructured CSG expressions into more editable programs with map and fold operators. We present Szalinski, a tool that uses Equality Saturation with semantics-preserving CAD rewrites to efficiently search for smaller equivalent programs. Szalinski relies on inverse transformations, a novel way for solvers to speculatively add equivalences to an E-graph. We qualitatively evaluate Szalinski in case studies, show how it composes with an existing mesh decompiler, and demonstrate that Szalinski can shrink large models in seconds.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {31–44},
numpages = {14},
keywords = {Program Synthesis, Computer-Aided Design, Decompilation, Equality Saturation},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395645,
author = {Nandi, Chandrakana and Willsey, Max and Anderson, Adam and Wilcox, James R. and Darulova, Eva and Grossman, Dan and Tatlock, Zachary},
title = {Szalinski: A Tool for Synthesizing Structured CAD Models with Equality Saturation and Inverse Transformations},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395645},
abstract = {
    <h2 id="paper-pldi20main-p471-p">Paper pldi20main-p471-p</h2>
<h3 id="goals-of-the-artifact">Goals of the artifact</h3>
<p>In our paper, we evaluated the following about Szalinski (<code>Section 7</code>):</p>
<ol style="list-style-type: decimal">
<li><p>End-to-End: we ran Szalinski on the flat CSG outputs of a mesh decompiler (Reincarnate). The results are in <code>Table 2</code>.</p></li>
<li><p>Scalability: we evaluated Szalinski on a large dataset of models scraped from a popular online repository (Thingiverse). The results are in <code>Figure    14</code> (first three box plots).</p></li>
<li><p>Sensitivity: we evaluated the usefulness of Szalinski's two main features: CAD rewrites and Inverse Transformations. The results are in <code>Figure 14</code> (last two box plots).</p></li>
</ol>
<p>In support of these results, this artifact reproduces <code>Table 2</code> and <code>Figure 14</code>. In addition, it also generates the output programs in <code>Figure 15</code> that are used in the case studies.</p>
<p>This document contains the following parts:</p>
<ul>
<li><p>System requirements</p></li>
<li>How to run Szalinski</li>
<li>Reproducing Table 2 (takes &lt; 5 minutes)</li>
<li>Reproducing Figure 14 (takes approx. 1.5 hour)</li>
<li>Reproducing Figure 15 (takes &lt; 5 minutes)</li>
<li><p>Validation</p></li>
<li>Reusability</li>
<li>How to set up Szalinski on a different machine (this is also how we set up the VM we submitted for PLDI 2020 AEC)</li>
<li><p>Description of the code and how to modify it</p></li>
<li><p>Notes and remarks</p></li>
</ul>
<h3 id="system-requirements">System requirements</h3>
<ul>
<li>Specs of the machine where we ran the VM: Intel i7-8700K (12 threads @ 4.7GHz), 32GiB RAM</li>
</ul>
<h3 id="running-the-tools">Running the tools</h3>
<h4 id="reproducing-table-2">Reproducing Table 2</h4>
<p>Navigate to the directory that contains the <code>Makefile</code> and type <code>make out/aec-table2/table2.csv</code>. This should take about 3 minutes. This will reproduce <code>Table 2</code> from the paper. To view the content of the table, type <code>cat out/aec-table2/table2.csv | column -t -s,</code> and compare the numbers with <code>Table 2</code> in the paper.</p>
<p><strong>NOTE:</strong> - We suspect that different versions of OpenSCAD use different triangulation algorithms for compiling to mesh which can affect the numbers in the <code>#Tri</code> column.</p>
<h4 id="reproducing-figure-14">Reproducing Figure 14</h4>
<p>We have included in the repo the 2,127 examples from Thingiverse that we evaluated on in the paper. The remainder of the 12,939 scraped from Thingiverse were either malformed or used features not supported by Szalinski. The script (<code>scripts/scrape-thingiverse.sh</code>) scrapes models under the <code>customizable</code> category, from the first 500 pages.</p>
<p><em>NOTE:</em> Running this part takes about an hour. We recommend first reproducing <code>Figure 15</code> and <code>Table 2</code>, both of which take much less time.</p>
<p>Navigate to the directory that contains the <code>Makefile</code> and type <code>make out/fig14.pdf</code>. Open the generated file in a pdf viewer and compare with <code>Figure 14</code> in the paper.</p>
<h4 id="reproducing-figure-15-programs">Reproducing Figure 15 programs</h4>
<p>Navigate to the directory that contains the <code>Makefile</code> and type <code>make aec-fig15</code>. This should take less than a minute. Then look in the <code>out/aec-fig15</code> directory. The optimized programs generated by Szalinski are in the files with extensions <code>normal.csexp.opt</code>. There should be 6 such files. Open them and compare the content with the programs listed in <code>Figure 15</code> of the paper.</p>
<p><strong>NOTE:</strong> - The programs in the paper are sugared and represented more compactly for space. - <code>MapI</code> found in the artifact results corresponds to <code>Tabulate</code> in the paper. - When comparing the results generated by the artifact to the programs in <code>Figure 15</code> of the paper, it is most important to check that the high-level structure in terms of <code>Fold</code> and <code>MapI</code> synthesized by the artifact matches that reported in the paper.</p>
<h4 id="validation">Validation</h4>
<p><code>Section 6</code> of our paper describes Szalinski's validation process. We use OpenSCAD to compile CSG programs to meshes and use CGAL to compute the Hausdorff distance between two meshes.</p>
<p>To validate the programs in <code>Figure 15</code>, run <code>make out/aec-fig15/hausdorff</code>. This should terminate in less than 3 minutes. It should show you the names of the 6 examples in <code>Figure 15</code> and the corresponding Hausdorff distances which are close to zero.</p>
<p>We have also validated all our other results reported in the paper. However, our experience indicates that OpenSCAD's compilation step is often very slow. Therefore, the other commands mentioned in the instruction for reproducing the results do not perform validation by default.</p>
<p>You can validate any example from our evaluation by typing: <code>make out/dir_name/example_name.normal.diff</code>, where <code>dir_name</code> can be <code>aec-table2</code>, <code>aec_fig15</code> or <code>thingiverse</code>, and <code>example_name</code> is the name of whatever example you choose. Then open the generated <code>.diff</code> file and check that the Hausdorff distance is within some epsilon of 0.</p>
<p><strong>NOTE:</strong> For many example, CGAL crashes or is slow at computing the Hausdorff distance. For these, we recommend a manual validation if you are interested. In order to validate an example, type the following: <code>make out/dir_name/example_name.diff.scad</code>. You can open the generated <code>.scad</code> file in OpenSCAD (already installed in the VM). In OpenSCAD, click on the <code>Render</code> button (the second button from the right) in the toolbar. You should either see nothing rendered or some residual thin walls that are artifacts of rounding error prevalent in OpenSCAD.</p>
<h3 id="reusability">Reusability</h3>
<p>Here we provide instructions on how to start using Szalinski including installation and changing the rules and features of the Caddy language.</p>
<h4 id="setup-instructions">Setup instructions</h4>
<p>Following are the steps for setting up Szalinski from scratch on a machine that runs Ubuntu 19.10.</p>
<ul>
<li><p>Install rust. Type <code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh</code> in the terminal and follow the subsequent instructions. The version we used is <code>1.41.0</code>. See<code>https://www.rust-lang.org/tools/install</code> for more information.</p></li>
<li><p>Make sure you configure your current shell by typing: <code>source $HOME/.cargo/env</code> (the Rust installation will prompt you to do this).</p></li>
<li><p>Install make by typing: <code>sudo apt-get install make</code></p></li>
<li><p>Install g++ by typing: <code>sudo apt-get install g++</code></p></li>
<li><p>Install jq by typing: <code>sudo apt-get install jq</code></p></li>
<li><p>Install <a href="https://www.cgal.org/download/linux.html">CGAL</a> by typing <code>sudo apt-get install libcgal-dev</code></p></li>
<li><p>Install <a href="https://www.openscad.org/">OpenSCAD</a> by typing <code>sudo apt-get install openscad</code></p></li>
<li><p>Install git by typing <code>sudo apt install git</code></p></li>
<li><p>Install pip by typing <code>sudo apt install python3-pip</code> and then install <code>numpy</code> by typing <code>pip3 install numpy</code> and <code>matplotlib</code> by typing <code>pip3 install matplotlib</code></p></li>
<li><p>We have made a <a href="https://github.com/uwplse/szalinski/tree/pldi2020-aec">github release</a> for the PLDI AEC from where you can get the source.</p></li>
<li><p>Navigate to the project directory where the <code>Makefile</code> is and run the tool as described above.</p></li>
</ul>
<h4 id="changing-caddy-and-modifying-the-rules">Changing Caddy and modifying the rules</h4>
<ul>
<li><p>The Caddy language is defined in <code>cad.rs</code> in the <code>src</code> directory. A simple feature you can add is support for a new primitive or new transformations. You can also change the costs of various language constructs. The definition of the <code>cost</code> function starts at line <code>267</code>.</p></li>
<li><p>As we described in the paper, to verify the correctness of Szalinski, we evaluate Caddy programs to flat Core Caddy and pretty print to CSG. This code is in <code>eval.rs</code>.</p></li>
<li><p><code>solve.rs</code> and <code>permute.rs</code> contains code that solves for first and second degree polynomials in Cartesian and Spherical coordinates, and performs partitioning and permutations of lists.</p></li>
<li><p>The rewrites rules are in <code>rules.rs</code>. Syntactic rewrites are written using the <code>rw!</code> macro. Each rewrite has a name, a left hand side, and a right hand side. You can add / remove rules to see how that affects the final Caddy output of Szalinski. For example, if you comment out the rules for inverse transformations, they will not be propagated and eliminated, and therefore the quality of Szalinski's output will not be as good.</p></li>
</ul>
<h3 id="notes-and-remarks">Notes and remarks</h3>
<p>Szalinski is implemented in <a href="https://www.rust-lang.org/">Rust</a>. As mentioned in <code>Section 6</code> of the paper, it uses <a href="https://www.openscad.org/">OpenSCAD</a> to compile CSG programs to triangular meshes, and <a href="https://www.cgal.org/">CGAL</a> to compute the Hausdorff distance between two meshes.</p>

},
keywords = {CAD, Compiler Optimization, Computational Geometry, Synthesis}
}

@inproceedings{10.1145/3385412.3385981,
author = {Flatt, Matthew and Dybvig, R. Kent},
title = {Compiler and Runtime Support for Continuation Marks},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385981},
doi = {10.1145/3385412.3385981},
abstract = {Continuation marks enable dynamic binding and context inspection in a language with proper handling of tail calls and first-class, multi-prompt, delimited continuations. The simplest and most direct use of continuation marks is to implement dynamically scoped variables, such as the current output stream or the current exception handler. Other uses include stack inspection for debugging or security checks, serialization of an in-progress computation, and run-time elision of redundant checks. By exposing continuation marks to users of a programming language, more kinds of language extensions can be implemented as libraries without further changes to the compiler. At the same time, the compiler and runtime system must provide an efficient implementation of continuation marks to ensure that library-implemented language extensions are as effective as changing the compiler. Our implementation of continuation marks for Chez Scheme (in support of Racket) makes dynamic binding and lookup constant-time and fast, preserves the performance of Chez Scheme's first-class continuations, and imposes negligible overhead on program fragments that do not use first-class continuations or marks.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {45–58},
numpages = {14},
keywords = {context inspection, Dynamic binding},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395632,
author = {Flatt, Matthew and Dybvig, R. Kent},
title = {Replication Package for Article: Compiler and Runtime Support for Continuation Marks},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395632},
abstract = {
    <p>The artifact contains the full source code for Racket and Chez Scheme and variants as described in the paper, and it provides benchmarks that can be used to support the following claims:</p>
<ul>
<li><p>The implementation of continuation marks is compatible with a high-performance implementation of first-class, delimited continuations.</p></li>
<li><p>Compiler and runtime support for continuation marks can improve the performance of applications.</p></li>
<li><p>Specific optimizations described in the paper improve the performance of continuation marks and applications that use them.</p></li>
</ul>

},
keywords = {context inspection, Dynamic binding}
}

@inproceedings{10.1145/3385412.3385991,
author = {Gen\c{c}, Kaan and Bond, Michael D. and Xu, Guoqing Harry},
title = {Crafty: Efficient, HTM-Compatible Persistent Transactions},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385991},
doi = {10.1145/3385412.3385991},
abstract = {Byte-addressable persistent memory, such as Intel/Micron 3D XPoint, is an emerging technology that bridges the gap between volatile memory and persistent storage. Data in persistent memory survives crashes and restarts; however, it is challenging to ensure that this data is consistent after failures. Existing approaches incur significant performance costs to ensure crash consistency.  This paper introduces Crafty, a new approach for ensuring consistency and atomicity on persistent memory operations using commodity hardware with existing hardware transactional memory (HTM) capabilities, while incurring low overhead. Crafty employs a novel technique called nondestructive undo logging that leverages commodity HTM to control persist ordering. Our evaluation shows that Crafty outperforms state-of-the-art prior work under low contention, and performs competitively under high contention.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {59–74},
numpages = {16},
keywords = {persistent transactions, transactional memory},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3752546,
author = {Gen\c{c}, Kaan and Bond, Michael D. and Xu, Guoqing Harry},
title = {Artifact for Article: Crafty: Efficient, HTM-Compatible Persistent Transactions},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3752546},
abstract = {
    <p>The artifact contains the source code of our implementation, including the microbenchmarks we evaluated and the code required to generate the graphs seen in the paper. It also contains a Docker image that includes all requirements for building and running the code. Using the Docker image is optional but highly recommended. A README file detailing how to reproduce our results is included.</p>

},
keywords = {Crafty, non-volatile memory, persistent memory, persistent transactions}
}

@inproceedings{10.1145/3385412.3385994,
author = {Farvardin, Kavon and Reppy, John},
title = {From Folklore to Fact: Comparing Implementations of Stacks and Continuations},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385994},
doi = {10.1145/3385412.3385994},
abstract = {The efficient implementation of function calls and non-local control transfers is a critical part of modern language implementations and is important in the implementation of everything from recursion, higher-order functions, concurrency and coroutines, to task-based parallelism. In a compiler, these features can be supported by a variety of mechanisms, including call stacks, segmented stacks, and heap-allocated continuation closures.  An implementor of a high-level language with advanced control features might ask the question ``what is the best choice for my implementation?'' Unfortunately, the current literature does not provide much guidance, since previous studies suffer from various flaws in methodology and are outdated for modern hardware. In the absence of recent, well-normalized measurements and a holistic overview of their implementation specifics, the path of least resistance when choosing a strategy is to trust folklore, but the folklore is also suspect.  This paper attempts to remedy this situation by providing an ``apples-to-apples'' comparison of six different approaches to implementing call stacks and continuations. This comparison uses the same source language, compiler pipeline, LLVM-backend, and runtime system, with the only differences being those required by the differences in implementation strategy. We compare the implementation challenges of the different approaches, their sequential performance, and their suitability to support advanced control mechanisms, including supporting heavily threaded code. In addition to the comparison of implementation strategies, the paper's contributions also include a number of useful implementation techniques that we discovered along the way.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {75–90},
numpages = {16},
keywords = {Call stacks, Concurrency, Compilers, Functional Programming, Continuations},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395636,
author = {Farvardin, Kavon and Reppy, John},
title = {Replication Package for Article: From Folklore to Fact: Comparing Implementations of Stacks and Continuations},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395636},
abstract = {
    <p>This artifact contains all materials needed to reproduce or extend the results of this work. It includes the raw data and plots corresponding to the results presented in the paper, the full source code of Manticore &amp; LLVM (plus the benchmark programs and plotting scripts), a Docker image that contains the entire system pre-built, and an extensive README that describes how to build and run our benchmark suite to replicate the experiments in the paper.</p>

},
keywords = {Call stacks, Compilers, Concurrency, Continuations, Functional Programming}
}

@inproceedings{10.1145/3385412.3385997,
author = {Allamanis, Miltiadis and Barr, Earl T. and Ducousso, Soline and Gao, Zheng},
title = {Typilus: Neural Type Hints},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385997},
doi = {10.1145/3385412.3385997},
abstract = {Type inference over partial contexts in dynamically typed languages is challenging. In this work, we present a graph neural network model that predicts types by probabilistically reasoning over a program’s structure, names, and patterns. The network uses deep similarity learning to learn a TypeSpace — a continuous relaxation of the discrete space of types — and how to embed the type properties of a symbol (i.e. identifier) into it. Importantly, our model can employ one-shot learning to predict an open vocabulary of types, including rare and user-defined ones. We realise our approach in Typilus for Python that combines the TypeSpace with an optional type checker. We show that Typilus accurately predicts types. Typilus confidently predicts types for 70% of all annotatable symbols; when it predicts a type, that type optionally type checks 95% of the time. Typilus can also find incorrect type annotations; two important and popular open source libraries, fairseq and allennlp, accepted our pull requests that fixed the annotation errors Typilus discovered.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {91–105},
numpages = {15},
keywords = {type inference, graph neural networks, structured learning, meta-learning, deep learning},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3385986,
author = {Yao, Jianan and Ryan, Gabriel and Wong, Justin and Jana, Suman and Gu, Ronghui},
title = {Learning Nonlinear Loop Invariants with Gated Continuous Logic Networks},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385986},
doi = {10.1145/3385412.3385986},
abstract = {Verifying real-world programs often requires inferring loop invariants with nonlinear constraints. This is especially true in programs that perform many numerical operations, such as control systems for avionics or industrial plants. Recently, data-driven methods for loop invariant inference have shown promise, especially on linear loop invariants. However, applying data-driven inference to nonlinear loop invariants is challenging due to the large numbers of and large magnitudes of high-order terms, the potential for overfitting on a small number of samples, and the large space of possible nonlinear inequality bounds. In this paper, we introduce a new neural architecture for general SMT learning, the Gated Continuous Logic Network (G-CLN), and apply it to nonlinear loop invariant learning. G-CLNs extend the Continuous Logic Network (CLN) architecture with gating units and dropout, which allow the model to robustly learn general invariants over large numbers of terms. To address overfitting that arises from finite program sampling, we introduce fractional sampling—a sound relaxation of loop semantics to continuous functions that facilitates unbounded sampling on the real domain. We additionally design a new CLN activation function, the Piecewise Biased Quadratic Unit (PBQU), for naturally learning tight inequality bounds. We incorporate these methods into a nonlinear loop invariant inference system that can learn general nonlinear loop invariants. We evaluate our system on a benchmark of nonlinear loop invariants and show it solves 26 out of 27 problems, 3 more than prior work, with an average runtime of 53.3 seconds. We further demonstrate the generic learning ability of G-CLNs by solving all 124 problems in the linear Code2Inv benchmark. We also perform a quantitative stability evaluation and show G-CLNs have a convergence rate of 97.5% on quadratic problems, a 39.2% improvement over CLN models.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {106–120},
numpages = {15},
keywords = {Continuous Logic Networks, Loop Invariant Inference, Program Verification},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3385999,
author = {Wang, Ke and Su, Zhendong},
title = {Blended, Precise Semantic Program Embeddings},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385999},
doi = {10.1145/3385412.3385999},
abstract = {Learning neural program embeddings is key to utilizing deep neural networks in program languages research --- precise and efficient program representations enable the application of deep models to a wide range of program analysis tasks. Existing approaches predominately learn to embed programs from their source code, and, as a result, they do not capture deep, precise program semantics. On the other hand, models learned from runtime information critically depend on the quality of program executions, thus leading to trained models with highly variant quality. This paper tackles these inherent weaknesses of prior approaches by introducing a new deep neural network, Liger, which learns program representations from a mixture of symbolic and concrete execution traces. We have evaluated Liger on two tasks: method name prediction and semantics classification. Results show that Liger is significantly more accurate than the state-of-the-art static model code2seq in predicting method names, and requires on average around 10x fewer executions covering nearly 4x fewer paths than the state-of-the-art dynamic model DYPRO in both tasks. Liger offers a new, interesting design point in the space of neural program embeddings and opens up this new direction for exploration.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {121–134},
numpages = {14},
keywords = {Semantic Program Embedding, Attention Network, Static and Dynamic Program Features},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3385968,
author = {Brown, Fraser and Renner, John and N\"{o}tzli, Andres and Lerner, Sorin and Shacham, Hovav and Stefan, Deian},
title = {Towards a Verified Range Analysis for JavaScript JITs},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385968},
doi = {10.1145/3385412.3385968},
abstract = {We present VeRA, a system for verifying the range analysis pass in browser just-in-time (JIT) compilers. Browser developers write range analysis routines in a subset of C++, and verification developers write infrastructure to verify custom analysis properties. Then, VeRA automatically verifies the range analysis routines, which browser developers can integrate directly into the JIT. We use VeRA to translate and verify Firefox range analysis routines, and it detects a new, confirmed bug that has existed in the browser for six years. },
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {135–150},
numpages = {16},
keywords = {JIT verification, JavaScript, range analysis},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395642,
author = {Brown, Fraser and Renner, John and N\"{o}tzli, Andres and Lerner, Sorin and Shacham, Hovav and Stefan, Deian},
title = {Towards a Verified Range Analysis for JavaScript JITs: Artifact},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395642},
abstract = {
    <p>Artifact for "Towards a Verified Range Analysis for JavaScript JITS"</p>

},
keywords = {verification}
}

@inproceedings{10.1145/3385412.3385972,
author = {Duck, Gregory J. and Gao, Xiang and Roychoudhury, Abhik},
title = {Binary Rewriting without Control Flow Recovery},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385972},
doi = {10.1145/3385412.3385972},
abstract = {Static binary rewriting has many important applications in software security and systems, such as hardening, repair, patching, instrumentation, and debugging. While many different static binary rewriting tools have been proposed, most rely on recovering control flow information from the input binary. The recovery step is necessary since the rewriting process may move instructions, meaning that the set of jump targets in the rewritten binary needs to be adjusted accordingly. Since the static recovery of control flow information is a hard problem in general, most tools rely on a set of simplifying heuristics or assumptions, such as specific compilers, specific source languages, or binary file meta information. However, the reliance on assumptions or heuristics tends to scale poorly in practice, and most state-of-the-art static binary rewriting tools cannot handle very large/complex programs such as web browsers.  In this paper we present E9Patch, a tool that can statically rewrite x86_64 binaries without any knowledge of control flow information. To do so, E9Patch develops a suite of binary rewriting methodologies---such as instruction punning, padding, and eviction---that can insert jumps to trampolines without the need to move other instructions. Since this preserves the set of jump targets, the need for control flow recovery and related heuristics is eliminated. As such, E9Patch is robust by design, and can scale to very large (&gt;100MB) stripped binaries including the Google Chrome and FireFox web browsers. We also evaluate the effectiveness of E9Patch against realistic applications such as binary instrumentation, hardening and repair.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {151–163},
numpages = {13},
keywords = {instruction punning, instruction eviction, binary repair, static binary rewriting, memory management, binary patching, binary instrumentation},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3386017,
author = {Porter, Chris and Mururu, Girish and Barua, Prithayan and Pande, Santosh},
title = {BlankIt Library Debloating: Getting What You Want Instead of Cutting What You Don’t},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386017},
doi = {10.1145/3385412.3386017},
abstract = {Modern software systems make extensive use of libraries derived from C and C++. Because of the lack of memory safety in these languages, however, the libraries may suffer from vulnerabilities, which can expose the applications to potential attacks. For example, a very large number of return-oriented programming gadgets exist in glibc that allow stitching together semantically valid but malicious Turing-complete and -incomplete programs. While CVEs get discovered and often patched and remedied, such gadgets serve as building blocks of future undiscovered attacks, opening an ever-growing set of possibilities for generating malicious programs. Thus, significant reduction in the quantity and expressiveness (utility) of such gadgets for libraries is an important problem. In this work, we propose a new approach for handling an application’s library functions that focuses on the principle of “getting only what you want.” This is a significant departure from the current approaches that focus on “cutting what is unwanted.” Our approach focuses on activating/deactivating library functions on demand in order to reduce the dynamically linked code surface, so that the possibilities of constructing malicious programs diminishes substantially. The key idea is to load only the set of library functions that will be used at each library call site within the application at runtime. This approach of demand-driven loading relies on an input-aware oracle that predicts a near-exact set of library functions needed at a given call site during the execution. The predicted functions are loaded just in time and unloaded on return. We present a decision-tree based predictor, which acts as an oracle, and an optimized runtime system, which works directly with library binaries like GNU libc and libstdc++. We show that on average, the proposed scheme cuts the exposed code surface of libraries by 97.2%, reduces ROP gadgets present in linked libraries by 97.9%, achieves a prediction accuracy in most cases of at least 97%, and adds a runtime overhead of 18% on all libraries (16% for glibc, 2% for others) across all benchmarks of SPEC 2006. Further, we demonstrate BlankIt on two real-world applications, sshd and nginx, with a high amount of debloating and low overheads.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {164–180},
numpages = {17},
keywords = {program security, software debloating},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395649,
author = {Porter, Chris and Mururu, Girish and Barua, Prithayan and Pande, Santosh},
title = {Replication Package for Article: BlankIt Library Debloating: Getting What You Want Instead of Cutting What You Don’t},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395649},
abstract = {
    <p>These are the docker containers to reproduce the main results of the paper.</p>

},
keywords = {software debloating}
}

@inproceedings{10.1145/3385412.3386029,
author = {Krishna, Siddharth and Patel, Nisarg and Shasha, Dennis and Wies, Thomas},
title = {Verifying Concurrent Search Structure Templates},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386029},
doi = {10.1145/3385412.3386029},
abstract = {Concurrent separation logics have had great success reasoning about concurrent data structures. This success stems from their application of modularity on multiple levels, leading to proofs that are decomposed according to program structure, program state, and individual threads. Despite these advances, it remains difficult to achieve proof reuse across different data structure implementations. For the large class of search structures, we demonstrate how one can achieve further proof modularity by decoupling the proof of thread safety from the proof of structural integrity. We base our work on the template algorithms of Shasha and Goodman that dictate how threads interact but abstract from the concrete layout of nodes in memory. Building on the recently proposed flow framework of compositional abstractions and the separation logic Iris, we show how to prove correctness of template algorithms, and how to instantiate them to obtain multiple verified implementations. We demonstrate our approach by mechanizing the proofs of three concurrent search structure templates, based on link, give-up, and lock-coupling synchronization, and deriving verified implementations based on B-trees, hash tables, and linked lists. These case studies include algorithms used in real-world file systems and databases, which have been beyond the capability of prior automated or mechanized verification techniques. In addition, our approach reduces proof complexity and is able to achieve significant proof reuse.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {181–196},
numpages = {16},
keywords = {separation logic, template-based verification, concurrent data structures, flow framework},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395655,
author = {Krishna, Siddharth and Patel, Nisarg and Shasha, Dennis and Wies, Thomas},
title = {Mechanized Proofs Accompanying Paper: Verifying Concurrent Search Structure Templates},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395655},
abstract = {
    <p>Our artifact consists of two parts: the proofs of template algorithms, to be verified by Iris/Coq, and the proofs of implementations, to be verified by GRASShopper.</p>

},
keywords = {concurrent search structures, Coq, functional correctness, Grasshopper, Iris, mechanized proofs, memory safety}
}

@inproceedings{10.1145/3385412.3385971,
author = {Lorch, Jacob R. and Chen, Yixuan and Kapritsos, Manos and Parno, Bryan and Qadeer, Shaz and Sharma, Upamanyu and Wilcox, James R. and Zhao, Xueyuan},
title = {Armada: Low-Effort Verification of High-Performance Concurrent Programs},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385971},
doi = {10.1145/3385412.3385971},
abstract = {Safely writing high-performance concurrent programs is notoriously difficult. To aid developers, we introduce Armada, a language and tool designed to formally verify such programs with relatively little effort. Via a C-like language and a small-step, state-machine-based semantics, Armada gives developers the flexibility to choose arbitrary memory layout and synchronization primitives so they are never constrained in their pursuit of performance. To reduce developer effort, Armada leverages SMT-powered automation and a library of powerful reasoning techniques, including rely-guarantee, TSO elimination, reduction, and alias analysis. All these techniques are proven sound, and Armada can be soundly extended with additional strategies over time. Using Armada, we verify four concurrent case studies and show that we can achieve performance equivalent to that of unverified code.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {197–210},
numpages = {14},
keywords = {weak memory models, refinement, x86-TSO},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395653,
author = {Lorch, Jacob R. and Chen, Yixuan and Kapritsos, Manos and Parno, Bryan and Qadeer, Shaz and Sharma, Upamanyu and Wilcox, James R. and Zhao, Xueyuan},
title = {Replication Package for Article "Armada: Low-Effort Verification of High-Performance Concurrent Programs"},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395653},
abstract = {
    <p>This artifact contains the materials necessary for replication of the paper "Armada: Low-Effort Verification of High-Performance Concurrent Programs". It contains a Docker image containing the source code, executables, external dependencies, and examples. It also contains documentation of the artifact and a description of the Armada language.</p>

},
keywords = {refinement, weak memory models, x86-TSO}
}

@inproceedings{10.1145/3385412.3385966,
author = {Lahav, Ori and Boker, Udi},
title = {Decidable Verification under a Causally Consistent Shared Memory},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385966},
doi = {10.1145/3385412.3385966},
abstract = {Causal consistency is one of the most fundamental and widely used consistency models weaker than sequential consistency. In this paper, we study the verification of safety properties for finite-state concurrent programs running under a causally consistent shared memory model. We establish the decidability of this problem for a standard model of causal consistency (called also "Causal Convergence" and "Strong-Release-Acquire"). Our proof proceeds by developing an alternative operational semantics, based on the notion of a thread potential, that is equivalent to the existing declarative semantics and constitutes a well-structured transition system. In particular, our result allows for the verification of a large family of programs in the Release/Acquire fragment of C/C++11 (RA). Indeed, while verification under RA was recently shown to be undecidable for general programs, since RA coincides with the model we study here for write/write-race-free programs, the decidability of verification under RA for this widely used class of programs follows from our result. The novel operational semantics may also be of independent use in the investigation of weakly consistent shared memory models and their verification.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {211–226},
numpages = {16},
keywords = {release/acquire, shared-memory, weak memory models, verification, well-structured transition systems, causal consistency, decidability, concurrency},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395637,
author = {Lahav, Ori and Boker, Udi},
title = {Coq Proofs for: Decidable Verification under a Causally Consistent Shared Memory},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395637},
abstract = {
    <p>The artifact consists of the Coq development accompanying the paper. The main result is the equivalence between loSRA (called SRAL in the Coq development) and opSRA (called SRAG in the Coq development) that is given in Lemmas 5.15 and 5.16 in the paper. This final result is included in "SRAL_SRAG.v".</p>

},
keywords = {causal consistency, concurrency, Coq, decidability, release/acquire, shared-memory, verification, weak memory models, well-structured transition systems}
}

@inproceedings{10.1145/3385412.3385980,
author = {Kragl, Bernhard and Enea, Constantin and Henzinger, Thomas A. and Mutluergil, Suha Orhun and Qadeer, Shaz},
title = {Inductive Sequentialization of Asynchronous Programs},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385980},
doi = {10.1145/3385412.3385980},
abstract = {Asynchronous programs are notoriously difficult to reason about because they spawn computation tasks which take effect asynchronously in a nondeterministic way. Devising inductive invariants for such programs requires understanding and stating complex relationships between an unbounded number of computation tasks in arbitrarily long executions. In this paper, we introduce inductive sequentialization, a new proof rule that sidesteps this complexity via a sequential reduction, a sequential program that captures every behavior of the original program up to reordering of coarse-grained commutative actions. A sequential reduction of a concurrent program is easy to reason about since it corresponds to a simple execution of the program in an idealized synchronous environment, where processes act in a fixed order and at the same speed. We have implemented and integrated our proof rule in the CIVL verifier, allowing us to provably derive fine-grained implementations of asynchronous programs. We have successfully applied our proof rule to a diverse set of message-passing protocols, including leader election protocols, two-phase commit, and Paxos.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {227–242},
numpages = {16},
keywords = {concurrency, invariants, induction, asynchrony, reduction, layers, verification, movers, refinement, abstraction},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3754772,
author = {Kragl, Bernhard and Enea, Constantin and Henzinger, Thomas A. and Mutluergil, Suha Orhun and Qadeer, Shaz},
title = {Inductive Sequentialization of Asynchronous Programs (Evaluated Artifact)},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3754772},
abstract = {
    <p>Inductive sequentialization is implemented as an extension of the CIVL verifier. This implementation and all examples listed in Table 1 of the paper are part of the open-source project Boogie. This artifact is for long-term archiving purposes and contains a snapshot of Boogie version 2.6.4. Since the project is under active development, we recommend to obtain the most recent version from https://github.com/boogie-org/boogie.</p>
<p>For further information and instructions, see the included README.md file.</p>

},
keywords = {abstraction, asynchrony, concurrency, induction, invariants, layers, movers, reduction, refinement, verification}
}

@inproceedings{10.1145/3385412.3385965,
author = {Bourgeat, Thomas and Pit-Claudel, Cl\'{e}ment and Chlipala, Adam and Arvind},
title = {The Essence of Bluespec: A Core Language for Rule-Based Hardware Design},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385965},
doi = {10.1145/3385412.3385965},
abstract = {The Bluespec hardware-description language presents a significantly higher-level view than hardware engineers are used to, exposing a simpler concurrency model that promotes formal proof, without compromising on performance of compiled circuits. Unfortunately, the cost model of Bluespec has been unclear, with performance details depending on a mix of user hints and opaque static analysis of potential concurrency conflicts within a design. In this paper we present Koika, a derivative of Bluespec that preserves its desirable properties and yet gives direct control over the scheduling decisions that determine performance. Koika has a novel and deterministic operational semantics that uses dynamic analysis to avoid concurrency anomalies. Our implementation includes Coq definitions of syntax, semantics, key metatheorems, and a verified compiler to circuits. We argue that most of the extra circuitry required for dynamic analysis can be eliminated by compile-time BSV-style static analysis.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {243–257},
numpages = {15},
keywords = {Compiler Correctness, Hardware Description Language, Semantics},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3386024,
author = {Schuiki, Fabian and Kurth, Andreas and Grosser, Tobias and Benini, Luca},
title = {LLHD: A Multi-Level Intermediate Representation for Hardware Description Languages},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386024},
doi = {10.1145/3385412.3386024},
abstract = {Modern Hardware Description Languages (HDLs) such as SystemVerilog or VHDL are, due to their sheer complexity, insufficient to transport designs through modern circuit design flows. Instead, each design automation tool lowers HDLs to its own Intermediate Representation (IR). These tools are monolithic and mostly proprietary, disagree in their implementation of HDLs, and while many redundant IRs exists, no IR today can be used through the entire circuit design flow. To solve this problem, we propose the LLHD multi-level IR. LLHD is designed as simple, unambiguous reference description of a digital circuit, yet fully captures existing HDLs. We show this with our reference compiler on designs as complex as full CPU cores. LLHD comes with lowering passes to a hardware-near structural IR, which readily integrates with existing tools. LLHD establishes the basis for innovation in HDLs and tools without redundant compilers or disjoint IRs. For instance, we implement an LLHD simulator that runs up to 2.4\texttimes{} faster than commercial simulators but produces equivalent, cycle-accurate results. An initial vertically-integrated research prototype is capable of representing all levels of the IR, implements lowering from the behavioural to the structural IR, and covers a sufficient subset of SystemVerilog to support a full CPU design.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {258–271},
numpages = {14},
keywords = {hardware description languages, intermediate representations, transformation passes},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395654,
author = {Schuiki, Fabian and Kurth, Andreas and Grosser, Tobias and Benini, Luca},
title = {Replication Package for Paper: LLHD: A Multi-Level Intermediate Representation for Hardware Description Languages},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395654},
abstract = {
    <p>A Docker image with the necessary toolchains pre-installed and source codes pre-loaded to reproduce the main results of the LLHD paper, and to serve as a starting point for individual exploration.</p>

},
keywords = {domain-specific languages, hardware, language design, language implementation, new programming models or languages, parallelism}
}

@inproceedings{10.1145/3385412.3386011,
author = {Zhu, Shaopeng and Hung, Shih-Han and Chakrabarti, Shouvanik and Wu, Xiaodi},
title = {On the Principles of Differentiable Quantum Programming Languages},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386011},
doi = {10.1145/3385412.3386011},
abstract = {Variational Quantum Circuits (VQCs), or the so-called quantum neural-networks, are predicted to be one of the most important near-term quantum applications, not only because of their similar promises as classical neural-networks, but also because of their feasibility on near-term noisy intermediate-size quantum (NISQ) machines. The need for gradient information in the training procedure of VQC applications has stimulated the development of auto-differentiation techniques for quantum circuits. We propose the first formalization of this technique, not only in the context of quantum circuits but also for imperative quantum programs (e.g., with controls), inspired by the success of differentiable programming languages in classical machine learning. In particular, we overcome a few unique difficulties caused by exotic quantum features (such as quantum no-cloning) and provide a rigorous formulation of differentiation applied to bounded-loop imperative quantum programs, its code-transformation rules, as well as a sound logic to reason about their correctness. Moreover, we have implemented our code transformation in OCaml and demonstrated the resource-efficiency of our scheme both analytically and empirically. We also conduct a case study of training a VQC instance with controls, which shows the advantage of our scheme over existing auto-differentiation for quantum circuits without controls.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {272–285},
numpages = {14},
keywords = {quantum machine learning, differentiable programming languages, quantum programming languages},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395644,
author = {Zhu, Shaopeng and Hung, Shih-Han and Chakrabarti, Shouvanik and Wu, Xiaodi},
title = {Replication Package for: On the Principles of Differentiable Quantum Programming Languages},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395644},
abstract = {
    <p>The artifact includes two parts: (1) A parser and compiler that implements the rules for autodifferentiaton; (2) A simulation that demonstrates the advantage of differentiating quantum programs with control flow. Classical simulation of quantum programs is used in this part for evaluation.</p>

},
keywords = {autodifferentiation, differentiable programming, quantum computing, quantum machine learning, quantum programming languages}
}

@inproceedings{10.1145/3385412.3386007,
author = {Bichsel, Benjamin and Baader, Maximilian and Gehr, Timon and Vechev, Martin},
title = {Silq: A High-Level Quantum Language with Safe Uncomputation and Intuitive Semantics},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386007},
doi = {10.1145/3385412.3386007},
abstract = {Existing quantum languages force the programmer to work at a low level of abstraction leading to unintuitive and cluttered code. A fundamental reason is that dropping temporary values from the program state requires explicitly applying quantum operations that safely uncompute these values.  We present Silq, the first quantum language that addresses this challenge by supporting safe, automatic uncomputation. This enables an intuitive semantics that implicitly drops temporary values, as in classical computation. To ensure physicality of Silq's semantics, its type system leverages novel annotations to reject unphysical programs.  Our experimental evaluation demonstrates that Silq programs are not only easier to read and write, but also significantly shorter than equivalent programs in other quantum languages (on average -46% for Q#, -38% for Quipper), while using only half the number of quantum primitives.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {286–300},
numpages = {15},
keywords = {Uncomputation, Semantics, Quantum Language},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3764961,
author = {Bichsel, Benjamin and Baader, Maximilian and Gehr, Timon and Vechev, Martin},
title = {Silq-Artifact},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3764961},
abstract = {
    <p>Artifact for PLDI'20 paper "Silq: A High-level Quantum Programming Language with Safe Uncomputation and Intuitive Semantics".</p>

},
keywords = {Quantum Language, Semantics, Uncomputation}
}

@inproceedings{10.1145/3385412.3385977,
author = {Yang, Albert Mingkun and \"{O}sterlund, Erik and Wrigstad, Tobias},
title = {Improving Program Locality in the GC Using Hotness},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385977},
doi = {10.1145/3385412.3385977},
abstract = {The hierarchical memory system with increasingly small and increasingly fast memory closer to the CPU has for long been at the heart of hiding, or mitigating the performance gap between memories and processors. To utilise this hardware, programs must be written to exhibit good object locality. In languages like C/C++, programmers can carefully plan how objects should be laid out (albeit time consuming and error-prone); for managed languages, especially ones with moving garbage collectors, a manually created optimal layout may be destroyed in the process of object relocation. For managed languages that present an abstract view of memory, the solution lies in making the garbage collector aware of object locality, and strive to achieve and maintain good locality, even in the face of multi-phased programs that exhibit different behaviour across different phases. This paper presents a GC design that dynamically reorganises objects in the order mutators access them, and additionally strives to separate frequently and infrequently used objects in memory. This improves locality and the efficiency of hardware prefetching. Identifying frequently used objects is done at run-time, with small overhead. HCSGC also offers tunability, for shifting relocation work towards mutators, or for more or less aggressive object relocation. The ideas are evaluated in the context of the ZGC collector on OpenJDK and yields performance improvements of 5% (tradebeans), 9% (h2) and an impressive 25–45% (JGraphT), all with 95% confidence. For SPECjbb, results are inconclusive due to a fluctuating baseline.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {301–313},
numpages = {13},
keywords = {GC, data locality, cache optimisation, prefetching},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395659,
author = {Yang, Albert Mingkun and \"{O}sterlund, Erik and Wrigstad, Tobias},
title = {Replication Package for Article: Improving Program Locality in the GC Using Hotness},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395659},
abstract = {
    <p>It includes our implementation, benchmarks used, and scripts for running benchmarks and collecting/visualizing results.</p>

},
keywords = {garbage collection, load barrier, openjdk, program locality}
}

@inproceedings{10.1145/3385412.3385978,
author = {Kang, Jeehoon and Jung, Jaehwang},
title = {A Marriage of Pointer- and Epoch-Based Reclamation},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385978},
doi = {10.1145/3385412.3385978},
abstract = {All pointer-based nonblocking concurrent data structures should deal with the problem of safe memory reclamation: before reclaiming a memory block, a thread should ensure no other threads hold a local pointer to the block that may later be dereferenced. Various safe memory reclamation schemes have been proposed in the literature, but none of them satisfy the following desired properties at the same time: (i) robust: a non-cooperative thread does not prevent the other threads from reclaiming an unbounded number of blocks; (ii) fast: it does not incur significant time overhead; (iii) compact: it does not incur significant space overhead; (iv) self-contained: it neither relies on special hardware/OS supports nor intrusively affects execution environments; and (v) widely applicable: it supports many data structures. We introduce PEBR, which we believe is the first scheme that satisfies all the properties above. PEBR is inspired by Snowflake’s hybrid design of pointer- and epoch-based reclamation schemes (PBR and EBR, resp.) that is mostly robust, fast, and compact but neither self-contained nor widely applicable. To achieve self-containedness, we design algorithms using only the standard C/C++ concurrency features and process-wide memory fence. To achieve wide applicability, we characterize PEBR’s requirement for safe reclamation that is satisfied by a variety of data structures, including Harris’s and Harris-Herlihy-Shavit’s lists that are not supported by PBR. We experimentally evaluate whether PEBR is fast and robust using microbenchmarks, for which PEBR performs comparably to the state-of-the-art schemes.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {314–328},
numpages = {15},
keywords = {safe memory reclamation, garbage collection, pointer-based reclamation, hazard pointer, epoch-based reclamation},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395630,
author = {Kang, Jeehoon and Jung, Jaehwang},
title = {Implementation and Benchmark Suite for Article: A Marriage of Pointer- and Epoch-Based Reclamation},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395630},
abstract = {
    <h2 id="a-marriage-of-pointer--and-epoch-based-reclamation">A Marriage of Pointer- and Epoch-Based Reclamation</h2>
<p>This is the artifact for</p>
<p>Jeehoon Kang and Jaehwang Jung, A Marriage of Pointer- and Epoch-Based Reclamation, PLDI 2020.</p>
<p>The latest developments are on <a href="https://github.com/kaist-cp/pebr-benchmark" class="uri">https://github.com/kaist-cp/pebr-benchmark</a>.</p>
<h3 id="summary">Summary</h3>
<p>On Ubuntu 18.04,</p>
<pre><code>sudo apt install build-essential python3-pip
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
pip3 install --user pandas plotnine
python3 bench.py
python3 plot.py</code></pre>
<h3 id="dependencies">Dependencies</h3>
<ul>
<li>Linux &gt;= 4.14 for <a href="http://man7.org/linux/man-pages/man2/membarrier.2.html"><code>MEMBARRIER_CMD_PRIVATE_EXPEDITED</code> and <code>MEMBARRIER_CMD_REGISTER_PRIVATE_EXPEDITED</code></a>, used in the implementation of PEBR.
<ul>
<li><strong>IMPORTANT</strong>: Docker disables this feature by default. To enable, please use <code>--security-opt seccomp:unconfined</code> option when launching Docker.</li>
</ul></li>
<li><a href="https://rustup.rs/"><code>rustup</code></a> for building the implementation of NR, EBR, PEBR and data structures
<ul>
<li>Rust requires GCC for linking in Linux.</li>
</ul></li>
<li>Python &gt;= 3.6, pandas and plotnine for benchmark runner and plotting scripts</li>
</ul>
<h3 id="usage">Usage</h3>
<p>To build the benchmark,</p>
<pre><code>git submodule update --init --recursive # not needed if you got the archived source code
cargo build --release                   # remove --release for debug build</code></pre>
<p>To run a single test,</p>
<pre><code>./target/release/pebr-benchmark -d <data structure=""> -m <reclamation scheme=""> -t <threads></threads></reclamation></data></code></pre>
<p>where</p>
<ul>
<li>data structure: HList, HMList, HHSList, HashMap, NMTree, BonsaiTree</li>
<li>reclamation scheme: NR, EBR, PEBR</li>
</ul>
<p>For detailed usage information,</p>
<pre><code>./target/release/pebr-benchmark -h</code></pre>
<p>To run the entire benchmark,</p>
<pre><code>python3 bench.py</code></pre>
<p>This takes several hours and creates raw CSV data under <code>./results/</code>.</p>
<p>To generate plots,</p>
<pre><code>python3 plot.py</code></pre>
<p>This creates plots presented in the paper under <code>./results/</code>.</p>
<h3 id="debug">Debug</h3>
<p>We used <code>./sanitize.sh</code> to debug our implementation. This script runs the benchmark with <a href="https://github.com/japaric/rust-san">LLVM address sanitizer for Rust</a> and uses parameters that impose high stress on PEBR by triggering more frequent ejection.</p>
<p>Note that sanitizer may report memory leaks when used against <code>-m EBR</code>. This is because of a minor bug in original Crossbeam but it doesn't affect performance of our benchmark.</p>
<h3 id="project-structure">Project structure</h3>
<ul>
<li><p><code>./crossbeam-pebr</code> is the fork of <a href="https://github.com/crossbeam-rs/crossbeam">Crossbeam</a> that implements PEBR. The main implementation of PEBR lies under <code>./crossbeam-pebr/crossbeam-epoch</code>.</p></li>
<li><p><code>./crossbeam-ebr</code> is the original Crossbeam source code.</p></li>
<li><p><code>./src</code> contains the benchmark driver (<code>./src/main.rs</code>) and the implementation of data structures based on PEBR (<code>./src/pebr/</code>) and original Crossbeam (<code>./src/ebr/</code>).</p></li>
</ul>
<h3 id="results">Results</h3>
<p><code>./paper-results</code> contains the raw results and graphs used in the paper.</p>
<h3 id="note">Note</h3>
<ul>
<li><p>On Windows, the benchmark uses the default memory allocator instead of jemalloc since <a href="https://crates.io/crates/jemallocator">the Rust library for jemalloc</a> does not support Windows.</p></li>
<li><p>The benchmark run by <code>./sanitize.sh</code> will generate inaccurate memory usage report since it uses the default memory allocator instead of jemalloc. The memory tracker relies on jemalloc's functionalities which doesn't keep track of allocations by the default allocator.</p></li>
</ul>

},
keywords = {epoch-based reclamation, garbage collection, hazard pointer, pointer-based reclamation, safe memory reclamation}
}

@inproceedings{10.1145/3385412.3385987,
author = {Suchy, Brian and Campanoni, Simone and Hardavellas, Nikos and Dinda, Peter},
title = {CARAT: A Case for Virtual Memory through Compiler- and Runtime-Based Address Translation},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385987},
doi = {10.1145/3385412.3385987},
abstract = {Virtual memory is a critical abstraction in modern computer systems. Its common model, paging, is currently seeing considerable innovation, yet its implementations continue to be co-designs between power-hungry/latency-adding hardware (e.g., TLBs, pagewalk caches, pagewalkers, etc) and software (the OS kernel). We make a case for a new model for virtual memory, compiler- and runtime-based address translation (CARAT), which instead is a co-design between the compiler and the OS kernel. CARAT can operate without any hardware support, although it could also be retrofitted into a traditional paging model, and could leverage simpler hardware support. CARAT uses compile-time transformations and optimizations combined with tightly-coupled runtime/kernel interaction to generate programs that run efficiently in a physical address space, but nonetheless allow the kernel to maintain protection and dynamically manage physical memory similar to what is possible using traditional virtual memory. We argue for the feasibility of CARAT through an empirical study of application characteristics and kernel behavior, as well as through the design, implementation, and performance evaluation of a CARAT prototype. Because our prototype works at the IR level (in particular, via LLVM bitcode), it can be applied to most C and C++ programs with minimal or no restrictions.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {329–345},
numpages = {17},
keywords = {memory management, virtual memory},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3385973,
author = {Watt, Conrad and Pulte, Christopher and Podkopaev, Anton and Barbier, Guillaume and Dolan, Stephen and Flur, Shaked and Pichon-Pharabod, Jean and Guo, Shu-yu},
title = {Repairing and Mechanising the JavaScript Relaxed Memory Model},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385973},
doi = {10.1145/3385412.3385973},
abstract = {Modern JavaScript includes the SharedArrayBuffer feature, which provides access to true shared memory concurrency. SharedArrayBuffers are simple linear buffers of bytes, and the JavaScript specification defines an axiomatic relaxed memory model to describe their behaviour. While this model is heavily based on the C/C++11 model, it diverges in some key areas. JavaScript chooses to give a well-defined semantics to data-races, unlike the "undefined behaviour" of C/C++11. Moreover, the JavaScript model is mixed-size. This means that its accesses are not to discrete locations, but to (possibly overlapping) ranges of bytes.  We show that the model, in violation of the design intention, does not support a compilation scheme to ARMv8 which is used in practice. We propose a correction, which also incorporates a previously proposed fix for a failure of the model to provide Sequential Consistency of Data-Race-Free programs (SC-DRF), an important correctness condition. We use model checking, in Alloy, to generate small counter-examples for these deficiencies, and investigate our correction. To accomplish this, we also develop a mixed-size extension to the existing ARMv8 axiomatic model.  Guided by our Alloy experimentation, we mechanise (in Coq) the JavaScript model (corrected and uncorrected), our ARMv8 model, and, for the corrected JavaScript model, a "model-internal" SC-DRF proof and a compilation scheme correctness proof to ARMv8. In addition, we investigate a non-mixed-size subset of the corrected JavaScript model, and give proofs of compilation correctness for this subset to x86-TSO, Power, RISC-V, ARMv7, and (again) ARMv8, via the Intermediate Memory Model (IMM).  As a result of our work, the JavaScript standards body (ECMA TC39) will include fixes for both issues in an upcoming edition of the specification.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {346–361},
numpages = {16},
keywords = {Web worker, ARMv8, Coq, weak memory, Alloy},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3386010,
author = {Lee, Sung-Hwan and Cho, Minki and Podkopaev, Anton and Chakraborty, Soham and Hur, Chung-Kil and Lahav, Ori and Vafeiadis, Viktor},
title = {Promising 2.0: Global Optimizations in Relaxed Memory Concurrency},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386010},
doi = {10.1145/3385412.3386010},
abstract = {For more than fifteen years, researchers have tried to support global optimizations in a usable semantics for a concurrent programming language, yet this task has been proven to be very difficult because of (1) the infamous “out of thin air” problem, and (2) the subtle interaction between global and thread-local optimizations. In this paper, we present a solution to this problem by redesigning a key component of the promising semantics (PS) of Kang et al. Our updated PS 2.0 model supports all the results known about the original PS model (i.e., thread-local optimizations, hardware mappings, DRF theorems), but additionally enables transformations based on global value-range analysis as well as register promotion (i.e., making accesses to a shared location local if the location is accessed by only one thread). PS 2.0 also resolves a problem with the compilation of relaxed RMWs to ARMv8, which required an unintended extra fence.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {362–376},
numpages = {15},
keywords = {Relaxed Memory Concurrency, Compiler Optimizations, Operational Semantics},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395643,
author = {Lee, Sung-Hwan and Cho, Minki and Podkopaev, Anton and Chakraborty, Soham and Hur, Chung-Kil and Lahav, Ori and Vafeiadis, Viktor},
title = {Replication Package for Article: Promising 2.0: Global Optimizations in Relaxed Memory Concurrency},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395643},
abstract = {
    <p>The artifact is a Coq development of the paper Promising 2.0: Global Optimizations and Relaxed Memory Concurrency. It includes (i) a Coq development of Promising 2.0 memory model and (ii) compilation correctness proof of Promising 2.0 to IMM. See README.md for more detail.</p>

},
keywords = {Compiler Optimizations, Operational Semantics, Relaxed Memory Concurrency}
}

@inproceedings{10.1145/3385412.3386031,
author = {Friedman, Michal and Ben-David, Naama and Wei, Yuanhao and Blelloch, Guy E. and Petrank, Erez},
title = {NVTraverse: In NVRAM Data Structures, the Destination is More Important than the Journey},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386031},
doi = {10.1145/3385412.3386031},
abstract = {The recent availability of fast, dense, byte-addressable non-volatile memory has led to increasing interest in the problem of designing durable data structures that can recover from system crashes. However, designing durable concurrent data structures that are correct and efficient has proven to be very difficult, leading to many inefficient or incorrect algorithms. In this paper, we present a general transformation that takes a lock-free data structure from a general class called traversal data structure (that we formally define) and automatically transforms it into an implementation of the data structure for the NVRAM setting that is provably durably linearizable and highly efficient. The transformation hinges on the observation that many data structure operations begin with a traversal phase that does not need to be persisted, and thus we only begin persisting when the traversal reaches its destination. We demonstrate the transformation's efficiency through extensive measurements on a system with Intel's recently released Optane DC persistent memory, showing that it can outperform competitors on many workloads.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {377–392},
numpages = {16},
keywords = {Lock-free, Non-blocking, Concurrent Data Structures, Non-volatile Memory},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3385974,
author = {Nigam, Rachit and Atapattu, Sachille and Thomas, Samuel and Li, Zhijing and Bauer, Theodore and Ye, Yuwei and Koti, Apurva and Sampson, Adrian and Zhang, Zhiru},
title = {Predictable Accelerator Design with Time-Sensitive Affine Types},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385974},
doi = {10.1145/3385412.3385974},
abstract = {Field-programmable gate arrays (FPGAs) provide an opportunity to co-design applications with hardware accelerators, yet they remain difficult to program. High-level synthesis (HLS) tools promise to raise the level of abstraction by compiling C or C++ to accelerator designs. Repurposing legacy software languages, however, requires complex heuristics to map imperative code onto hardware structures. We find that the black-box heuristics in HLS can be unpredictable: changing parameters in the program that should improve performance can counterintuitively yield slower and larger designs. This paper proposes a type system that restricts HLS to programs that can predictably compile to hardware accelerators. The key idea is to model consumable hardware resources with a time-sensitive affine type system that prevents simultaneous uses of the same hardware structure. We implement the type system in Dahlia, a language that compiles to HLS C++, and show that it can reduce the size of HLS parameter spaces while accepting Pareto-optimal designs.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {393–407},
numpages = {15},
keywords = {Affine Type Systems, High-Level Synthesis},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395657,
author = {Nigam, Rachit and Atapattu, Sachille and Thomas, Samuel and Li, Zhijing and Bauer, Theodore and Ye, Yuwei and Koti, Apurva and Sampson, Adrian and Zhang, Zhiru},
title = {Replication Package for Article: Predictable Accelerator Design with Time-Sensitive Affine Types},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395657},
abstract = {
    <p>Contains the following repositories: - Dahlia (v0.0.1): The reference compiler built for the paper. - Dahlia-evaluation: The data and the evaluation scripts. - Dahlia-spatial-comparison: Repository to reproduce Dahlia's Spatial evaluation. - Polyphemus: Server framework used to run FPGA experiments on AWS.</p>

},
keywords = {affine types, high-level synthesis}
}

@inproceedings{10.1145/3385412.3385983,
author = {Durst, David and Feldman, Matthew and Huff, Dillon and Akeley, David and Daly, Ross and Bernstein, Gilbert Louis and Patrignani, Marco and Fatahalian, Kayvon and Hanrahan, Pat},
title = {Type-Directed Scheduling of Streaming Accelerators},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385983},
doi = {10.1145/3385412.3385983},
abstract = {Designing efficient, application-specialized hardware accelerators requires assessing trade-offs between a hardware module’s performance and resource requirements. To facilitate hardware design space exploration, we describe Aetherling, a system for automatically compiling data-parallel programs into statically scheduled, streaming hardware circuits. Aetherling contributes a space- and time-aware intermediate language featuring data-parallel operators that represent parallel or sequential hardware modules, and sequence data types that encode a module’s throughput by specifying when sequence elements are produced or consumed. As a result, well-typed operator composition in the space-time language corresponds to connecting hardware modules via statically scheduled, streaming interfaces. We provide rules for transforming programs written in a standard data-parallel language (that carries no information about hardware implementation) into equivalent space-time language programs. We then provide a scheduling algorithm that searches over the space of transformations to quickly generate area-efficient hardware designs that achieve a programmer-specified throughput. Using benchmarks from the image processing domain, we demonstrate that Aetherling enables rapid exploration of hardware designs with different throughput and area characteristics, and yields results that require 1.8-7.9\texttimes{} fewer FPGA slices than those of prior hardware generation systems.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {408–422},
numpages = {15},
keywords = {hardware description languages, FPGAs, space-time types, scheduling, image processing},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395633,
author = {Durst, David and Feldman, Matthew and Huff, Dillon and Akeley, David and Daly, Ross and Bernstein, Gilbert Louis and Patrignani, Marco and Fatahalian, Kayvon and Hanrahan, Pat},
title = {Replication Package for Article: Type-Directed Scheduling of Streaming Accelerators},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395633},
abstract = {
    <p>This is the artifact for the PLDI 2020 paper "Type-Directed Scheduling of Streaming Accelerators". The artifact is a virtual machine that supports the claims, including reproducing the results, of the submission version of the paper. Please go to https://aetherling.org for the latest version of the code.</p>

},
keywords = {FPGAs, hardware description languages, image processing, scheduling, space-time types}
}

@inproceedings{10.1145/3385412.3386003,
author = {Emrich, Frank and Lindley, Sam and Stolarek, Jan and Cheney, James and Coates, Jonathan},
title = {FreezeML: Complete and Easy Type Inference for First-Class Polymorphism},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386003},
doi = {10.1145/3385412.3386003},
abstract = {ML is remarkable in providing statically typed polymorphism without the programmer ever having to write any type annotations. The cost of this parsimony is that the programmer is limited to a form of polymorphism in which quantifiers can occur only at the outermost level of a type and type variables can be instantiated only with monomorphic types.  Type inference for unrestricted System F-style polymorphism is undecidable in general. Nevertheless, the literature abounds with a range of proposals to bridge the gap between ML and System F.  We put forth a new proposal, FreezeML, a conservative extension of ML with two new features. First, let- and lambda-binders may be annotated with arbitrary System F types. Second, variable occurrences may be frozen, explicitly disabling instantiation. FreezeML is equipped with type-preserving translations back and forth between System F and admits a type inference algorithm, an extension of algorithm W, that is sound and complete and which yields principal types.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {423–437},
numpages = {15},
keywords = {type inference, first-class polymorphism, impredicative types},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395639,
author = {Emrich, Frank and Lindley, Sam and Stolarek, Jan and Cheney, James and Coates, Jonathan},
title = {Virtual Machine for Paper "FreezeML: Complete and Easy Type Inference for First-Class Polymorphism"},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395639},
abstract = {
    <p>The artifact is a virtual machine that contains an implementation of the FreezeML system described in the paper. The artifact also contains a collection of examples presented in the paper so that the results can be easily reproduced.</p>

},
keywords = {first-class polymorphism, impredicative types, type inference}
}

@inproceedings{10.1145/3385412.3385982,
author = {Li, Ao and Choi, Jemin Andrew and Long, Fan},
title = {Securing Smart Contract with Runtime Validation},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385982},
doi = {10.1145/3385412.3385982},
abstract = {We present Solythesis, a source to source Solidity compiler which takes a smart contract code and a user specified invariant as the input and produces an instrumented contract that rejects all transactions that violate the invariant. The design of Solythesis is driven by our observation that the consensus protocol and the storage layer are the primary and the secondary performance bottlenecks of Ethereum, respectively. Solythesis operates with our novel delta update and delta check techniques to minimize the overhead caused by the instrumented storage access statements. Our experimental results validate our hypothesis that the overhead of runtime validation, which is often too expensive for other domains, is in fact negligible for smart contracts. The CPU overhead of Solythesis is only 0.1% on average for our 23 benchmark contracts.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {438–453},
numpages = {16},
keywords = {compiler, runtime validation, smart contract},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3385990,
author = {Brent, Lexi and Grech, Neville and Lagouvardos, Sifis and Scholz, Bernhard and Smaragdakis, Yannis},
title = {Ethainter: A Smart Contract Security Analyzer for Composite Vulnerabilities},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385990},
doi = {10.1145/3385412.3385990},
abstract = {Smart contracts on permissionless blockchains are exposed to inherent security risks due to interactions with untrusted entities. Static analyzers are essential for identifying security risks and avoiding millions of dollars worth of damage. We introduce Ethainter, a security analyzer checking information flow with data sanitization in smart contracts. Ethainter identifies composite attacks that involve an escalation of tainted information, through multiple transactions, leading to severe violations. The analysis scales to the entire blockchain, consisting of hundreds of thousands of unique smart contracts, deployed over millions of accounts. Ethainter is more precise than previous approaches, as we confirm by automatic exploit generation (e.g., destroying over 800 contracts on the Ropsten network) and by manual inspection, showing a very high precision of 82.5% valid warnings for end-to-end vulnerabilities. Ethainter’s balance of precision and completeness offers significant advantages over other tools such as Securify, Securify2, and teEther.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {454–469},
numpages = {16},
keywords = {smart contracts, static analysis, information flow},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3760403,
author = {Brent, Lexi and Grech, Neville and Lagouvardos, Sifis and Scholz, Bernhard and Smaragdakis, Yannis},
title = {Ethainter: A Smart Contract Security Analyzer for Composite Vulnerabilities},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3760403},
abstract = {
    <p>The artifact is composed of: - A decompiler (modified Gigahorse) - Ethainter implementation - Data for recreating experiments</p>

},
keywords = {Program Analysis, Smart Contracts}
}

@inproceedings{10.1145/3385412.3386022,
author = {Beillahi, Sidi Mohamed and Ciocarlie, Gabriela and Emmi, Michael and Enea, Constantin},
title = {Behavioral Simulation for Smart Contracts},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386022},
doi = {10.1145/3385412.3386022},
abstract = {While smart contracts have the potential to revolutionize many important applications like banking, trade, and supply-chain, their reliable deployment begs for rigorous formal verification. Since most smart contracts are not annotated with formal specifications, general verification of functional properties is impeded. In this work, we propose an automated approach to verify unannotated smart contracts against specifications ascribed to a few manually-annotated contracts. In particular, we propose a notion of behavioral refinement, which implies inheritance of functional properties. Furthermore, we propose an automated approach to inductive proof, by synthesizing simulation relations on the states of related contracts. Empirically, we demonstrate that behavioral simulations can be synthesized automatically for several ubiquitous classes like tokens, auctions, and escrow, thus enabling the verification of unannotated contracts against functional specifications.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {470–486},
numpages = {17},
keywords = {Simulation, Smart contracts, Refinement, Blockchain},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3385988,
author = {Chen, Qiaochu and Wang, Xinyu and Ye, Xi and Durrett, Greg and Dillig, Isil},
title = {Multi-Modal Synthesis of Regular Expressions},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385988},
doi = {10.1145/3385412.3385988},
abstract = {In this paper, we propose a multi-modal synthesis technique for automatically constructing regular expressions (regexes) from a combination of examples and natural language. Using multiple modalities is useful in this context because natural language alone is often highly ambiguous, whereas examples in isolation are often not sufficient for conveying user intent. Our proposed technique first parses the English description into a so-called hierarchical sketch that guides our programming-by-example (PBE) engine. Since the hierarchical sketch captures crucial hints, the PBE engine can leverage this information to both prioritize the search as well as make useful deductions for pruning the search space. We have implemented the proposed technique in a tool called Regel and evaluate it on over three hundred regexes. Our evaluation shows that Regel achieves 80 % accuracy whereas the NLP-only and PBE-only baselines achieve 43 % and 26 % respectively. We also compare our proposed PBE engine against an adaptation of AlphaRegex, a state-of-the-art regex synthesis tool, and show that our proposed PBE engine is an order of magnitude faster, even if we adapt the search algorithm of AlphaRegex to leverage the sketch. Finally, we conduct a user study involving 20 participants and show that users are twice as likely to successfully come up with the desired regex using Regel compared to without it.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {487–502},
numpages = {16},
keywords = {Programming by Example, Regular Expression, Program Synthesis, Programming by Natural Languages},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3385996,
author = {Lee, DongKwon and Lee, Woosuk and Oh, Hakjoo and Yi, Kwangkeun},
title = {Optimizing Homomorphic Evaluation Circuits by Program Synthesis and Term Rewriting},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385996},
doi = {10.1145/3385412.3385996},
abstract = {We present a new and general method for optimizing homomorphic evaluation circuits. Although fully homomorphic encryption (FHE) holds the promise of enabling safe and secure third party computation, building FHE applications has been challenging due to their high computational costs. Domain-specific optimizations require a great deal of expertise on the underlying FHE schemes, and FHE compilers that aims to lower the hurdle, generate outcomes that are typically sub-optimal as they rely on manually-developed optimization rules. In this paper, based on the prior work of FHE compilers, we propose a method for automatically learning and using optimization rules for FHE circuits. Our method focuses on reducing the maximum multiplicative depth, the decisive performance bottleneck, of FHE circuits by combining program synthesis and term rewriting. It first uses program synthesis to learn equivalences of small circuits as rewrite rules from a set of training circuits. Then, we perform term rewriting on the input circuit to obtain a new circuit that has lower multiplicative depth. Our rewriting method maximally generalizes the learned rules based on the equational matching and its soundness and termination properties are formally proven. Experimental results show that our method generates circuits that can be homomorphically evaluated 1.18x – 3.71x faster (with the geometric mean of 2.05x) than the state-of-the-art method. Our method is also orthogonal to existing domain-specific optimizations.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {503–518},
numpages = {16},
keywords = {Program Synthesis, Homomorphic Encryption Circuit, Term Rewriting},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3756609,
author = {Lee, DongKwon and Lee, Woosuk and Oh, Hakjoo and Yi, Kwangkeun},
title = {Lobster - Optimizing Homomorphic Evaluation Circuits by Program Synthesis and Term Rewriting},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3756609},
abstract = {
    <p>Lobster optimizes homomorphic encryption circuit using aggressive rewrite rules automatically learned by program synthesis technique.</p>

},
keywords = {Homomorphic encryption circuit, Program synthesis, Term rewriting}
}

@inproceedings{10.1145/3385412.3386008,
author = {Vila, Pepe and Ganty, Pierre and Guarnieri, Marco and K\"{o}pf, Boris},
title = {CacheQuery: Learning Replacement Policies from Hardware Caches},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386008},
doi = {10.1145/3385412.3386008},
abstract = {We show how to infer deterministic cache replacement policies using off-the-shelf automata learning and program synthesis techniques. For this, we construct and chain two abstractions that expose the cache replacement policy of any set in the cache hierarchy as a membership oracle to the learning algorithm, based on timing measurements on a silicon CPU. Our experiments demonstrate an advantage in scope and scalability over prior art and uncover two previously undocumented cache replacement policies.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {519–532},
numpages = {14},
keywords = {Automata Learning, Program Synthesis, Reverse Engineering, Cache Replacement Policies},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3759110,
author = {Vila, Pepe and Ganty, Pierre and Guarnieri, Marco and K\"{o}pf, Boris},
title = {Polca: A Tool for Learning Cache Replacement Policies as Automata Models},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3759110},
abstract = {
    <p>Polca implements the automata interface to LearnLib for automatically learning cache replacement policies. It can be connected to CacheQuery for directly interacting with hardware caches. It also contains the learned models, instructions to generate them, and the templates (and results) for (from) the program synthesis evaluation.</p>

},
keywords = {automata, cache, cachequery, learning, learnlib, program synthesis, replacement policy, sketch}
}

@inproceedings{10.1145/3385412.3385984,
author = {Berry, G\'{e}rard and Serrano, Manuel},
title = {HipHop.Js: (A)Synchronous Reactive Web Programming},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385984},
doi = {10.1145/3385412.3385984},
abstract = {We present HipHop.js, a synchronous reactive language that adds synchronous concurrency and preemption to JavaScript. Inspired from Esterel, HipHop.js simplifies the programming of non-trivial temporal behaviors as found in complex web interfaces or IoT controllers and the cooperation between synchronous and asynchronous activities. HipHop.js is compiled into plain sequential JavaScript and executes on unmodified runtime environments. We use three examples to present and discuss HipHop.js: a simple web login form to introduce the language and show how it differs from JavaScript, and two real life examples, a medical prescription pillbox and an interactive music system that show why concurrency and preemption help programming such temporal applications.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {533–545},
numpages = {13},
keywords = {JavaScript, Synchronous Programming, Reactive Programming, Web Programming},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395634,
author = {Berry, G\'{e}rard and Serrano, Manuel},
title = {Replication Package for: HipHop.Js: (A)Synchronous Reactive Web Programming},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395634},
abstract = {
    <p>Our artifact consists of:</p>
<ul>
<li><p>The current HipHop.js implementation, which you can either download fully constructed in the docker file available from the PLDI artifact site (see Part 1 below) or entirely rebuild from the public Hop/HipHop sources files (see Part 3) if you find the 1.3 GB docker file too big or if you want to do it yourself. Once the docker image is downloaded or built, you can validate it by running the standard HipHop implementation test suite (see Part 4).</p></li>
<li><p>The implementation and step-by-step simulation of our Lisinopril medical prescription application described in the paper (Part 2 below). The main Lisinopril HipHop reactive module is exacly the one in the paper. The complete source files can be accessed within the docker image or dowloaded from the web.</p></li>
</ul>

},
keywords = {JavaScript, Reactive Programming, Synchronous Programming, Web Programming}
}

@inproceedings{10.1145/3385412.3386023,
author = {Dathathri, Roshan and Kostova, Blagovesta and Saarikivi, Olli and Dai, Wei and Laine, Kim and Musuvathi, Madan},
title = {EVA: An Encrypted Vector Arithmetic Language and Compiler for Efficient Homomorphic Computation},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386023},
doi = {10.1145/3385412.3386023},
abstract = {Fully-Homomorphic Encryption (FHE) offers powerful capabilities by enabling secure offloading of both storage and computation, and recent innovations in schemes and implementations have made it all the more attractive. At the same time, FHE is notoriously hard to use with a very constrained programming model, a very unusual performance profile, and many cryptographic constraints. Existing compilers for FHE either target simpler but less efficient FHE schemes or only support specific domains where they can rely on expert-provided high-level runtimes to hide complications. This paper presents a new FHE language called Encrypted Vector Arithmetic (EVA), which includes an optimizing compiler that generates correct and secure FHE programs, while hiding all the complexities of the target FHE scheme. Bolstered by our optimizing compiler, programmers can develop efficient general-purpose FHE applications directly in EVA. For example, we have developed image processing applications using EVA, with a very few lines of code. EVA is designed to also work as an intermediate representation that can be a target for compiling higher-level domain-specific languages. To demonstrate this, we have re-targeted CHET, an existing domain-specific compiler for neural network inference, onto EVA. Due to the novel optimizations in EVA, its programs are on average 5.3\texttimes{} faster than those generated by CHET. We believe that EVA would enable a wider adoption of FHE by making it easier to develop FHE applications and domain-specific FHE compilers.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {546–561},
numpages = {16},
keywords = {Homomorphic encryption, neural networks, compiler, privacy-preserving machine learning},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3386037,
author = {Boehm, Hans-J.},
title = {Towards an API for the Real Numbers},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386037},
doi = {10.1145/3385412.3386037},
abstract = {The real numbers are pervasive, both in daily life, and in mathematics. Students spend much time studying their properties. Yet computers and programming languages generally provide only an approximation geared towards performance, at the expense of many of the nice properties we were taught in high school.  Although this is entirely appropriate for many applications, particularly those that are sensitive to arithmetic performance in the usual sense, we argue that there are others where it is a poor choice. If arithmetic computations and result are directly exposed to human users who are not floating point experts, floating point approximations tend to be viewed as bugs. For applications such as calculators, spreadsheets, and various verification tasks, the cost of precision sacrifices is high, and the performance benefit is often not critical. We argue that previous attempts to provide accurate and understandable results for such applications using the recursive reals were great steps in the right direction, but they do not suffice. Comparing recursive reals diverges if they are equal. In many cases, comparison of numbers, including equal ones, is both important, particularly in simple cases, and intractable in the general case.  We propose an API for a real number type that explicitly provides decidable equality in the easy common cases, in which it is often unnatural not to. We describe a surprisingly compact and simple implementation in detail. The approach relies heavily on classical number theory results. We demonstrate the utility of such a facility in two applications: testing floating point functions, and to implement arithmetic in Google's Android calculator application.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {562–576},
numpages = {15},
keywords = {real numbers, decidability, API design, comparison},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395658,
author = {Boehm, Hans-J.},
title = {Real Arithmetic Package and Code to Check Floating Point Precision},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395658},
abstract = {
    <p>Source code and instructions to run the floating point accuracy test described in the paper. This includes the real arithmetic package, with the described comparison support, and an updated version of the previously described underlying recursive reals package (which diverges on exact comparison of equal real numbers). The real arithmetic package is very similar to that currently used by Google's Android calculator.</p>

},
keywords = {accuracy checking, comparison, decidability, floating point, real numbers}
}

@inproceedings{10.1145/3385412.3386013,
author = {Muller, Stefan K. and Singer, Kyle and Goldstein, Noah and Acar, Umut A. and Agrawal, Kunal and Lee, I-Ting Angelina},
title = {Responsive Parallelism with Futures and State},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386013},
doi = {10.1145/3385412.3386013},
abstract = {Motivated by the increasing shift to multicore computers, recent work has developed language support for responsive parallel applications that mix compute-intensive tasks with latency-sensitive, usually interactive, tasks. These developments include calculi that allow assigning priorities to threads, type systems that can rule out priority inversions, and accompanying cost models for predicting responsiveness. These advances share one important limitation: all of this work assumes purely functional programming. This is a significant restriction, because many realistic interactive applications, from games to robots to web servers, use mutable state, e.g., for communication between threads. In this paper, we lift the restriction concerning the use of state. We present λi4, a calculus with implicit parallelism in the form of prioritized futures and mutable state in the form of references. Because both futures and references are first-class values, λi4 programs can exhibit complex dependencies, including interaction between threads and with the external world (users, network, etc). To reason about the responsiveness of λi4 programs, we extend traditional graph-based cost models for parallelism to account for dependencies created via mutable state, and we present a type system to outlaw priority inversions that can lead to unbounded blocking. We show that these techniques are practical by implementing them in C++ and present an empirical evaluation.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {577–591},
numpages = {15},
keywords = {Cilk, shared memory, responsiveness, type systems, parallelism, concurrency, futures},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3692205,
author = {Muller, Stefan K. and Singer, Kyle and Goldstein, Noah and Acar, Umut A. and Agrawal, Kunal and Lee, I-Ting Angelina},
title = {Responsive Parallelism with Futures and State - Software Artifact},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3692205},
abstract = {
    <p>This artifact contains a prototype of the I-Cilk runtime for scheduling parallel code with task priorities, a C++ type system for preventing priority inversions, and benchmarks using both of the former. The artifact demonstrates the practicality of the type system in implementing performant parallel code that uses task priorities. The prototype I-Cilk runtime is in the interactive-cilk directory, with the type system located in interactive-cilk/include/cilk/cilk_priority.h.</p>

},
keywords = {futures, Interactive Cilk, priority inversion, responsiveness, task parallelism, type systems}
}

@inproceedings{10.1145/3385412.3385961,
author = {Zhuo, Youwei and Chen, Jingji and Luo, Qinyi and Wang, Yanzhi and Yang, Hailong and Qian, Depei and Qian, Xuehai},
title = {SympleGraph: Distributed Graph Processing with Precise Loop-Carried Dependency Guarantee},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385961},
doi = {10.1145/3385412.3385961},
abstract = {Graph analytics is an important way to understand relationships in real-world applications. At the age of big data, graphs have grown to billions of edges. This motivates distributed graph processing. Graph processing frameworks ask programmers to specify graph computations in user- defined functions (UDFs) of graph-oriented programming model. Due to the nature of distributed execution, current frameworks cannot precisely enforce the semantics of UDFs, leading to unnecessary computation and communication. In essence, there exists a gap between programming model and runtime execution. This paper proposes SympleGraph, a novel distributed graph processing framework that precisely enforces loop-carried dependency, i.e., when a condition is satisfied by a neighbor, all following neighbors can be skipped. SympleGraph instruments the UDFs to express the loop-carried dependency, then the distributed execution framework enforces the precise semantics by performing dependency propagation dynamically. Enforcing loop-carried dependency requires the sequential processing of the neighbors of each vertex distributed in different nodes. Therefore, the major challenge is to enable sufficient parallelism to achieve high performance. We propose to use circulant scheduling in the framework to allow different machines to process disjoint sets of edges/vertices in parallel while satisfying the sequential requirement. It achieves a good trade-off between precise semantics and parallelism. The significant speedups in most graphs and algorithms indicate that the benefits of eliminating unnecessary computation and communication overshadow the reduced parallelism. Communication efficiency is further optimized by 1) selectively propagating dependency for large-degree vertices to increase net benefits; 2) double buffering to hide communication latency. In a 16-node cluster, SympleGraph outperforms the state-of-the-art system Gemini and D-Galois on average by 1.42\texttimes{} and 3.30\texttimes{}, and up to 2.30\texttimes{} and 7.76\texttimes{}, respectively. The communication reduction compared to Gemini is 40.95% on average and up to 67.48%.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {592–607},
numpages = {16},
keywords = {compilers, big data, graph analytics, graph algorithms},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3385995,
author = {Ritter, Fabian and Hack, Sebastian},
title = {PMEvo: Portable Inference of Port Mappings for out-of-Order Processors by Evolutionary Optimization},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385995},
doi = {10.1145/3385412.3385995},
abstract = {Achieving peak performance in a computer system requires optimizations in every layer of the system, be it hardware or software. A detailed understanding of the underlying hardware, and especially the processor, is crucial to optimize software. One key criterion for the performance of a processor is its ability to exploit instruction-level parallelism. This ability is determined by the port mapping of the processor, which describes the execution units of the processor for each instruction.  Processor manufacturers usually do not share the port mappings of their microarchitectures. While approaches to automatically infer port mappings from experiments exist, they are based on processor-specific hardware performance counters that are not available on every platform.  We present PMEvo, a framework to automatically infer port mappings solely based on the measurement of the execution time of short instruction sequences. PMEvo uses an evolutionary algorithm that evaluates the fitness of candidate mappings with an analytical throughput model formulated as a linear program. Our prototype implementation infers a port mapping for Intel's Skylake architecture that predicts measured instruction throughput with an accuracy that is competitive to existing work. Furthermore, it finds port mappings for AMD's Zen+ architecture and the ARM Cortex-A72 architecture, which are out of scope of existing techniques.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {608–622},
numpages = {15},
keywords = {processor reverse engineering, port mapping, evolutionary algorithm},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3386000,
author = {Wu, Zhenwei and Lu, Kai and Nisbet, Andrew and Zhang, Wenzhe and Luj\'{a}n, Mikel},
title = {PMThreads: Persistent Memory Threads Harnessing Versioned Shadow Copies},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386000},
doi = {10.1145/3385412.3386000},
abstract = {Byte-addressable non-volatile memory (NVM) makes it possible to perform fast in-memory accesses to persistent data using standard load/store processor instructions. Some approaches for NVM are based on durable memory transactions and provide a persistent programming paradigm. However, they cannot be applied to existing multi-threaded applications without extensive source code modifications. Durable transactions typically rely on logging to enforce failure-atomic commits that include additional writes to NVM and considerable ordering overheads. This paper presents PMThreads, a novel user-space runtime that provides transparent failure-atomicity for lock-based parallel programs. A shadow DRAM page is used to buffer application writes for efficient propagation to a dual-copy NVM persistent storage framework during a global quiescent state. In this state, the working NVM copy and the crash-consistent copy of each page are atomically updated, and their roles are switched. A global quiescent state is entered at timed intervals by intercepting pthread lock acquire and release operations to ensure that no thread holds a lock to persistent data. Running on a dual-socket system with 20 cores, we show that PMThreads substantially outperforms the state-of-the-art Atlas, Mnemosyne and NVthreads systems for lock-based benchmarks (Phoenix, PARSEC benchmarks, and microbenchmark stress tests). Using Memcached, we also investigate the scalability of PMThreads and the effect of different time intervals for the quiescent state.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {623–637},
numpages = {15},
keywords = {non-volatile memory, memory persistence, parallel programs},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3756416,
author = {Wu, Zhenwei and Lu, Kai and Nisbet, Andrew and Zhang, Wenzhe and Luj\'{a}n, Mikel},
title = {PMThreads: Persistent Memory Threads Harnessing Versioned Shadow Copies (Artifact)},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3756416},
abstract = {
    <p>This is the artifact for the paper "PMThreads: Persistent Memory Threads Harnessing Versioned Shadow Copies", which is set to be published in PLDI 2020. The artifact contains code, and a Dockerfile for assembling a Docker image with all required dependencies to run the code and reproduce the paper results.</p>

},
keywords = {memory persistence, non-volatile memory}
}

@inproceedings{10.1145/3385412.3386028,
author = {Apostolakis, Sotiris and Xu, Ziyang and Tan, Zujun and Chan, Greg and Campanoni, Simone and August, David I.},
title = {SCAF: A Speculation-Aware Collaborative Dependence Analysis Framework},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386028},
doi = {10.1145/3385412.3386028},
abstract = {Program analysis determines the potential dataflow and control flow relationships among instructions so that compiler optimizations can respect these relationships to transform code correctly. Since many of these relationships rarely or never occur, speculative optimizations assert they do not exist while optimizing the code. To preserve correctness, speculative optimizations add validation checks to activate recovery code when these assertions prove untrue. This approach results in many missed opportunities because program analysis and thus other optimizations remain unaware of the full impact of these dynamically-enforced speculative assertions. To address this problem, this paper presents SCAF, a Speculation-aware Collaborative dependence Analysis Framework. SCAF learns of available speculative assertions via profiling, computes their full impact on memory dependence analysis, and makes this resulting information available for all code optimizations. SCAF is modular (adding new analysis modules is easy) and collaborative (modules cooperate to produce a result more precise than the confluence of all individual results). Relative to the best prior speculation-aware dependence analysis technique, by computing the full impact of speculation on memory dependence analysis, SCAF dramatically reduces the need for expensive-to-validate memory speculation in the hot loops of all 16 evaluated C/C++ SPEC benchmarks.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {638–654},
numpages = {17},
keywords = {speculation, dependence analysis, collaboration},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3751586,
author = {Apostolakis, Sotiris and Xu, Ziyang and Tan, Zujun and Chan, Greg and Campanoni, Simone and August, David I.},
title = {SCAF: A Speculation-Aware Collaborative Dependence Analysis Framework},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3751586},
abstract = {
    <p>Artifact archive for the artifact evaluation of the PLDI 2020 paper, titled "SCAF: A Speculation-Aware Collaborative Dependence Analysis Framework". It contains a Dockerfile along with relevant to this paper software to create a docker image used to reproduce the evaluation results presented in this PLDI 2020 paper.</p>

},
keywords = {compilers, program analysis, speculation}
}

@inproceedings{10.1145/3385412.3385964,
author = {Dasgupta, Sandeep and Dinesh, Sushant and Venkatesh, Deepan and Adve, Vikram S. and Fletcher, Christopher W.},
title = {Scalable Validation of Binary Lifters},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385964},
doi = {10.1145/3385412.3385964},
abstract = {Validating the correctness of binary lifters is pivotal to gain trust in binary analysis, especially when used in scenarios where correctness is important. Existing approaches focus on validating the correctness of lifting instructions or basic blocks in isolation and do not scale to full programs. In this work, we show that formal translation validation of single instructions for a complex ISA like x86-64 is not only practical, but can be used as a building block for scalable full-program validation. Our work is the first to do translation validation of single instructions on an architecture as extensive as x86-64, uses the most precise formal semantics available, and has the widest coverage in terms of the number of instructions tested for correctness. Next, we develop a novel technique that uses validated instructions to enable program-level validation, without resorting to performance-heavy semantic equivalence checking. Specifically, we compose the validated IR sequences using a tool we develop called Compositional Lifter to create a reference standard. The semantic equivalence check between the reference and the lifter output is then reduced to a graph-isomorphism check through the use of semantic preserving transformations. The translation validation of instructions in isolation revealed 29 new bugs in McSema – a mature open-source lifter from x86-64 to LLVM IR. Towards the validation of full programs, our approach was able to prove the translational correctness of 2254/2348 functions taken from LLVM’s single-source benchmark test-suite.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {655–671},
numpages = {17},
keywords = {x86-64, Translation Validation, Compiler Optimizations, LLVM IR, Formal Semantics, Graph Isomorphism},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3756283,
author = {Dasgupta, Sandeep and Dinesh, Sushant and Venkatesh, Deepan and Adve, Vikram S. and Fletcher, Christopher W.},
title = {Artifact for "Scalable Validation of Binary Lifters"},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3756283},
abstract = {
    <p>Snapshot of peer-evaluated artifact corresponding to the published conference paper [1].</p>
<p>[1] Sandeep Dasgupta, Sushant Dinesh, Deepan Venkatesh, Vikram S. Adve, and Christopher W. Fletcher 2020. Scalable Validation of Binary Lifters. In Proceedings of the 2020 ACM SIGPLAN Conference on Programming Language Design and Implementation. ACM. https://doi.org/10.1145/3385412.3385964</p>

},
keywords = {compiler-optimization, detecting-bugs, evaluation, formal-semantics, graph-matching, language-semantics, llvm-ir, mcsema, pldi, reproducing-bugs, reverse-engineering, symbolic-execution-engine, symbolic-summaries, translation-validation, validation, verification-conditions, verification-queries, virtualbox, x86-64}
}

@inproceedings{10.1145/3385412.3385969,
author = {Chatterjee, Krishnendu and Fu, Hongfei and Goharshady, Amir Kafshdar and Goharshady, Ehsan Kafshdar},
title = {Polynomial Invariant Generation for Non-Deterministic Recursive Programs},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385969},
doi = {10.1145/3385412.3385969},
abstract = {We consider the classical problem of invariant generation for programs with polynomial assignments and focus on synthesizing invariants that are a conjunction of strict polynomial inequalities. We present a sound and semi-complete method based on positivstellensaetze, i.e. theorems in semi-algebraic geometry that characterize positive polynomials over a semi-algebraic set.  On the theoretical side, the worst-case complexity of our approach is subexponential, whereas the worst-case complexity of the previous complete method (Kapur, ACA 2004) is doubly-exponential. Even when restricted to linear invariants, the best previous complexity for complete invariant generation is exponential (Colon et al, CAV 2003). On the practical side, we reduce the invariant generation problem to quadratic programming (QCLP), which is a classical optimization problem with many industrial solvers. We demonstrate the applicability of our approach by providing experimental results on several academic benchmarks. To the best of our knowledge, the only previous invariant generation method that provides completeness guarantees for invariants consisting of polynomial inequalities is (Kapur, ACA 2004), which relies on quantifier elimination and cannot even handle toy programs such as our running example.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {672–687},
numpages = {16},
keywords = {Positivstellensaetze, Polynomial programs, Invariant generation},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3386035,
author = {Breck, Jason and Cyphert, John and Kincaid, Zachary and Reps, Thomas},
title = {Templates and Recurrences: Better Together},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386035},
doi = {10.1145/3385412.3386035},
abstract = {This paper is the confluence of two streams of ideas in the literature on generating numerical invariants, namely: (1) template-based methods, and (2) recurrence-based methods. A template-based method begins with a template that contains unknown quantities, and finds invariants that match the template by extracting and solving constraints on the unknowns. A disadvantage of template-based methods is that they require fixing the set of terms that may appear in an invariant in advance. This disadvantage is particularly prominent for non-linear invariant generation, because the user must supply maximum degrees on polynomials, bases for exponents, etc. On the other hand, recurrence-based methods are able to find sophisticated non-linear mathematical relations, including polynomials, exponentials, and logarithms, because such relations arise as the solutions to recurrences. However, a disadvantage of past recurrence-based invariant-generation methods is that they are primarily loop-based analyses: they use recurrences to relate the pre-state and post-state of a loop, so it is not obvious how to apply them to a recursive procedure, especially if the procedure is non-linearly recursive (e.g., a tree-traversal algorithm). In this paper, we combine these two approaches and obtain a technique that uses templates in which the unknowns are functions rather than numbers, and the constraints on the unknowns are recurrences. The technique synthesizes invariants involving polynomials, exponentials, and logarithms, even in the presence of arbitrary control-flow, including any combination of loops, branches, and (possibly non-linear) recursion. For instance, it is able to show that (i) the time taken by merge-sort is O(n log(n)), and (ii) the time taken by Strassen’s algorithm is O(nlog2(7)).},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {688–702},
numpages = {15},
keywords = {Recurrence relation, Invariant generation},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395656,
author = {Breck, Jason and Cyphert, John and Kincaid, Zachary and Reps, Thomas},
title = {Artifact for "Templates and Recurrences: Better Together"},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395656},
abstract = {
    <p>This is the artifact for the PLDI 2020 paper, "Templates and Recurrences: Better Together." The artifact is a virtual machine in OVA (Open Virtualization Archive) format. Its operating system is Ubuntu 18. The virtual machine contains an installation of the CHORA static analysis tool, which is the implementation of the technique described by the associated paper. The virtual machine also contains the benchmark programs that were used in the experiments of the associated paper, along with a script that is designed to replicate the paper's experimental results. The virtual machine has a user account with name "chorauser" and password "chorapassword", and allows login via graphical user interface, or via SSH on port 22.</p>

},
keywords = {Invariant generation, Recurrence relation}
}

@inproceedings{10.1145/3385412.3386018,
author = {Koenig, Jason R. and Padon, Oded and Immerman, Neil and Aiken, Alex},
title = {First-Order Quantified Separators},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386018},
doi = {10.1145/3385412.3386018},
abstract = {Quantified first-order formulas, often with quantifier alternations, are increasingly used in the verification of complex systems. While automated theorem provers for first-order logic are becoming more robust, invariant inference tools that handle quantifiers are currently restricted to purely universal formulas. We define and analyze first-order quantified separators and their application to inferring quantified invariants with alternations. A separator for a given set of positively and negatively labeled structures is a formula that is true on positive structures and false on negative structures. We investigate the problem of finding a separator from the class of formulas in prenex normal form with a bounded number of quantifiers and show this problem is NP-complete by reduction to and from SAT. We also give a practical separation algorithm, which we use to demonstrate the first invariant inference procedure able to infer invariants with quantifier alternations.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {703–717},
numpages = {15},
keywords = {first-order logic, invariant inference},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395650,
author = {Koenig, Jason R. and Padon, Oded and Immerman, Neil and Aiken, Alex},
title = {Artifact for Article: First-Order Quantified Separators},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395650},
abstract = {
    <p>This artifact contains implementations of the separation algorithm from Section 5, the IC3/PDR implementation from Section 6.3, and the evaluation data from Section 7. This artifact can be used to both reproduce the results of the article as well as run these algorithms on novel distributed protocols.</p>

},
keywords = {first-order logic, invariant inference, software verification}
}

@inproceedings{10.1145/3385412.3385985,
author = {Winterer, Dominik and Zhang, Chengyu and Su, Zhendong},
title = {Validating SMT Solvers via Semantic Fusion},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385985},
doi = {10.1145/3385412.3385985},
abstract = {We introduce Semantic Fusion, a general, effective methodology for validating Satisfiability Modulo Theory (SMT) solvers. Our key idea is to fuse two existing equisatisfiable (i.e., both satisfiable or unsatisfiable) formulas into a new formula that combines the structures of its ancestors in a novel manner and preserves the satisfiability by construction. This fused formula is then used for validating SMT solvers. We realized Semantic Fusion as YinYang, a practical SMT solver testing tool. During four months of extensive testing, YinYang has found 45 confirmed, unique bugs in the default arithmetic and string solvers of Z3 and CVC4, the two state-of-the-art SMT solvers. Among these, 41 have already been fixed by the developers. The majority (29/45) of these bugs expose critical soundness issues. Our bug reports and testing effort have been well-appreciated by SMT solver developers.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {718–730},
numpages = {13},
keywords = {Fuzz testing, SMT solvers, Semantic fusion},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3386004,
author = {Chowdhary, Sangeeta and Lim, Jay P. and Nagarakatte, Santosh},
title = {Debugging and Detecting Numerical Errors in Computation with Posits},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386004},
doi = {10.1145/3385412.3386004},
abstract = {Posit is a recently proposed alternative to the floating point representation (FP). It provides tapered accuracy. Given a fixed number of bits, the posit representation can provide better precision for some numbers compared to FP, which has generated significant interest in numerous domains. Being a representation with tapered accuracy, it can introduce high rounding errors for numbers outside the above golden zone. Programmers currently lack tools to detect and debug errors while programming with posits.  This paper presents PositDebug, a compile-time instrumentation that performs shadow execution with high precision values to detect various errors in computation using posits. To assist the programmer in debugging the reported error, PositDebug also provides directed acyclic graphs of instructions, which are likely responsible for the error. A contribution of this paper is the design of the metadata per memory location for shadow execution that enables productive debugging of errors with long-running programs. We have used PositDebug to detect and debug errors in various numerical applications written using posits. To demonstrate that these ideas are applicable even for FP programs, we have built a shadow execution framework for FP programs that is an order of magnitude faster than Herbgrind.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {731–746},
numpages = {16},
keywords = {numerical errors, Posits, dynamic analysis},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3742225,
author = {Chowdhary, Sangeeta and Lim, Jay P. and Nagarakatte, Santosh},
title = {PositDebug Artifact: Debugging and Detecting Numerical Errors in Computation with Posits},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3742225},
abstract = {
    <p>This artifact contains a shadow execution framework for finding numerical errors in applications using both posits and floating point. The prototype for posits is called PositDebug and the prototype for floating point programs is called FPSsanitizer.</p>

},
keywords = {cancellation, CORDIC, floating point, FPSanitizer, numerical errors, PositDebug, posits}
}

@inproceedings{10.1145/3385412.3385993,
author = {Roemer, Jake and Gen\c{c}, Kaan and Bond, Michael D.},
title = {SmartTrack: Efficient Predictive Race Detection},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385993},
doi = {10.1145/3385412.3385993},
abstract = {Widely used data race detectors, including the state-of-the-art FastTrack algorithm, incur performance costs that are acceptable for regular in-house testing, but miss races detectable from the analyzed execution. Predictive analyses detect more data races in an analyzed execution than FastTrack detects, but at significantly higher performance cost.  This paper presents SmartTrack, an algorithm that optimizes predictive race detection analyses, including two analyses from prior work and a new analysis introduced in this paper. SmartTrack incorporates two main optimizations: (1) epoch and ownership optimizations from prior work, applied to predictive analysis for the first time, and (2) novel conflicting critical section optimizations introduced by this paper. Our evaluation shows that SmartTrack achieves performance competitive with FastTrack—a qualitative improvement in the state of the art for data race detection.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {747–762},
numpages = {16},
keywords = {dynamic predictive analysis, Data race detection},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3762236,
author = {Roemer, Jake and Gen\c{c}, Kaan and Bond, Michael D.},
title = {Efficient Predictive Race Detection},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3762236},
abstract = {
    <p>This is the artifact submitted for the Artifact Evaluation process for the PLDI 2020 paper "SmartTrack: Efficient Predictive Race Detection" by Jake Roemer, Kaan Gen\c{c}, and Michael D. Bond. This artifact reproduces the paper's experiments; the SmartTrack source code is available here:</p>
<p>https://github.com/PLaSSticity/SmartTrack-pldi20</p>
<p>The artifact file contains a README with more details. Two key issues encountered by Artifact Evaluation Committee members: (1) many executions needed a lot of time and often timed out (for unknown reasons), and (2) after an execution timed out, it would keep running while the next execution started. Artifact author Jake was not able to reproduce the first problem, but the following change should help with the second problem:</p>
<p>In exp/util/ExecUtil.java, change</p>
<p>Runtime.getRuntime().exec("pkill -6 JikesRVM");</p>
<p>to</p>
<p>if (options.config.isRoadRunner()) { Runtime.getRuntime().exec("pkill -6 --full rragent.jar"); } else { Runtime.getRuntime().exec("pkill -6 JikesRVM"); } and then recompile ExecUtil.java with javac.</p>
<p>Sadly, Jake (artifact author and first author of the paper) passed away in April 2020. Please contact Kaan Gen\c{c} and Mike Bond for questions about the artifact.</p>

},
keywords = {Data race detection, dynamic predictive analysis}
}

@inproceedings{10.1145/3385412.3386036,
author = {Qin, Boqin and Chen, Yilun and Yu, Zeming and Song, Linhai and Zhang, Yiying},
title = {Understanding Memory and Thread Safety Practices and Issues in Real-World Rust Programs},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386036},
doi = {10.1145/3385412.3386036},
abstract = {Rust is a young programming language designed for systems software development. It aims to provide safety guarantees like high-level languages and performance efficiency like low-level languages. The core design of Rust is a set of strict safety rules enforced by compile-time checking. To support more low-level controls, Rust allows programmers to bypass these compiler checks to write unsafe code.   It is important to understand what safety issues exist in real Rust programs and how Rust safety mechanisms impact programming practices. We performed the first empirical study of Rust by close, manual inspection of 850 unsafe code usages and 170 bugs in five open-source Rust projects, five widely-used Rust libraries, two online security databases, and the Rust standard library. Our study answers three important questions: how and why do programmers write unsafe code, what memory-safety issues real Rust programs have, and what concurrency bugs Rust programmers make. Our study reveals interesting real-world Rust program behaviors and new issues Rust programmers make. Based on our study results, we propose several directions of building Rust bug detectors and built two static bug detectors, both of which revealed previously unknown bugs.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {763–779},
numpages = {17},
keywords = {Memory Bug, Concurrency Bug, Rust, Bug Study},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3756301,
author = {Qin, Boqin and Chen, Yilun and Yu, Zeming and Song, Linhai and Zhang, Yiying},
title = {Replication Package for Article: Understanding Memory and Thread Safety Practices and Issues in Real-World Rust Programs},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3756301},
abstract = {
    <p>The artifact is to support the data in our paper with programs. It contains five directories related to the corresponding sections in the paper. section-2-background-and-related-work contains scripts and raw data to plot Fig. 1 and Fig. 2. section-4-unsafe-usages contains a bench testing for safe and unsafe code, and a script to count unsafe statistics. section-5-memory-safety-issues contains the fix commits of our studied memory bugs, and our reproduced memory bugs. section-6-thread-safety-issues contains the fix commits of our studied blocking and non-blocking bugs, our reproduced blocking and non-blocking bugs, and the code to count cases where locks are manually dropped. section-7-bug-detection contains our detection tools for use-after-free and double-lock.</p>

},
keywords = {Bug Study, Concurrency Bug, Memory Bug, Rust}
}

@inproceedings{10.1145/3385412.3386021,
author = {Li, Yuanbo and Zhang, Qirun and Reps, Thomas},
title = {Fast Graph Simplification for Interleaved Dyck-Reachability},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386021},
doi = {10.1145/3385412.3386021},
abstract = {Many program-analysis problems can be formulated as graph-reachability problems. Interleaved Dyck language reachability. Interleaved Dyck language reachability (InterDyck-reachability) is a fundamental framework to express a wide variety of program-analysis problems over edge-labeled graphs. The InterDyck language represents an intersection of multiple matched-parenthesis languages (i.e., Dyck languages). In practice, program analyses typically leverage one Dyck language to achieve context-sensitivity, and other Dyck languages to model data dependences, such as field-sensitivity and pointer references/dereferences. In the ideal case, an InterDyck-reachability framework should model multiple Dyck languages simultaneously. Unfortunately, precise InterDyck-reachability is undecidable. Any practical solution must over-approximate the exact answer. In the literature, a lot of work has been proposed to over-approximate the InterDyck-reachability formulation. This paper offers a new perspective on improving both the precision and the scalability of InterDyck-reachability: we aim to simplify the underlying input graph G. Our key insight is based on the observation that if an edge is not contributing to any InterDyck-path, we can safely eliminate it from G. Our technique is orthogonal to the InterDyck-reachability formulation, and can serve as a pre-processing step with any over-approximating approaches for InterDyck-reachability. We have applied our graph simplification algorithm to pre-processing the graphs from a recent InterDyck-reachability-based taint analysis for Android. Our evaluation on three popular InterDyck-reachability algorithms yields promising results. In particular, our graph-simplification method improves both the scalability and precision of all three InterDyck-reachability algorithms, sometimes dramatically.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {780–793},
numpages = {14},
keywords = {CFL-Reachability, Static Analysis},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3386026,
author = {Antoniadis, Anastasios and Filippakis, Nikos and Krishnan, Paddy and Ramesh, Raghavendra and Allen, Nicholas and Smaragdakis, Yannis},
title = {Static Analysis of Java Enterprise Applications: Frameworks and Caches, the Elephants in the Room},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386026},
doi = {10.1145/3385412.3386026},
abstract = {Enterprise applications are a major success domain of Java, and Java is the default setting for much modern static analysis research. It would stand to reason that high-quality static analysis of Java enterprise applications would be commonplace, but this is far from true. Major analysis frameworks feature virtually no support for enterprise applications and offer analyses that are woefully incomplete and vastly imprecise, when at all scalable.  In this work, we present two techniques for drastically enhancing the completeness and precision of static analysis for Java enterprise applications. The first technique identifies domain-specific concepts underlying all enterprise application frameworks, captures them in an extensible, declarative form, and achieves modeling of components and entry points in a largely framework-independent way. The second technique offers precision and scalability via a sound-modulo-analysis modeling of standard data structures.  In realistic enterprise applications (an order of magnitude larger than prior benchmarks in the literature) our techniques achieve high degrees of completeness (on average more than 4x higher than conventional techniques) and speedups of about 6x compared to the most precise conventional analysis, with higher precision on multiple metrics. The result is JackEE, an enterprise analysis framework that can offer precise, high-completeness static modeling of realistic enterprise applications.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {794–807},
numpages = {14},
keywords = {points-to analysis, Java EE, static analysis},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3742711,
author = {Antoniadis, Anastasios and Filippakis, Nikos and Krishnan, Paddy and Ramesh, Raghavendra and Allen, Nicholas and Smaragdakis, Yannis},
title = {Artifact: Static Analysis of Enterprise Applications: Frameworks and Caches, the Elephants in the Room},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3742711},
abstract = {
    <p>This artifact contains the evaluation benchmarks for the paper "Static Analysis of Java Enterprise Applications: Frameworks and Caches, the Elephants in the Room" , accepted in the Programming Language Design and Implementation Conference (PLDI'20).</p>
<p>Link to paper preprint</p>
<p>Abstract:</p>
<p>Enterprise applications are a major success domain of Java, and Java</p>
<p>is the default setting for much modern static analysis research. It</p>
<p>would stand to reason that high-quality static analysis of Java</p>
<p>enterprise applications would be commonplace, but this is far from</p>
<p>true. Major analysis frameworks feature virtually no support for</p>
<p>enterprise applications and offer analyses that are woefully</p>
<p>incomplete and vastly imprecise, when at all scalable.</p>
<p>In this work, we present two techniques for drastically enhancing</p>
<p>the completeness and precision of static analysis for Java</p>
<p>enterprise applications. The first technique identifies</p>
<p>domain-specific concepts underlying all enterprise application</p>
<p>frameworks, captures them in an extensible, declarative form, and</p>
<p>achieves modeling of components and entry points in a largely</p>
<p>framework-independent way. The second technique offers precision and</p>
<p>scalability via a sound-modulo-analysis modeling of standard data</p>
<p>structures.</p>
<p>In realistic enterprise applications (an order of magnitude larger than</p>
<p>prior benchmarks in the literature) our techniques achieve high degrees of</p>
<p>completeness (on average more than 4x higher than conventional techniques) and</p>
<p>speedups of about 6x compared to the most precise conventional analysis, with</p>
<p>higher precision on multiple metrics. The result is JackEE, an</p>
<p>enterprise analysis framework that can offer precise, high-completeness</p>
<p>static modeling of realistic enterprise applications.</p>

},
keywords = {Java EE, points-to analysis, static analysis}
}

@inproceedings{10.1145/3385412.3385989,
author = {Olivry, Auguste and Langou, Julien and Pouchet, Louis-No\"{e}l and Sadayappan, P. and Rastello, Fabrice},
title = {Automated Derivation of Parametric Data Movement Lower Bounds for Affine Programs},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385989},
doi = {10.1145/3385412.3385989},
abstract = {Researchers and practitioners have for long worked on improving the computational complexity of algorithms, focusing on reducing the number of operations needed to perform a computation. However the hardware trend nowadays clearly shows a higher performance and energy cost for data movements than computations: quality algorithms have to minimize data movements as much as possible. The theoretical operational complexity of an algorithm is a function of the total number of operations that must be executed, regardless of the order in which they will actually be executed. But theoretical data movement (or, I/O) complexity is fundamentally different: one must consider all possible legal schedules of the operations to determine the minimal number of data movements achievable, a major theoretical challenge. I/O complexity has been studied via complex manual proofs, e.g., refined from Ω(n3/√S) for matrix-multiply on a cache size S by Hong &amp; Kung to 2n3/√S by Smith et al. While asymptotic complexity may be sufficient to compare I/O potential between broadly different algorithms, the accuracy of the reasoning depends on the tightness of these I/O lower bounds. Precisely, exposing constants is essential to enable precise comparison between different algorithms: for example the 2n3/√S lower bound allows to demonstrate the optimality of panel-panel tiling for matrix-multiplication. We present the first static analysis to automatically derive non-asymptotic parametric expressions of data movement lower bounds with scaling constants, for arbitrary affine computations. Our approach is fully automatic, assisting algorithm designers to reason about I/O complexity and make educated decisions about algorithmic alternatives.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {808–822},
numpages = {15},
keywords = {I/O lower bounds, Affine programs, Static analysis, Data access complexity},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3385963,
author = {Chou, Stephen and Kjolstad, Fredrik and Amarasinghe, Saman},
title = {Automatic Generation of Efficient Sparse Tensor Format Conversion Routines},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385963},
doi = {10.1145/3385412.3385963},
abstract = {This paper shows how to generate code that efficiently converts sparse tensors between disparate storage formats (data layouts) such as CSR, DIA, ELL, and many others. We decompose sparse tensor conversion into three logical phases: coordinate remapping, analysis, and assembly. We then develop a language that precisely describes how different formats group together and order a tensor’s nonzeros in memory. This lets a compiler emit code that performs complex remappings of nonzeros when converting between formats. We also develop a query language that can extract statistics about sparse tensors, and we show how to emit efficient analysis code that computes such queries. Finally, we define an abstract interface that captures how data structures for storing a tensor can be efficiently assembled given specific statistics about the tensor. Disparate formats can implement this common interface, thus letting a compiler emit optimized sparse tensor conversion code for arbitrary combinations of many formats without hard-coding for any specific combination. Our evaluation shows that the technique generates sparse tensor conversion routines with performance between 1.00 and 2.01\texttimes{} that of hand-optimized versions in SPARSKIT and Intel MKL, two popular sparse linear algebra libraries. And by emitting code that avoids materializing temporaries, which both libraries need for many combinations of source and target formats, our technique outperforms those libraries by 1.78 to 4.01\texttimes{} for CSC/COO to DIA/ELL conversion.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {823–838},
numpages = {16},
keywords = {attribute query language, sparse tensor algebra, coordinate remapping notation, sparse tensor assembly, sparse tensor conversion, sparse tensor formats},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3385962,
author = {Phulia, Ankush and Bhagee, Vaibhav and Bansal, Sorav},
title = {OOElala: Order-of-Evaluation Based Alias Analysis for Compiler Optimization},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385962},
doi = {10.1145/3385412.3385962},
abstract = {In C, the order of evaluation of expressions is unspecified; further for expressions that do not involve function calls, C semantics ensure that there cannot be a data race between two evaluations that can proceed in either order (or concurrently). We explore the optimization opportunity enabled by these non-deterministic expression evaluation semantics in C, and provide a sound compile-time alias analysis to realize the same. Our algorithm is implemented as a part of the Clang/LLVM infrastructure, in a tool called OOElala. Our experimental results demonstrate that the untapped optimization opportunity is significant: code patterns that enable such optimizations are common; the enabled transformations can range from vectorization to improved instruction selection and register allocation; and the resulting speedups can be as high as 2.6x on already-optimized code.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {839–853},
numpages = {15},
keywords = {Compiler Optimization, Undefined Behaviour},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395635,
author = {Phulia, Ankush and Bhagee, Vaibhav and Bansal, Sorav},
title = {Replication Package for OOElala: Order-of-Evaluation Based Alias Analysis for Compiler Optimization},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395635},
abstract = {
    <p>This artifact contains the implementation for the algorithm described in the paper, <em>OOElala: Order-of-Evaluation Based Alias Analysis for Compiler Optimization</em>, accepted at PLDI 2020.</p>
<h3 id="terminology">Terminology</h3>
<ul>
<li>We use <code>OOElala</code>, <code>ooelala</code> and <code>clang-unseq</code> interchangeably to refer to the tool/binary which we have implemented and produced as a part of this work.</li>
<li><code><artifact-home></artifact-home></code> refers to <code>/home/$USER/ooelala-project</code></li>
</ul>
<h3 id="structure-of-the-artifact">Structure of the artifact</h3>
<p>This artifact directory is structured into the following subdirectories, each of which is described subsequently:</p>
<ul>
<li><em>litmus_tests</em> - This directory contains the implementation of the two examples, which have been introduced in Section 1.1 of the paper, to spearhead the discussion about the key idea of the paper. The makefile is used to run these two examples, as described in detail in the later sections.
<ul>
<li><em>array_min_max</em> - This subdirectory contains the code, for the first example, on the left of the figure.</li>
<li><em>nested_loop</em> - This subdirectory contains the code for an example, which isolates the kernel initialization code described in the second image on the right. The implementation captures the basic idea for the SPEC 2017 example, as discussed in section 1.1 of the paper.</li>
</ul></li>
<li><em>ooelala</em> - This directory contains the source code for our tool, OOELala, which has been implemented over clang/llvm 8.0.0.
<ul>
<li><em>src</em> - This sub-directory contains the source code for the optimizing compiler implementation which includes the AST analysis to identify the must-not-alias predicates and the Alias Analysis which utilises the must-not-alias information to enable further optimisations. This has been added as a sub-module and the specific implementation details can be found in the commit history and commit messages of this sub-module.</li>
<li><em>ubsan</em> - This sub-directory contains the source code for the implementation of the UB Sanitizer which uses the must-not-alias predicates generated after the AST analysis to implement the runtime checks. This has been added as a sub-module and the specific implementation details can be found in the commit history and commit messages of this sub-module.</li>
</ul></li>
<li><em>spec</em> - This directory contains the resources which we use to run the SPEC CPU 2017 benchmarks, with clang and OOELala.
<ul>
<li><em>configs</em> - This subdirectory contains the config files <code>clang-unseq.cfg</code> and <code>clang.cfg</code>, which are used when we build and run the SPEC CPU 2017 suite with OOELala and clang respectively. Additionally, this directory also contains config files <code>clang-ubsan.cfg</code> and <code>clang-ubsan-unseq.cfg</code>, which are used to specify that SPEC should be built and run with clang and OOELala respectively, with UB Sanitizer checks enabled and no optimisations.</li>
<li><em>makefiles</em> - This subdirectory contains the makefiles, which are used to compile and run specific SPEC benchmarks or generate object/LLVM files for some specific source files, in some specific benchmarks. These have been used to identify the patterns discussed in figure 2 of the paper. For running these on SPEC refrate inputs refer to <code>Readme.md</code> in the subdirectory.</li>
<li><em>scripts</em> - This subdirectory contains the scripts which are used to build and run the SPEC 2017 benchmarks and generate the performance numbers presented in table 6 of the paper. This also contains the post processing python script which is used to generate the summary of the aliasing stats, which are presented in Table 5 of the paper. The list of scripts and their functionality is described in the Readme.md file present in this subdirectory.</li>
</ul></li>
<li><em>polybench</em> - This directory contains the resources which we use to run the Polybench benchmark suite, with clang and OOELala
<ul>
<li><em>common</em> - This subdirectory contains the Polybench header file which needs to be included in the benchmarks.</li>
<li><em>scripts</em> - This subdirectory contains the scripts used to build and run the Polybench benchmarks to obtain the speedups listed in Table 4 of the paper. Comparisons between various compilers have been drawn.</li>
<li><em>selected_benchmarks</em> - This represents the selected subset of benchmarks which we have annotated with custom <code>RESTRICT</code> macro predicates (corresponding to <code>CANT_ALIAS</code> used in the paper), used to provide the additional aliasing information, but in no way modifying the behaviour of the program</li>
</ul></li>
<li><em>sample_outputs</em> - This directory contains a set of sample outputs which are obtained on running the SPEC CPU 2017 and the polybench benchmarks. These can be used by the developers to verify the output format
<ul>
<li><em>spec</em> - This contains the results and stats obtained for a sample run of SPEC CPU 2017, with clang and clang-unseq</li>
<li><em>polybench</em> - This contains the results and stats obtained for a sample run of Polybench, with clang and clang-unseq</li>
</ul></li>
<li><em>CANT_ALIAS.md</em> - This is a tutorial which discusses the <em>CANT_ALIAS</em> predicate described in the paper. It outlines the use of the macro and the subtleties associated with that.</li>
</ul>

},
keywords = {Alias Analysis, Clang, Compiler Optimization, LLVM, Polybench, SPEC CPU 2017, Undefined behaviour}
}

@inproceedings{10.1145/3385412.3386030,
author = {Rocha, Rodrigo C. O. and Petoumenos, Pavlos and Wang, Zheng and Cole, Murray and Leather, Hugh},
title = {Effective Function Merging in the SSA Form},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386030},
doi = {10.1145/3385412.3386030},
abstract = {Function merging is an important optimization for reducing code size. This technique eliminates redundant code across functions by merging them into a single function. While initially limited to identical or trivially similar functions, the most recent approach can identify all merging opportunities in arbitrary pairs of functions. However, this approach has a serious limitation which prevents it from reaching its full potential. Because it cannot handle phi-nodes, the state-of-the-art applies register demotion to eliminate them before applying its core algorithm. While a superficially minor workaround, this has a three-fold negative effect: by artificially lengthening the instruction sequences to be aligned, it hinders the identification of mergeable instruction; it prevents a vast number of functions from being profitably merged; it increases compilation overheads, both in terms of compile-time and memory usage.  We present SalSSA, a novel approach that fully supports the SSA form, removing any need for register demotion. By doing so, we notably increase the number of profitably merged functions. We implement SalSSA in LLVM and apply it to the SPEC 2006 and 2017 suites. Experimental results show that our approach delivers on average, 7.9% to 9.7% reduction on the final size of the compiled code. This translates to around 2x more code size reduction over the state-of-the-art. Moreover, as a result of aligning shorter sequences of instructions and reducing the number of wasteful merge operations, our new approach incurs an average compile-time overhead of only 5%, 3x less than the state-of-the-art, while also reducing memory usage by over 2x.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {854–868},
numpages = {15},
keywords = {Code Size Reduction, LTO, Function Merging},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.6084/m9.figshare.12089217,
author = {Rocha, Rodrigo C. O. and Petoumenos, Pavlos and Wang, Zheng and Cole, Murray and Leather, Hugh},
title = {Artifact for "Effective Function Merging in the SSA Form"},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.6084/m9.figshare.12089217},
abstract = {
    <p>This artifact includes a modified version of the open-source LLVM compiler framework. It includes a prototype implementation of our novel function merging technique, called SalSSA. It also includes an implementation of the state-of-the-art function merging technique. This artifact also provides Makefile scripts to run spec2006 using these function merging techniques and scripts to compare them.</p>

},
keywords = {code size, compiler optimization, function merging, link-time optimization, LTO}
}

@inproceedings{10.1145/3385412.3386002,
author = {Chen, Jianhui and He, Fei},
title = {Proving Almost-Sure Termination by Omega-Regular Decomposition},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386002},
doi = {10.1145/3385412.3386002},
abstract = {Almost-sure termination is the most basic liveness property of probabilistic programs. We present a novel decomposition-based approach for proving almost-sure termination of probabilistic programs with complex control-flow structure and non-determinism. Our approach automatically decomposes the runs of the probabilistic program into a finite union of ω-regular subsets and then proves almost-sure termination of each subset based on the notion of localized ranking supermartingales. Compared to the lexicographic methods and the compositional methods, our approach does not require a lexicographic order over the ranking supermartingales as well as the so-called unaffecting condition. Thus it has high generality. We present the algorithm of our approach and prove its soundness, as well as its relative completeness. We show that our approach can be applied to some hard cases and the evaluation on the benchmarks of previous works shows the significant efficiency of our approach.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {869–882},
numpages = {14},
keywords = {Omega-Regular Languages, Ranking Supermartingales, Almost-Sure Termination, Probabilistic Programs},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3386006,
author = {Gehr, Timon and Steffen, Samuel and Vechev, Martin},
title = {λPSI: Exact Inference for Higher-Order Probabilistic Programs},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386006},
doi = {10.1145/3385412.3386006},
abstract = {We present λPSI, the first probabilistic programming language and system that supports higher-order exact inference for probabilistic programs with first-class functions, nested inference and discrete, continuous and mixed random variables. λPSI’s solver is based on symbolic reasoning and computes the exact distribution represented by a program. We show that λPSI is practically effective—it automatically computes exact distributions for a number of interesting applications, from rational agents to information theory, many of which could so far only be handled approximately.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {883–897},
numpages = {15},
keywords = {Exact, Probabilistic Programming, Higher-order},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3765314,
author = {Gehr, Timon and Steffen, Samuel and Vechev, Martin},
title = {Lpsi-Artifact},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3765314},
abstract = {
    <p>Artifact for PLDI'20 paper "λPSI: Exact Inference for Higher-Order Probabilistic Programs".</p>

},
keywords = {Exact, Higher-order, Probabilistic Programming}
}

@inproceedings{10.1145/3385412.3386009,
author = {Baudart, Guillaume and Mandel, Louis and Atkinson, Eric and Sherman, Benjamin and Pouzet, Marc and Carbin, Michael},
title = {Reactive Probabilistic Programming},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386009},
doi = {10.1145/3385412.3386009},
abstract = {Synchronous modeling is at the heart of programming languages like Lustre, Esterel, or Scade used routinely for implementing safety critical control software, e.g., fly-by-wire and engine control in planes. However, to date these languages have had limited modern support for modeling uncertainty --- probabilistic aspects of the software's environment or behavior --- even though modeling uncertainty is a primary activity when designing a control system.  In this paper we present ProbZelus the first synchronous probabilistic programming language. ProbZelus conservatively provides the facilities of a synchronous language to write control software, with probabilistic constructs to model uncertainties and perform inference-in-the-loop.  We present the design and implementation of the language. We propose a measure-theoretic semantics of probabilistic stream functions and a simple type discipline to separate deterministic and probabilistic expressions. We demonstrate a semantics-preserving compilation into a first-order functional language that lends itself to a simple presentation of inference algorithms for streaming models. We also redesign the delayed sampling inference algorithm to provide efficient streaming inference. Together with an evaluation on several reactive applications, our results demonstrate that ProbZelus enables the design of reactive probabilistic applications and efficient, bounded memory inference.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {898–912},
numpages = {15},
keywords = {Probabilistic programming, Compilation, Reactive programming, Streaming inference, Semantics},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395641,
author = {Baudart, Guillaume and Mandel, Louis and Atkinson, Eric and Sherman, Benjamin and Pouzet, Marc and Carbin, Michael},
title = {Replication Package for Article: Reactive Probabilistic Programming},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395641},
abstract = {
    <p>This is the artifact of the PLDI 2020 paper <em>Reactive Probabilistic Programming</em> by Guillaume Baudart, Louis Mandel, Eric Atkinson, Benjamin Sherman, Marc Pouzet, and Michael Carbin. The archive contains - <code>pldi2020.pdf</code>: Paper with appendices - <code>README.md</code>: Details on how the code is linked to the paper and how to run the code and the benchmarks - <code>ProbZelus-PLDI20.ova</code>: Linux image in the Open Virtualization Format with the ProbZelus compiler and benchmarks - <code>LICENSE.txt</code>: ProbZelus license.</p>
<p>The artifact can be used to check the implementation of the algorithms presented in the paper and to reproduce the evaluation.</p>

},
keywords = {Benchmark, Compiler, Delayed sampling, Inference, Probabilistic programming, Runtime, Sequential Monte Carlo, Synchronous programming}
}

@inproceedings{10.1145/3385412.3385970,
author = {Cauligi, Sunjay and Disselkoen, Craig and Gleissenthall, Klaus v. and Tullsen, Dean and Stefan, Deian and Rezk, Tamara and Barthe, Gilles},
title = {Constant-Time Foundations for the New Spectre Era},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385970},
doi = {10.1145/3385412.3385970},
abstract = {The constant-time discipline is a software-based countermeasure used for protecting high assurance cryptographic implementations against timing side-channel attacks. Constant-time is effective (it protects against many known attacks), rigorous (it can be formalized using program semantics), and amenable to automated verification. Yet, the advent of micro-architectural attacks makes constant-time as it exists today far less useful.  This paper lays foundations for constant-time programming in the presence of speculative and out-of-order execution. We present an operational semantics and a formal definition of constant-time programs in this extended setting. Our semantics eschews formalization of microarchitectural features (that are instead assumed under adversary control), and yields a notion of constant-time that retains the elegance and tractability of the usual notion. We demonstrate the relevance of our semantics in two ways: First, by contrasting existing Spectre-like attacks with our definition of constant-time. Second, by implementing a static analysis tool, Pitchfork, which detects violations of our extended constant-time property in real world cryptographic libraries.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {913–926},
numpages = {14},
keywords = {speculative execution, semantics, static analysis, pectre},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395651,
author = {Cauligi, Sunjay and Disselkoen, Craig and Gleissenthall, Klaus v. and Tullsen, Dean and Stefan, Deian and Rezk, Tamara and Barthe, Gilles},
title = {Pitchfork-Angr},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395651},
abstract = {
    <p>Pitchfork is a static analysis tool, built on angr (https://github.com/angr/angr), which performs speculative symbolic execution. That is, it not only executes the "correct" or "sequential" paths of a program, but also the "mispredicted" or "speculative" paths, subject to some speculation window size. Pitchfork finds paths where secret data is used in either address calculations or branch conditions (and thus leaked), even speculatively - these paths represent Spectre vulnerabilities. Pitchfork covers Spectre v1, Spectre v1.1, and Spectre v4.</p>

},
keywords = {spectre, speculative execution, static analysis, symbolic execution}
}

@inproceedings{10.1145/3385412.3386014,
author = {Fragoso Santos, Jos\'{e} and Maksimovi\'{c}, Petar and Ayoun, Sacha-\'{E}lie and Gardner, Philippa},
title = {Gillian, Part i: A Multi-Language Platform for Symbolic Execution},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386014},
doi = {10.1145/3385412.3386014},
abstract = {We introduce Gillian, a platform for developing symbolic analysis tools for programming languages. Here, we focus on the symbolic execution engine at the heart of Gillian, which is parametric on the memory model of the target language. We give a formal description of the symbolic analysis and a modular implementation that closely follows this description. We prove a parametric soundness result, introducing restriction on abstract states, which generalises path conditions used in classical symbolic execution. We instantiate to obtain trusted symbolic testing tools for JavaScript and C, and use these tools to find bugs in real-world code, thus demonstrating the viability of our parametric approach.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {927–942},
numpages = {16},
keywords = {bounded verification, parametric semantics, C, Symbolic execution, bug-finding, JavaScript},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395646,
author = {Fragoso Santos, Jos\'{e} and Maksimovi\'{c}, Petar and Ayoun, Sacha-\'{E}lie and Gardner, Philippa},
title = {Gillian, Part I: A Multi-Language Platform for Symbolic Execution},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395646},
abstract = {
    <p>This artifact contains the Gillian platform, as it was for the submission of the corresponding PLDI'20 paper.</p>
<p>Note: This artifact is licensed under the GNU General Public License v3.0 and can only be used for academic purposes. The Gillian platform itself is not licensed under the same license.</p>

},
keywords = {bounded verification, bug-finding, C, JavaScript, parametric semantics, Symbolic execution}
}

@inproceedings{10.1145/3385412.3386034,
author = {Abdulla, Parosh Aziz and Atig, Mohamed Faouzi and Chen, Yu-Fang and Diep, Bui Phi and Dolby, Julian and Jank\r{u}, Petr and Lin, Hsin-Hung and Hol\'{\i}k, Luk\'{a}\v{s} and Wu, Wei-Cheng},
title = {Efficient Handling of String-Number Conversion},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386034},
doi = {10.1145/3385412.3386034},
abstract = {String-number conversion is an important class of constraints needed for the symbolic execution of string-manipulating programs. In particular solving string constraints with string-number conversion is necessary for the analysis of scripting languages such as JavaScript and Python, where string-number conversion is a part of the definition of the core semantics of these languages. However, solving this type of constraint is very challenging for the state-of-the-art solvers. We propose in this paper an approach that can efficiently support both string-number conversion and other common types of string constraints. Experimental results show that it significantly outperforms other state-of-the-art tools on benchmarks that involves string-number conversion.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {943–957},
numpages = {15},
keywords = {String Solver, Automata, Formal Verification},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3386019,
author = {Giannarakis, Nick and Loehr, Devon and Beckett, Ryan and Walker, David},
title = {NV: An Intermediate Language for Verification of Network Control Planes},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386019},
doi = {10.1145/3385412.3386019},
abstract = {Network misconfiguration has caused a raft of high-profile outages over the past decade, spurring researchers to develop a variety of network analysis and verification tools. Unfortunately, developing and maintaining such tools is an enormous challenge due to the complexity of network configuration languages. Inspired by work on intermediate languages for verification such as Boogie and Why3, we develop NV, an intermediate language for verification of network control planes. NV carefully walks the line between expressiveness and tractability, making it possible to build models for a practical subset of real protocols and their configurations, and also facilitate rapid development of tools that outperform state-of-the-art simulators (seconds vs minutes) and verifiers (often 10x faster). Furthermore, we show that it is possible to develop novel analyses just by writing new NV programs. In particular, we implement a new fault-tolerance analysis that scales to far larger networks than existing tools.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {958–973},
numpages = {16},
keywords = {Control Plane Analysis, Intermediate Verification Language, Network Simulation, Network Verification, Router Configuration Analysis},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395652,
author = {Giannarakis, Nick and Loehr, Devon and Beckett, Ryan and Walker, David},
title = {Software Artifact for NV: An Intermediate Language for Verification of Network Control Planes},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395652},
abstract = {
    <p>The artifact contains a VirtualBox VM image with the source code and the binary for the NV framework (the simulator, SMT verifier, etc.). Also included is a modified version of Batfish that can translate router configurations to NV source files.</p>

},
keywords = {intermediate verification language, network simulation, network verification, static analysis}
}

@inproceedings{10.1145/3385412.3385976,
author = {Subramanian, Kausik and Abhashkumar, Anubhavnidhi and D'Antoni, Loris and Akella, Aditya},
title = {Detecting Network Load Violations for Distributed Control Planes},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385976},
doi = {10.1145/3385412.3385976},
abstract = {One of the major challenges faced by network operators pertains to whether their network can meet input traffic demand, avoid overload, and satisfy service-level agreements. Automatically verifying if no network links are overloaded is complicated---requires modeling frequent network failures, complex routing and load-balancing technologies, and evolving traffic requirements. We present QARC, a distributed control plane abstraction that can automatically verify whether a control plane may cause link-load violations under failures. QARC is fully automatic and can help operators program networks that are more resilient to failures and upgrade the network to avoid violations. We apply QARC to real datacenter and ISP networks and find interesting cases of load violations. QARC can detect violations in under an hour.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {974–988},
numpages = {15},
keywords = {Quantitative Verification, Fault Tolerance, Abstract Representation, Routing Protocols},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3386033,
author = {Kim, Hongjune and Zeng, Jianping and Liu, Qingrui and Abdel-Majeed, Mohammad and Lee, Jaejin and Jung, Changhee},
title = {Compiler-Directed Soft Error Resilience for Lightweight GPU Register File Protection},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386033},
doi = {10.1145/3385412.3386033},
abstract = {This paper presents Penny, a compiler-directed resilience scheme for protecting GPU register files (RF) against soft errors. Penny replaces the conventional error correction code (ECC) based RF protection by using less expensive error detection code (EDC) along with idempotence based recovery. Compared to the ECC protection, Penny can achieve either the same level of RF resilience yet with significantly lower hardware costs or stronger resilience using the same ECC due to its ability to detect multi-bit errors when it is used solely for detection. In particular, to address the lack of store buffers in GPUs, which causes both checkpoint storage overwriting and the high cost of checkpointing stores, Penny provides several compiler optimizations such as storage coloring and checkpoint pruning. Across 25 benchmarks, Penny causes only ≈3% run-time overhead on average.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {989–1004},
numpages = {16},
keywords = {GPU, Resilience, ECC},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3385998,
author = {Maeng, Kiwan and Lucia, Brandon},
title = {Adaptive Low-Overhead Scheduling for Periodic and Reactive Intermittent Execution},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385998},
doi = {10.1145/3385412.3385998},
abstract = {Batteryless energy-harvesting devices eliminate the need in batteries for deployed sensor systems, enabling longer lifetime and easier maintenance. However, such devices cannot support an event-driven execution model (e.g., periodic or reactive execution), restricting the use cases and hampering real-world deployment. Without knowing exactly how much energy can be harvested in the future, robustly scheduling periodic and reactive workloads is challenging. We introduce CatNap, an event-driven energy-harvesting system with a new programming model that asks the programmer to express a subset of the code that is time-critical. CatNap isolates and reserves energy for the time-critical code, reliably executing it on schedule while deferring execution of the rest of the code. CatNap degrades execution quality when a decrease in the incoming power renders it impossible to maintain its schedule. Our evaluation on a real energy-harvesting setup shows that CatNap works well with end-to-end, real-world deployment settings. CatNap reliably runs periodic events when a prior system misses the deadline by 7.3x and supports reactive applications with a 100% success rate when a prior work shows less than a 2% success rate.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1005–1021},
numpages = {17},
keywords = {energy-harvesting, intermittent computing},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3386032,
author = {Herman, Grzegorz},
title = {Faster General Parsing through Context-Free Memoization},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386032},
doi = {10.1145/3385412.3386032},
abstract = {We present a novel parsing algorithm for all context-free languages. The algorithm features a clean mathematical formulation: parsing is expressed as a series of standard operations on regular languages and relations. Parsing complexity w.r.t. input length matches the state of the art: it is worst-case cubic, quadratic for unambiguous grammars, and linear for LR-regular grammars. What distinguishes our approach is that parsing can be implemented using only immutable, acyclic data structures. We also propose a parsing optimization technique called context-free memoization. It allows handling an overwhelming majority of input symbols using a simple stack and a lookup table, similarly to the operation of a deterministic LR(1) parser. This allows our proof-of-concept implementation to outperform the best current implementations of common generalized parsing algorithms (Earley, GLR, and GLL). Tested on a large Java source corpus, parsing is 3–5 times faster, while recognition—35 times faster.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1022–1035},
numpages = {14},
keywords = {context-free, GLR, generalized parsing, GLL, memoization, Earley},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3385992,
author = {Edelmann, Romain and Hamza, Jad and Kun\v{c}ak, Viktor},
title = {Zippy LL(1) Parsing with Derivatives},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385992},
doi = {10.1145/3385412.3385992},
abstract = {In this paper, we present an efficient, functional, and formally verified parsing algorithm for LL(1) context-free expressions based on the concept of derivatives of formal languages. Parsing with derivatives is an elegant parsing technique, which, in the general case, suffers from cubic worst-case time complexity and slow performance in practice. We specialise the parsing with derivatives algorithm to LL(1) context-free expressions, where alternatives can be chosen given a single token of lookahead. We formalise the notion of LL(1) expressions and show how to efficiently check the LL(1) property. Next, we present a novel linear-time parsing with derivatives algorithm for LL(1) expressions operating on a zipper-inspired data structure. We prove the algorithm correct in Coq and present an implementation as a part of Scallion, a parser combinators framework in Scala with enumeration and pretty printing capabilities.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1036–1051},
numpages = {16},
keywords = {Zipper, Derivatives, Parsing, LL(1), Formal proof},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3386020,
author = {Li, Yuanbo and Ding, Shuo and Zhang, Qirun and Italiano, Davide},
title = {Debug Information Validation for Optimized Code},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386020},
doi = {10.1145/3385412.3386020},
abstract = {Almost all modern production software is compiled with optimization. Debugging optimized code is a desirable functionality. For example, developers usually perform post-mortem debugging on the coredumps produced by software crashes. Designing reliable debugging techniques for optimized code has been well-studied in the past. However, little is known about the correctness of the debug information generated by optimizing compilers when debugging optimized code. Optimizing compilers emit debug information (e.g., DWARF information) to support source code debuggers. Wrong debug information causes debuggers to either crash or to display wrong variable values. Existing debugger validation techniques only focus on testing the interactive aspect of debuggers for dynamic languages (i.e., with unoptimized code). Validating debug information for optimized code raises some unique challenges: (1) many breakpoints cannot be reached by debuggers due to code optimization; and (2) inspecting some arbitrary variables such as uninitialized variables introduces undefined behaviors. This paper presents the first generic framework for systematically testing debug information with optimized code. We introduce a novel concept called actionable program. An actionable program P⟨ s, v⟩ contains a program location s and a variable v to inspect. Our key insight is that in both the unoptimized program P⟨ s,v⟩ and the optimized program P⟨ s,v⟩′, debuggers should be able to stop at the program location s and inspect the value of the variable v without any undefined behaviors. Our framework generates actionable programs and does systematic testing by comparing the debugger output of P⟨ s, v⟩′ and the actual value of v at line s in P⟨ s, v⟩. We have applied our framework to two mainstream optimizing C compilers (i.e., GCC and LLVM). Our framework has led to 47 confirmed bug reports, 11 of which have already been fixed. Moreover, in three days, our technique has found 2 confirmed bugs in the Rust compiler. The results have demonstrated the effectiveness and generality of our framework.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1052–1065},
numpages = {14},
keywords = {Debug Information, Optimizing Compilers},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3386001,
author = {Premtoon, Varot and Koppel, James and Solar-Lezama, Armando},
title = {Semantic Code Search via Equational Reasoning},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386001},
doi = {10.1145/3385412.3386001},
abstract = {We present a new approach to semantic code search based on equational reasoning, and the Yogo tool implementing this approach. Our approach works by considering not only the dataflow graph of a function, but also the dataflow graphs of all equivalent functions reachable via a set of rewrite rules. In doing so, it can recognize an operation even if it uses alternate APIs, is in a different but mathematically-equivalent form, is split apart with temporary variables, or is interleaved with other code. Furthermore, it can recognize when code is an instance of some higher-level concept such as iterating through a file. Because of this, from a single query, Yogo can find equivalent code in multiple languages. Our evaluation further shows the utility of Yogo beyond code search: encoding a buggy pattern as a Yogo query, we found a bug in Oracle’s Graal compiler which had been missed by a hand-written static analyzer designed for that exact kind of bug. Yogo is built on the Cubix multi-language infrastructure, and currently supports Java and Python.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1066–1082},
numpages = {17},
keywords = {code search, equational reasoning},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3743160,
author = {Premtoon, Varot and Koppel, James and Solar-Lezama, Armando},
title = {Yogo},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3743160},
abstract = {
    <p>Docker container with an executable copy of Yogo, and source code for the Haskell portion (frontend)</p>

},
keywords = {code search, equational reasoning}
}

@inproceedings{10.1145/3385412.3385975,
author = {Drews, Samuel and Albarghouthi, Aws and D'Antoni, Loris},
title = {Proving Data-Poisoning Robustness in Decision Trees},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385975},
doi = {10.1145/3385412.3385975},
abstract = {Machine learning models are brittle, and small changes in the training data can result in different predictions. We study the problem of proving that a prediction is robust to data poisoning, where an attacker can inject a number of malicious elements into the training set to influence the learned model. We target decision-tree models, a popular and simple class of machine learning models that underlies many complex learning techniques. We present a sound verification technique based on abstract interpretation and implement it in a tool called Antidote. Antidote abstractly trains decision trees for an intractably large space of possible poisoned datasets. Due to the soundness of our abstraction, Antidote can produce proofs that, for a given input, the corresponding prediction would not have changed had the training set been tampered with or not. We demonstrate the effectiveness of Antidote on a number of popular datasets.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1083–1097},
numpages = {15},
keywords = {Robustness, Decision Trees, Poisoning, Abstract Interpretation, Adversarial Machine Learning},
location = {London, UK},
series = {PLDI 2020}
}

@inproceedings{10.1145/3385412.3386015,
author = {Usman, Muhammad and Wang, Wenxi and Vasic, Marko and Wang, Kaiyuan and Vikalo, Haris and Khurshid, Sarfraz},
title = {A Study of the Learnability of Relational Properties: Model Counting Meets Machine Learning (MCML)},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386015},
doi = {10.1145/3385412.3386015},
abstract = {This paper introduces the MCML approach for empirically studying the learnability of relational properties that can be expressed in the well-known software design language Alloy. A key novelty of MCML is quantification of the performance of and semantic differences among trained machine learning (ML) models, specifically decision trees, with respect to entire (bounded) input spaces, and not just for given training and test datasets (as is the common practice). MCML reduces the quantification problems to the classic complexity theory problem of model counting, and employs state-of-the-art model counters. The results show that relatively simple ML models can achieve surprisingly high performance (accuracy and F1-score) when evaluated in the common setting of using training and test datasets -- even when the training dataset is much smaller than the test dataset -- indicating the seeming simplicity of learning relational properties. However, MCML metrics based on model counting show that the performance can degrade substantially when tested against the entire (bounded) input space, indicating the high complexity of precisely learning these properties, and the usefulness of model counting in quantifying the true performance.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1098–1111},
numpages = {14},
keywords = {SAT solving, machine learning, Relational properties, ApproxMC, model counting, ProjMC, Alloy},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395647,
author = {Usman, Muhammad and Wang, Wenxi and Vasic, Marko and Wang, Kaiyuan and Vikalo, Haris and Khurshid, Sarfraz},
title = {Replication Package for Article: A Study of the Learnability of Relational Properties: Model Counting Meets Machine Learning (MCML) - PLDI 2020},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395647},
abstract = {
    <p>The artifact contains datasets and code which were used to compute results for this paper (A Study of the Learnability of Relational Properties: Model Counting Meets Machine Learning (MCML) - PLDI 2020). Detailed instructions can be found in README.txt file.</p>

},
keywords = {Alloy, ApproxMC, machine learning, model counting, ProjMC, Relational Properties, SAT solving}
}

@inproceedings{10.1145/3385412.3386016,
author = {He, Jingxuan and Singh, Gagandeep and P\"{u}schel, Markus and Vechev, Martin},
title = {Learning Fast and Precise Numerical Analysis},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386016},
doi = {10.1145/3385412.3386016},
abstract = {Numerical abstract domains are a key component of modern static analyzers. Despite recent advances, precise analysis with highly expressive domains remains too costly for many real-world programs. To address this challenge, we introduce a new data-driven method, called LAIT, that produces a faster and more scalable numerical analysis without significant loss of precision. Our approach is based on the key insight that sequences of abstract elements produced by the analyzer contain redundancy which can be exploited to increase performance without compromising precision significantly. Concretely, we present an iterative learning algorithm that learns a neural policy that identifies and removes redundant constraints at various points in the sequence. We believe that our method is generic and can be applied to various numerical domains.  We instantiate LAIT for the widely used Polyhedra and Octagon domains. Our evaluation of LAIT on a range of real-world applications with both domains shows that while the approach is designed to be generic, it is orders of magnitude faster on the most costly benchmarks than a state-of-the-art numerical library while maintaining close-to-original analysis precision. Further, LAIT outperforms hand-crafted heuristics and a domain-specific learning approach in terms of both precision and speed.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1112–1127},
numpages = {16},
keywords = {Machine learning, Performance optimization, Abstract interpretation, Numerical domains},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395648,
author = {He, Jingxuan and Singh, Gagandeep and P\"{u}schel, Markus and Vechev, Martin},
title = {Reproduction Package for Article: Learning Fast and Precise Numerical Analysis},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395648},
abstract = {
    <p>This material is the artifact for paper "Learning Fast and Precise Numerical Analysis" at PLDI 2020. It contains raw data for main experiments, benchmarks, full source code, and scripts for reproducing main experiments.</p>

},
keywords = {Abstract interpretation, Machine learning, Numerical domains, Performance optimization.}
}

@inproceedings{10.1145/3385412.3385979,
author = {Hu, Qinheping and Cyphert, John and D'Antoni, Loris and Reps, Thomas},
title = {Exact and Approximate Methods for Proving Unrealizability of Syntax-Guided Synthesis Problems},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3385979},
doi = {10.1145/3385412.3385979},
abstract = {We consider the problem of automatically establishing that a given syntax-guided-synthesis (SyGuS) problem is unrealizable (i.e., has no solution). We formulate the problem of proving that a SyGuS problem is unrealizable over a finite set of examples as one of solving a set of equations: the solution yields an overapproximation of the set of possible outputs that any term in the search space can produce on the given examples. If none of the possible outputs agrees with all of the examples, our technique has proven that the given SyGuS problem is unrealizable. We then present an algorithm for exactly solving the set of equations that result from SyGuS problems over linear integer arithmetic (LIA) and LIA with conditionals (CLIA), thereby showing that LIA and CLIA SyGuS problems over finitely many examples are decidable. We implement the proposed technique and algorithms in a tool called Nay. Nay can prove unrealizability for 70/132 existing SyGuS benchmarks, with running times comparable to those of the state-of-the-art tool Nope. Moreover, Nay can solve 11 benchmarks that Nope cannot solve.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1128–1142},
numpages = {15},
keywords = {Unrealizability, Grammar Flow Analysis, Syntax-Guided Synthesis, Program Synthesis},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.1145/3395631,
author = {Hu, Qinheping and Cyphert, John and D'Antoni, Loris and Reps, Thomas},
title = {Artifact for Article: Exact and Approximate Methods for Proving Unrealizability of Syntax-Guided Synthesis Problems},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3395631},
abstract = {
    <p>This artifact contains Nay, a synthesizer which can prove unrealizability of unrealizable SyGuS problems. For detail can be found in this paper:https://arxiv.org/abs/2004.00878</p>

},
keywords = {program synthesis, syntax guided synthesis, unrealizability}
}

@inproceedings{10.1145/3385412.3386025,
author = {Ji, Ruyi and Liang, Jingjing and Xiong, Yingfei and Zhang, Lu and Hu, Zhenjiang},
title = {Question Selection for Interactive Program Synthesis},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386025},
doi = {10.1145/3385412.3386025},
abstract = {Interactive program synthesis aims to solve the ambiguity in specifications, and selecting the proper question to minimize the rounds of interactions is critical to the performance of interactive program synthesis. In this paper we address this question selection problem and propose two algorithms. SampleSy approximates a state-of-the-art strategy proposed for optimal decision tree and has a short response time to enable interaction. EpsSy further reduces the rounds of interactions by approximating SampleSy with a bounded error rate. To implement the two algorithms, we further propose VSampler, an approach to sampling programs from a probabilistic context-free grammar based on version space algebra. The evaluation shows the effectiveness of both algorithms.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1143–1158},
numpages = {16},
keywords = {Program Synthesis, Interaction},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3750961,
author = {Ji, Ruyi and Liang, Jingjing and Xiong, Yingfei and Zhang, Lu and Hu, Zhenjiang},
title = {Artifact for Paper "Question Selection for Interactive Program Synthesis"},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3750961},
abstract = {
    <p>This artifact is comprised of the appendix, source code and experiment scripts of paper "Question Selection for Interactive Program Synthesis". Readers can use them to reproduce the experiment results listed in our paper.</p>

},
keywords = {Interaction, Program Synthesis}
}

@inproceedings{10.1145/3385412.3386027,
author = {Huang, Kangjing and Qiu, Xiaokang and Shen, Peiyuan and Wang, Yanjun},
title = {Reconciling Enumerative and Deductive Program Synthesis},
year = {2020},
isbn = {9781450376136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385412.3386027},
doi = {10.1145/3385412.3386027},
abstract = {Syntax-guided synthesis (SyGuS) aims to find a program satisfying semantic specification as well as user-provided structural hypotheses. There are two main synthesis approaches: enumerative synthesis, which repeatedly enumerates possible candidate programs and checks their correctness, and deductive synthesis, which leverages a symbolic procedure to construct implementations from specifications. Neither approach is strictly better than the other: automated deductive synthesis is usually very efficient but only works for special grammars or applications; enumerative synthesis is very generally applicable but limited in scalability.  In this paper, we propose a cooperative synthesis technique for SyGuS problems with the conditional linear integer arithmetic (CLIA) background theory, as a novel integration of the two approaches, combining the best of the two worlds. The technique exploits several novel divide-and-conquer strategies to split a large synthesis problem to smaller subproblems. The subproblems are solved separately and their solutions are combined to form a final solution. The technique integrates two synthesis engines: a pure deductive component that can efficiently solve some problems, and a height-based enumeration algorithm that can handle arbitrary grammar. We implemented the cooperative synthesis technique, and evaluated it on a wide range of benchmarks. Experiments showed that our technique can solve many challenging synthesis problems not possible before, and tends to be more scalable than state-of-the-art synthesis algorithms.},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {1159–1174},
numpages = {16},
keywords = {syntax-guided synthesis, deductive synthesis, divide-and-conquer, enumerative synthesis},
location = {London, UK},
series = {PLDI 2020}
}

@software{10.5281/zenodo.3753963,
author = {Huang, Kangjing and Qiu, Xiaokang and Shen, Peiyuan and Wang, Yanjun},
title = {DryadSynth: Release as PLDI 2020 Artifact: Reconciling Enumerative and Deductive Program Synthesis},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.5281/zenodo.3753963},
abstract = {
    <p>DryadSynth: A syntax-guided synthesizer</p>

},
keywords = {deductive synthesis, divide-and-conquer, enumerative synthesis, syntax-guided synthesis}
}

