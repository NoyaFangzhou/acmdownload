@inproceedings{10.1145/3307650.3322207,
author = {Bhatia, Eshan and Chacon, Gino and Pugsley, Seth and Teran, Elvira and Gratz, Paul V. and Jim\'{e}nez, Daniel A.},
title = {Perceptron-Based Prefetch Filtering},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322207},
doi = {10.1145/3307650.3322207},
abstract = {Hardware prefetching is an effective technique for hiding cache miss latencies in modern processor designs. Prefetcher performance can be characterized by two main metrics that are generally at odds with one another: coverage, the fraction of baseline cache misses which the prefetcher brings into the cache; and accuracy, the fraction of prefetches which are ultimately used. An overly aggressive prefetcher may improve coverage at the cost of reduced accuracy. Thus, performance may be harmed by this over-aggressiveness because many resources are wasted, including cache capacity and bandwidth. An ideal prefetcher would have both high coverage and accuracy.In this paper, we introduce Perceptron-based Prefetch Filtering (PPF) as a way to increase the coverage of the prefetches generated by an underlying prefetcher without negatively impacting accuracy. PPF enables more aggressive tuning of the underlying prefetcher, leading to increased coverage by filtering out the growing numbers of inaccurate prefetches such an aggressive tuning implies. We also explore a range of features to use to train PPF's perceptron layer to identify inaccurate prefetches. PPF improves performance on a memory-intensive subset of the SPEC CPU 2017 benchmarks by 3.78% for a single-core configuration, and by 11.4% for a 4-core configuration, compared to the underlying prefetcher alone.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {1–13},
numpages = {13},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322267,
author = {Tarsa, Stephen J. and Chowdhury, Rangeen Basu Roy and Sebot, Julien and Chinya, Gautham and Gaur, Jayesh and Sankaranarayanan, Karthik and Lin, Chit-Kwan and Chappell, Robert and Singhal, Ronak and Wang, Hong},
title = {Post-Silicon CPU Adaptation Made Practical Using Machine Learning},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322267},
doi = {10.1145/3307650.3322267},
abstract = {Processors that adapt architecture to workloads at runtime promise compelling performance per watt (PPW) gains, offering one way to mitigate diminishing returns from pipeline scaling. State-of-the-art adaptive CPUs deploy machine learning (ML) models on-chip to optimize hardware by recognizing workload patterns in event counter data. However, despite breakthrough PPW gains, such designs are not yet widely adopted due to the potential for systematic adaptation errors in the field.This paper presents an adaptive CPU based on Intel SkyLake that (1) closes the loop to deployment, and (2) provides a novel mechanism for post-silicon customization. Our CPU performs predictive cluster gating, dynamically setting the issue width of a clustered architecture while clock-gating unused resources. Gating decisions are driven by ML adaptation models that execute on an existing microcontroller, minimizing design complexity and allowing performance characteristics to be adjusted with the ease of a firmware update. Crucially, we show that although adaptation models can suffer from statistical blindspots that risk degrading performance on new workloads, these can be reduced to minimal impact with careful design and training.Our adaptive CPU improves PPW by 31.4% over a comparable non-adaptive CPU on SPEC2017, and exhibits two orders of magnitude fewer Service Level Agreement (SLA) violations than the state-of-the-art. We show how to optimize PPW using models trained to different SLAs or to specific applications, e.g. to improve datacenter hardware in situ. The resulting CPU meets real world deployment criteria for the first time and provides a new means to tailor hardware to individual customers, even as their needs change.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {14–26},
numpages = {13},
keywords = {adaptive hardware, machine learning, clustered architectures, runtime optimization},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322217,
author = {Garza, Elba and Mirbagher-Ajorpaz, Samira and Khan, Tahsin Ahmad and Jim\'{e}nez, Daniel A.},
title = {Bit-Level Perceptron Prediction for Indirect Branches},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322217},
doi = {10.1145/3307650.3322217},
abstract = {Modern software uses indirect branches for various purposes including, but not limited to, virtual method dispatch and implementation of switch statements. Because an indirect branch's target address cannot be determined prior to execution, high-performance processors depend on highly-accurate indirect branch prediction techniques to mitigate control hazards.This paper proposes a new indirect branch prediction scheme that predicts target addresses at the bit level. Using a series of perceptron-based predictors, our predictor predicts individual branch target address bits based on correlations within branch history. Our evaluations show this new branch target predictor is competitive with state-of-the-art branch target predictors at an equivalent hardware budget. For instance, over a set of workloads including SPEC and mobile applications, our predictor achieves a misprediction rate of 0.183 mispredictions per 1000 instructions, compared with 0.193 for the state-of-the-art ITTAGE predictor and 0.29 for a VPC-based indirect predictor.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {27–38},
numpages = {12},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3326633,
author = {Ding, Yi and Mishra, Nikita and Hoffmann, Henry},
title = {Generative and Multi-Phase Learning for Computer Systems Optimization},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3326633},
doi = {10.1145/3307650.3326633},
abstract = {Machine learning and artificial intelligence are invaluable for computer systems optimization: as computer systems expose more resources for management, ML/AI is necessary for modeling these resources' complex interactions. The standard way to incorporate ML/AI into a computer system is to first train a learner to accurately predict the system's behavior as a function of resource usage---e.g., to predict energy efficiency as a function of core usage---and then deploy the learned model as part of a system---e.g., a scheduler. In this paper, we show that (1) continued improvement of learning accuracy may not improve the systems result, but (2) incorporating knowledge of the systems problem into the learning process improves the systems results even though it may not improve overall accuracy. Specifically, we learn application performance and power as a function of resource usage with the systems goal of meeting latency constraints with minimal energy. We propose a novel generative model which improves learning accuracy given scarce data, and we propose a multi-phase sampling technique, which incorporates knowledge of the systems problem. Our results are both positive and negative. The generative model improves accuracy, even for state-of-the-art learning systems, but negatively impacts energy. Multi-phase sampling reduces energy consumption compared to the state-of-the-art, but does not improve accuracy. These results imply that learning for systems optimization may have reached a point of diminishing returns where accuracy improvements have little effect on the systems outcome. Thus we advocate that future work on learning for systems should de-emphasize accuracy and instead incorporate the system problem's structure into the learner.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {39–52},
numpages = {14},
keywords = {resource allocation, energy, heterogeneous architectures, machine learning, real-time systems},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322247,
author = {Xie, Chenhao and Xin, Fu and Chen, Mingsong and Song, Shuaiwen Leon},
title = {OO-VR: NUMA Friendly <u>O</u>Bject-<u>O</u>Riented <u>VR</u> Rendering Framework for Future NUMA-Based Multi-GPU Systems},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322247},
doi = {10.1145/3307650.3322247},
abstract = {With the strong computation capability, NUMA-based multi-GPU system is a promising candidate to provide sustainable and scalable performance for Virtual Reality (VR) applications and deliver the excellent user experience. However, the entire multi-GPU system is viewed as a single GPU under the single programming model which greatly ignores the data locality among VR rendering tasks during the workload distribution, leading to tremendous remote memory accesses among GPU models (GPMs). The limited inter-GPM link bandwidth (e.g., 64GB/s for NVlink) becomes the major obstacle when executing VR applications in the multi-GPU system. By conducting comprehensive characterizations on different kinds of parallel rendering frameworks, we observe that distributing the rendering object along with its required data per GPM can reduce the inter-GPM memory accesses. However, this object-level rendering still faces two major challenges in NUMA-based multi-GPU system: (1) the large data locality between the left and right views of the same object and the data sharing among different objects and (2) the unbalanced workloads induced by the software-level distribution and composition mechanisms.To tackle these challenges, we propose object-oriented VR rendering framework (OO-VR) that conducts the software and hardware co-optimization to provide a NUMA friendly solution for VR multi-view rendering in NUMA-based multi-GPU systems. We first propose an object-oriented VR programming model to exploit the data sharing between two views of the same object and group objects into batches based on their texture sharing levels. Then, we design an object aware runtime batch distribution engine and distributed hardware composition unit to achieve the balanced workloads among GPMs and further improve the performance of VR rendering. Finally, evaluations on our VR featured simulator show that OO-VR provides 1.58x overall performance improvement and 76% inter-GPM memory traffic reduction over the state-of-the-art multi-GPU systems. In addition, OO-VR provides NUMA friendly performance scalability for the future larger multi-GPU scenarios with ever increasing asymmetric bandwidth between local and remote memory.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {53–65},
numpages = {13},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322248,
author = {Feng, Yu and Zhu, Yuhao},
title = {PES: Proactive Event Scheduling for Responsive and Energy-Efficient Mobile Web Computing},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322248},
doi = {10.1145/3307650.3322248},
abstract = {Web applications are gradually shifting toward resource-constrained mobile devices. As a result, the Web runtime system must simultaneously address two challenges: responsiveness and energy-efficiency. Conventional Web runtime systems fall short due to their reactive nature: they react to a user event only after it is triggered. The reactive strategy leads to local optimizations that schedule event executions one at a time, missing global optimization opportunities.This paper proposes Proactive Event Scheduling (PES). The key idea of PES is to proactively anticipate future events and thereby globally coordinate scheduling decisions across events. Specifically, PES predicts events that are likely to happen in the near future using a combination of statistical inference and application code analysis. PES then speculatively executes future events ahead of time in a way that satisfies the QoS constraints of all the events while minimizing the global energy consumption. Fundamentally, PES unlocks more optimization opportunities by enlarging the scheduling window, which enables coordination across both outstanding events and predicted events. Hardware measurements show that PES reduces the QoS violation and energy consumption by 61.2% and 26.5%, respectively, over the Android's default Interactive CPU governor. It also reduces the QoS violation and energy consumption by 63.1% and 17.9%, respectively, compared to EBS, a state-of-the-art reactive scheduler.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {66–78},
numpages = {13},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322260,
author = {Chen, Huixiang and Song, Mingcong and Zhao, Jiechen and Dai, Yuting and Li, Tao},
title = {<i>Retracted on January 26, 2021</i>: 3D-Based Video Recognition Acceleration by Leveraging Temporal Locality},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322260},
doi = {10.1145/3307650.3322260},
abstract = {This Work has been Retracted by ACM because one or more of the authors of this Work were proven to have known or believed that the Work contained incorrect and/or falsified results prior to publication of the Work and one or more of the authors of this Work violated the anonymity and independence of the review process for their paper "3D-based video recognition acceleration by leveraging temporal locality" to the Proceedings of the 46th International Symposium on Computer Architecture (ISCA '19). Association for Computing Machinery, New York, NY, USA, 79-90.https://dl.acm.org/doi/10.1145/3307650.3322260 },
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {79–90},
numpages = {12},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322264,
author = {Leng, Yue and Chen, Chi-Chun and Sun, Qiuyue and Huang, Jian and Zhu, Yuhao},
title = {Energy-Efficient Video Processing for Virtual Reality},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322264},
doi = {10.1145/3307650.3322264},
abstract = {Virtual reality (VR) has huge potential to enable radically new applications, behind which spherical panoramic video processing is one of the backbone techniques. However, current VR systems reuse the techniques designed for processing conventional planar videos, resulting in significant energy inefficiencies. Our characterizations show that operations that are unique to processing 360° VR content constitute 40% of the total processing energy consumption.We present EVR, an end-to-end system for energy-efficient VR video processing. EVR recognizes that the major contributor to the VR tax is the projective transformation (PT) operations. EVR mitigates the overhead of PT through two key techniques: semantic-aware streaming (SAS) on the server and hardware-accelerated rendering (HAR) on the client device. EVR uses SAS to reduce the chances of executing projective transformation on VR devices by pre-rendering 360° frames in the cloud. Different from conventional pre-rendering techniques, SAS exploits the key semantic information inherent in VR content that is previously ignored. Complementary to SAS, HAR mitigates the energy overhead of on-device rendering through a new hardware accelerator that is specialized for projective transformation. We implement an EVR prototype on an Amazon AWS server instance and a NVIDA Jetson TX2 board combined with a Xilinx Zynq-7000 FPGA. Real system measurements show that EVR reduces the energy of VR rendering by up to 58%, which translates to up to 42% energy saving for VR devices.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {91–103},
numpages = {13},
keywords = {pre-rendering, hardware accelerator, projective transformation, energy efficiency, virtual reality, video processing},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322250,
author = {Awad, Amro and Ye, Mao and Solihin, Yan and Njilla, Laurent and Zubair, Kazi Abu},
title = {Triad-NVM: Persistency for Integrity-Protected and Encrypted Non-Volatile Memories},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322250},
doi = {10.1145/3307650.3322250},
abstract = {Non-Volatile Memory is here and provides an attractive fabric for main memory. Unlike DRAM, non-volatile main memory (NVMM) retains data after power loss. This allows memory to host data persistently across crashes and reboots, but opens up opportunities for attackers to snoop and/or tamper with data between boot episodes. While memory encryption and integrity verification have been well studied for DRAM systems, new challenges surface for NVMM if we want to simultaneously preserve security guarantees, data recovery across crashes/reboots, good persistence performance, and fast recovery.In this paper, we explore persistency of data with all security metadata (counters, MACs, and Merkle Tree) to achieve secure persistency. We show that to ensure security guarantees, message authentication code (MAC) and Bonsai Merkle Tree (BMT) need to be maintained, in addition to counters, and they provide the majority of persistency overheads. We analyze the requirements for achieving secure persistency for both persistent and non-persistent memory regions. We found that the non-volatility nature of memory may trigger integrity verification failure at reboot, hence we propose a separate mechanism to support non-persistent memory region. Fourth, we propose designs to make recovery fast. Our evaluation shows that the proposed design, Triad-NVM, can improve the throughput by an average of 2\texttimes{} relative to strict persistence. Moreover, Triad-NVM can achieve orders of magnitude faster recovery time compared to systems without security metadata persistence.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {104–115},
numpages = {12},
keywords = {persistence, persistent security, security, non-volatile memories},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322275,
author = {Matam, Kiran Kumar and Koo, Gunjae and Zha, Haipeng and Tseng, Hung-Wei and Annavaram, Murali},
title = {GraphSSD: Graph Semantics Aware SSD},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322275},
doi = {10.1145/3307650.3322275},
abstract = {Graph analytics play a key role in a number of applications such as social networks, drug discovery, and recommendation systems. Given the large size of graphs that may exceed the capacity of the main memory, application performance is bounded by storage access time. Out-of-core graph processing frameworks try to tackle this storage access bottleneck through techniques such as graph sharding, and sub-graph partitioning. Even with these techniques, the need to access data across different graph shards or sub-graphs causes storage systems to become a significant performance hurdle. In this paper, we propose a graph semantic aware solid state drive (SSD) framework, called GraphSSD, which is a full system solution for storing, accessing, and performing graph analytics on SSDs. Rather than treating storage as a collection of blocks, GraphSSD considers graph structure while deciding on graph layout, access, and update mechanisms. GraphSSD replaces the conventional logical to physical page mapping mechanism in an SSD with a novel vertex-to-page mapping scheme and exploits the detailed knowledge of the flash properties to minimize page accesses. GraphSSD also supports efficient graph updates (vertex and edge modifications) by minimizing unnecessary page movement overheads. GraphSSD provides a simple programming interface that enables application developers to access graphs as native data in their applications, thereby simplifying the code development. It also augments the NVMe (non-volatile memory express) interface with a minimal set of changes to map the graph access APIs to appropriate storage access mechanisms.Our evaluation results show that the GraphSSD framework improves the performance by up to 1.85 \texttimes{} for the basic graph data fetch functions and on average 1.40\texttimes{}, 1.42\texttimes{}, 1.60\texttimes{}, 1.56\texttimes{}, and 1.29\texttimes{} for the widely used breadth-first search, connected components, random-walk, maximal independent set, and page rank applications, respectively.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {116–128},
numpages = {13},
keywords = {graphs, flash storage, SSD},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322231,
author = {Hassan, Hasan and Patel, Minesh and Kim, Jeremie S. and Yaglikci, A. Giray and Vijaykumar, Nandita and Ghiasi, Nika Mansouri and Ghose, Saugata and Mutlu, Onur},
title = {CROW: A Low-Cost Substrate for Improving DRAM Performance, Energy Efficiency, and Reliability},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322231},
doi = {10.1145/3307650.3322231},
abstract = {DRAM has been the dominant technology for architecting main memory for decades. Recent trends in multi-core system design and large-dataset applications have amplified the role of DRAM as a critical system bottleneck. We propose Copy-Row DRAM (CROW), a flexible substrate that enables new mechanisms for improving DRAM performance, energy efficiency, and reliability. We use the CROW substrate to implement 1) a low-cost in-DRAM caching mechanism that lowers DRAM activation latency to frequently-accessed rows by 38% and 2) a mechanism that avoids the use of short-retention-time rows to mitigate the performance and energy overhead of DRAM refresh operations. CROW's flexibility allows the implementation of both mechanisms at the same time. Our evaluations show that the two mechanisms synergistically improve system performance by 20.0% and reduce DRAM energy by 22.3% for memory-intensive four-core workloads, while incurring 0.48% extra area overhead in the DRAM chip and 11.3 KiB storage overhead in the memory controller, and consuming 1.6% of DRAM storage capacity, for one particular implementation.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {129–142},
numpages = {14},
keywords = {memory systems, reliability, DRAM, performance, energy, power},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322206,
author = {Liu, Sihang and Seemakhupt, Korakit and Pekhimenko, Gennady and Kolli, Aasheesh and Khan, Samira},
title = {Janus: Optimizing Memory and Storage Support for Non-Volatile Memory Systems},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322206},
doi = {10.1145/3307650.3322206},
abstract = {Non-volatile memory (NVM) technologies can manipulate persistent data directly in memory. Ensuring crash consistency of persistent data enforces that data updates reach all the way to NVM, which puts these write requests on the critical path. Recent literature sought to reduce this performance impact. However, prior works have not fully accounted for all the backend memory operations (BMOs) performed at the memory controller that are necessary to maintain persistent data in NVM. These BMOs include support for encryption, integrity protection, compression, deduplication, etc., necessary to provide security, endurance, and lifetime guarantees. These BMOs significantly increase the NVM write latency and exacerbate the performance degradation caused by the critical write requests. The goal of this work is to minimize the BMO overhead of write requests in an NVM system.The central challenge is to figure out how to optimize these seemingly dependent and monolithic BMOs. Our key insight is to decompose each BMO into a series of sub-operations and then reduce their overall latency through two mechanisms: (i) parallelize sub-operations across BMOs and (ii) pre-execute sub-operations off the critical path as soon as their inputs are ready. We expose a generic software interface that can be used to issue pre-execution requests compatible with common crash-consistency programming models and various BMOs. Based on these ideas, we propose Janus1 - a hardware-software co-design that parallelizes and pre-executes BMOs in an NVM system. We evaluate Janus in an NVM system that integrates encryption, integrity verification, and deduplication and issues pre-execution requests through the proposed software interface, either manually or using an automated compiler pass. Compared to a system that performs these operations serially, Janus achieves 2.35\texttimes{} and 2.00\texttimes{} speedup using manual and automated instrumentation, respectively.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {143–156},
numpages = {14},
keywords = {crash consistency, pre-execution, parallelization, non-volatile memory},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322252,
author = {Zubair, Kazi Abu and Awad, Amro},
title = {Anubis: Ultra-Low Overhead and Recovery Time for Secure Non-Volatile Memories},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322252},
doi = {10.1145/3307650.3322252},
abstract = {Implementing secure Non-Volatile Memories (NVMs) is challenging, mainly due to the necessity to persist security metadata along with data. Unlike conventional secure memories, NVM-equipped systems are expected to recover data after crashes and hence security metadata must be recoverable as well. While prior work explored recovery of encryption counters, fewer efforts have been focused on recovering integrity-protected systems. In particular, how to recover Merkle Tree. We observe two major challenges for this. First, recovering parallelizable integrity trees, e.g., Intel's SGX trees, requires very special handling due to inter-level dependency. Second, the recovery time of practical NVM sizes (terabytes are expected) would take hours. Most data centers, cloud systems, intermittent-power devices and even personal computers, are anticipated to recover almost instantly after power restoration. In fact, this is one of the major promises of NVMs.In this paper, we propose Anubis, a novel hardware-only solution that speeds up recovery time by almost 107 times (from 8 hours to only 0.03 seconds). Moreover, we propose a novel and elegant way to recover inter-level dependent trees, as in Intel's SGX. Most importantly, while ensuring recoverability of one of the most challenging integrity-protection schemes among others, Anubis incurs performance overhead that is only 2% higher than the state-of-the-art scheme, Osiris, which takes hours to recover systems with general Merkle Tree and fails to recover SGX-style trees.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {157–168},
numpages = {12},
keywords = {persistence, security, non-volatile memories, persistent security},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322221,
author = {Gubran, Ayub A. and Aamodt, Tor M.},
title = {Emerald: Graphics Modeling for SoC Systems},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322221},
doi = {10.1145/3307650.3322221},
abstract = {Mobile systems-on-chips (SoCs) have become ubiquitous computing platforms, and, in recent years, they have become increasingly heterogeneous and complex. A typical SoC includes CPUs, graphics processor units (GPUs), image processors, video encoders/decoders, AI engines, digital signal processors (DSPs) and 2D engines among others [33, 70, 71]. One of the most significant SoC units in terms of both off-chip memory bandwidth and SoC die area is the GPU. In this paper, we present Emerald, a simulator that builds on existing tools to provide a unified model for graphics and GPGPU applications. Emerald enables OpenGL (v4.5) and OpenGL ES (v3.2) shaders to run on GPGPU-Sim's timing model and is integrated with gem5 and Android to simulate full SoCs. Emerald thus provides a platform for studying system-level SoC interactions while including the impact of graphics.We present two case studies using Emerald. First, we use Emerald's full-system mode to highlight the importance of system-wide interactions by studying and analyzing memory organization and scheduling schemes for SoC systems. Second, we use Emerald's standalone mode to evaluate a novel mechanism for balancing the graphics shading work assigned to each GPU core.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {169–182},
numpages = {14},
keywords = {graphics, SoC, GPU, simulation},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322222,
author = {Oh, Yunho and Koo, Gunjae and Annavaram, Murali and Ro, Won Woo},
title = {Linebacker: Preserving Victim Cache Lines in Idle Register Files of GPUs},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322222},
doi = {10.1145/3307650.3322222},
abstract = {Modern GPUs suffer from cache contention due to the limited cache size that is shared across tens of concurrently running warps. To increase the per-warp cache size prior techniques proposed warp throttling which limits the number of active warps. Warp throttling leaves several registers to be dynamically unused whenever a warp is throttled. Given the stringent cache size limitation in GPUs this work proposes a new cache management technique named Linebacker (LB) that improves GPU performance by utilizing idle register file space as victim cache space. Whenever a CTA becomes inactive, linebacker backs up the registers of the throttled CTA to the off-chip memory. Then, linebacker utilizes the corresponding register file space as victim cache space. If any load instruction finds data in the victim cache line, the data is directly copied to the destination register through a simple register-register move operation. To further improve the efficiency of victim cache linebacker allocates victim cache space only to a select few load instructions that exhibit high data locality. Through a careful design of victim cache indexing and management scheme linebacker provides 29.0% of speedup compared to the previously proposed warp throttling techniques.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {183–196},
numpages = {14},
keywords = {GPU, register file, CTA scheduling, cache},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322230,
author = {Sun, Yifan and Baruah, Trinayan and Mojumder, Saiful A. and Dong, Shi and Gong, Xiang and Treadway, Shane and Bao, Yuhui and Hance, Spencer and McCardwell, Carter and Zhao, Vincent and Barclay, Harrison and Ziabari, Amir Kavyan and Chen, Zhongliang and Ubal, Rafael and Abell\'{a}n, Jos\'{e} L. and Kim, John and Joshi, Ajay and Kaeli, David},
title = {MGPUSim: Enabling Multi-GPU Performance Modeling and Optimization},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322230},
doi = {10.1145/3307650.3322230},
abstract = {The rapidly growing popularity and scale of data-parallel workloads demand a corresponding increase in raw computational power of Graphics Processing Units (GPUs). As single-GPU platforms struggle to satisfy these performance demands, multi-GPU platforms have started to dominate the high-performance computing world. The advent of such systems raises a number of design challenges, including the GPU microarchitecture, multi-GPU interconnect fabric, runtime libraries, and associated programming models. The research community currently lacks a publicly available and comprehensive multi-GPU simulation framework to evaluate next-generation multi-GPU system designs.In this work, we present MGPUSim, a cycle-accurate, extensively validated, multi-GPU simulator, based on AMD's Graphics Core Next 3 (GCN3) instruction set architecture. MGPUSim comes with in-built support for multi-threaded execution to enable fast, parallelized, and accurate simulation. In terms of performance accuracy, MGPUSim differs by only 5.5% on average from the actual GPU hardware. We also achieve a 3.5\texttimes{} and a 2.5\texttimes{} average speedup running functional emulation and detailed timing simulation, respectively, on a 4-core CPU, while delivering the same accuracy as serial simulation.We illustrate the flexibility and capability of the simulator through two concrete design studies. In the first, we propose the Locality API, an API extension that allows the GPU programmer to both avoid the complexity of multi-GPU programming, while precisely controlling data placement in the multi-GPU memory. In the second design study, we propose <u>P</u>rogressive P<u>a</u>ge <u>S</u>plitting M<u>i</u>gration (PASI), a customized multi-GPU memory management system enabling the hardware to progressively improve data placement. For a discrete 4-GPU system, we observe that the Locality API can speed up the system by 1.6\texttimes{} (geometric mean), and PASI can improve the system performance by 2.6\texttimes{} (geometric mean) across all benchmarks, compared to a unified 4-GPU platform.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {197–209},
numpages = {13},
keywords = {memory management, simulation, multi-GPU systems},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322212,
author = {Pattnaik, Ashutosh and Tang, Xulong and Kayiran, Onur and Jog, Adwait and Mishra, Asit and Kandemir, Mahmut T. and Sivasubramaniam, Anand and Das, Chita R.},
title = {Opportunistic Computing in GPU Architectures},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322212},
doi = {10.1145/3307650.3322212},
abstract = {Data transfer overhead between computing cores and memory hierarchy has been a persistent issue for von Neumann architectures and the problem has only become more challenging with the emergence of manycore systems. A conceptually powerful approach to mitigate this overhead is to bring the computation closer to data, known as Near Data Computing (NDC). Recently, NDC has been investigated in different flavors for CPU-based multicores, while the GPU domain has received little attention. In this paper, we present a novel NDC solution for GPU architectures with the objective of minimizing on-chip data transfer between the computing cores and Last-Level Cache (LLC). To achieve this, we first identify frequently occurring Load-Compute-Store instruction chains in GPU applications. These chains, when offloaded to a compute unit closer to where the data resides, can significantly reduce data movement. We develop two offloading techniques, called LLC-Compute and Omni-Compute. The first technique, LLC-Compute, augments the LLCs with computational hardware for handling the computation offloaded to them. The second technique (Omni-Compute) employs simple bookkeeping hardware to enable GPU cores to compute instructions offloaded by other GPU cores. Our experimental evaluations on nine GPGPU workloads indicate that the LLC-Compute technique provides, on an average, 19% performance improvement (IPC), 11% performance/watt improvement, and 29% reduction in on-chip data movement compared to the baseline GPU design. The Omni-Compute design boosts these benefits to 31%, 16% and 44%, respectively.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {210–223},
numpages = {14},
keywords = {near data computing, computation offloading, GPU},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322224,
author = {Ganguly, Debashis and Zhang, Ziyu and Yang, Jun and Melhem, Rami},
title = {Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322224},
doi = {10.1145/3307650.3322224},
abstract = {Memory capacity in GPGPUs is a major challenge for data-intensive applications with their ever increasing memory requirement. To fit a workload into the limited GPU memory space, a programmer needs to manually divide the workload by tiling the working set and perform user-level data migration. To relieve the programmer from this burden, Unified Virtual Memory (UVM) was developed to support on-demand paging and migration, transparent to the user. It further takes care of the memory over-subscription issue by automatically performing page replacement in an oversubscribed GPU memory situation. However, we found that na\"{\i}ve handling of page faults can cause orders of magnitude slowdown in performance. Moreover, we observed that although prefetching of data from CPU to GPU can hide the page fault latency, the difference among various prefetching mechanisms can lead to drastically different performance results. To this end, we performed extensive experiments on GeForceGTX 1080ti GPUs with PCI-e 3.0 16x to discover that there exists an effective prefetch mechanism to enhance locality in GPU memory. However, as the GPU memory is filled to its capacity, such prefetching mechanism quickly proves to be counterproductive due to locality unaware eviction policy. This necessitates the design of new eviction policies that are aware of the hardware prefetcher semantics. We propose two new programmer-agnostic, locality-aware pre-eviction policies which leverage the mechanics of existing hardware prefetcher and thus incur no additional implementation and performance overhead. We demonstrate that combining the proposed tree-based pre-eviction policy with the hardware prefetcher provides an average of 93% and 18.5% performance speed-up compared to LRU based 4KB and 2MB page replacement strategies, respectively. We further examine the memory access pattern of GPU workloads under consideration to analyze the achieved performance speed-up.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {224–235},
numpages = {12},
keywords = {gpu, page eviction policy, unified virtual memory, hardware prefetcher},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322271,
author = {Yang, Tzu-Hsien and Cheng, Hsiang-Yun and Yang, Chia-Lin and Tseng, I-Ching and Hu, Han-Wen and Chang, Hung-Sheng and Li, Hsiang-Pang},
title = {Sparse ReRAM Engine: Joint Exploration of Activation and Weight Sparsity in Compressed Neural Networks},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322271},
doi = {10.1145/3307650.3322271},
abstract = {Exploiting model sparsity to reduce ineffectual computation is a commonly used approach to achieve energy efficiency for DNN inference accelerators. However, due to the tightly coupled crossbar structure, exploiting sparsity for ReRAM-based NN accelerator is a less explored area. Existing architectural studies on ReRAM-based NN accelerators assume that an entire crossbar array can be activated in a single cycle. However, due to inference accuracy considerations, matrix-vector computation must be conducted in a smaller granularity in practice, called Operation Unit (OU). An OU-based architecture creates a new opportunity to exploit DNN sparsity. In this paper, we propose the first practical Sparse ReRAM Engine that exploits both weight and activation sparsity. Our evaluation shows that the proposed method is effective in eliminating ineffectual computation, and delivers significant performance improvement and energy savings.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {236–249},
numpages = {14},
keywords = {accelerator architecture, ReRAM, neural network, sparsity},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322214,
author = {Jang, Hanhwi and Kim, Joonsung and Jo, Jae-Eon and Lee, Jaewon and Kim, Jangwoo},
title = {MnnFast: A Fast and Scalable System Architecture for Memory-Augmented Neural Networks},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322214},
doi = {10.1145/3307650.3322214},
abstract = {Memory-augmented neural networks are getting more attention from many researchers as they can make an inference with the previous history stored in memory. Especially, among these memory-augmented neural networks, memory networks are known for their huge reasoning power and capability to learn from a large number of inputs rather than other networks. As the size of input datasets rapidly grows, the necessity of large-scale memory networks continuously arises. Such large-scale memory networks provide excellent reasoning power; however, the current computer infrastructure cannot achieve scalable performance due to its limited system architecture.In this paper, we propose MnnFast, a novel system architecture for large-scale memory networks to achieve fast and scalable reasoning performance. We identify the performance problems of the current architecture by conducting extensive performance bottleneck analysis. Our in-depth analysis indicates that the current architecture suffers from three major performance problems: high memory bandwidth consumption, heavy computation, and cache contention. To overcome these performance problems, we propose three novel optimizations. First, to reduce the memory bandwidth consumption, we propose a new column-based algorithm with streaming which minimizes the size of data spills and hides most of the off-chip memory accessing overhead. Second, to decrease the high computational overhead, we propose a zero-skipping optimization to bypass a large amount of output computation. Lastly, to eliminate the cache contention, we propose an embedding cache dedicated to efficiently cache the embedding matrix.Our evaluations show that MnnFast is significantly effective in various types of hardware: CPU, GPU, and FPGA. MnnFast improves the overall throughput by up to 5.38\texttimes{}, 4.34\texttimes{}, and 2.01\texttimes{} on CPU, GPU, and FPGA respectively. Also, compared to CPU-based MnnFast, our FPGA-based MnnFast achieves 6.54\texttimes{} higher energy efficiency.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {250–263},
numpages = {14},
keywords = {architecture, accelerator, parallel algorithm, attention-based neural networks, machine learning, algorithm-hardware co-design, computation/dataflow optimization, memory networks},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322258,
author = {Deng, Chunhua and Sun, Fangxuan and Qian, Xuehai and Lin, Jun and Wang, Zhongfeng and Yuan, Bo},
title = {TIE: Energy-Efficient Tensor Train-Based Inference Engine for Deep Neural Network},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322258},
doi = {10.1145/3307650.3322258},
abstract = {In the era of artificial intelligence (AI), deep neural networks (DNNs) have emerged as the most important and powerful AI technique. However, large DNN models are both storage and computation intensive, posing significant challenges for adopting DNNs in resource-constrained scenarios. Thus, model compression becomes a crucial technique to ensure wide deployment of DNNs.This paper advances the state-of-the-art by considering tensor train (TT) decomposition, an very promising but yet explored compression technique in architecture domain. The method features with the extremely high compression ratio. However, the challenge is that the inference on the TT-format DNN models inherently incurs massive amount of redundant computations, causing significant energy consumption. Thus, the straightforward application of TT decomposition is not feasible.To address this fundamental challenge, this paper develops a computation-efficient inference scheme for TT-format DNN, which enjoys two key merits: 1) it achieves theoretical limit of number of multiplications, thus eliminating all redundant computations; and 2) the multi-stage processing scheme reduces the intensive memory access to all tensor cores, bringing significant energy saving.Based on the novel inference scheme, we develop TIE, a TT-format compressed DNN-targeted inference engine. TIE is highly flexible, supporting different types of networks for different needs. A 16-processing elements (PE) prototype is implemented using CMOS 28nm technology. Operating on 1000MHz, the TIE accelerator consumes 1.74mm2 and 154.8mW. Compared with EIE, TIE achieves 7.22\texttimes{} ~ 10.66\texttimes{} better area efficiency and 3.03\texttimes{} ~ 4.48\texttimes{} better energy efficiency on different workloads, respectively. Compared with CirCNN, TIE achieves 5.96\texttimes{} and 4.56\texttimes{} higher throughput and energy efficiency, respectively. The results show that TIE exhibits significant advantages over state-of-the-art solutions.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {264–278},
numpages = {15},
keywords = {tensor-train decomposition, acceleration, compression, deep learning},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322259,
author = {Li, Youjie and Liu, Iou-Jen and Yuan, Yifan and Chen, Deming and Schwing, Alexander and Huang, Jian},
title = {Accelerating Distributed Reinforcement Learning with In-Switch Computing},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322259},
doi = {10.1145/3307650.3322259},
abstract = {Reinforcement learning (RL) has attracted much attention recently, as new and emerging AI-based applications are demanding the capabilities to intelligently react to environment changes. Unlike distributed deep neural network (DNN) training, the distributed RL training has its unique workload characteristics - it generates orders of magnitude more iterations with much smaller sized but more frequent gradient aggregations. More specifically, our study with typical RL algorithms shows that their distributed training is latency critical and that the network communication for gradient aggregation occupies up to 83.2% of the execution time of each training iteration.In this paper, we present iSwitch, an in-switch acceleration solution that moves the gradient aggregation from server nodes into the network switches, thus we can reduce the number of network hops for gradient aggregation. This not only reduces the end-to-end network latency for synchronous training, but also improves the convergence with faster weight updates for asynchronous training. Upon the in-switch accelerator, we further reduce the synchronization overhead by conducting on-the-fly gradient aggregation at the granularity of network packets rather than gradient vectors. Moreover, we rethink the distributed RL training algorithms and also propose a hierarchical aggregation mechanism to further increase the parallelism and scalability of the distributed RL training at rack scale.We implement iSwitch using a real-world programmable switch NetFPGA board. We extend the control and data plane of the programmable switch to support iSwitch without affecting its regular network functions. Compared with state-of-the-art distributed training approaches, iSwitch offers a system-level speedup of up to 3.66\texttimes{} for synchronous distributed training and 3.71\texttimes{} for asynchronous distributed training, while achieving better scalability.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {279–291},
numpages = {13},
keywords = {in-switch accelerator, distributed machine learning, in-network computing, reinforcement learning},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322263,
author = {Zhang, Jiaqi and Chen, Xiangru and Song, Mingcong and Li, Tao},
title = {Eager Pruning: Algorithm and Architecture Support for Fast Training of Deep Neural Networks},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322263},
doi = {10.1145/3307650.3322263},
abstract = {Today's big and fast data and the changing circumstance require fast training of Deep Neural Networks (DNN) in various applications. However, training a DNN with tons of parameters involves intensive computation. Enlightened by the fact that redundancy exists in DNNs and the observation that the ranking of the significance of the weights changes slightly during training, we propose Eager Pruning, which speeds up DNN training by moving pruning to an early stage.Eager Pruning is supported by an algorithm and architecture co-design. The proposed algorithm dictates the architecture to identify and prune insignificant weights during training without accuracy loss. A novel architecture is designed to transform the reduced training computation into performance improvement. Our proposed Eager Pruning system gains an average of 1.91x speedup over state-of-the-art hardware accelerator and 6.31x energy-efficiency over Nvidia GPUs.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {292–303},
numpages = {12},
keywords = {neural network training, neural network pruning, software-hardware co-design},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322255,
author = {Sharify, Sayeh and Lascorz, Alberto Delmas and Mahmoud, Mostafa and Nikolic, Milos and Siu, Kevin and Stuart, Dylan Malone and Poulos, Zissis and Moshovos, Andreas},
title = {Laconic Deep Learning Inference Acceleration},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322255},
doi = {10.1145/3307650.3322255},
abstract = {We present a method for transparently identifying ineffectual computations during inference with Deep Learning models. Specifically, by decomposing multiplications down to the bit level, the amount of work needed by multiplications during inference can be potentially reduced by at least 40\texttimes{} across a wide selection of neural networks (8b and 16b). This method produces numerically identical results and does not affect overall accuracy. We present Laconic, a hardware accelerator that implements this approach to boost energy efficiency for inference with Deep Learning Networks. Laconic judiciously gives up some of the work reduction potential to yield a low-cost, simple, and energy efficient design that outperforms other state-of-the-art accelerators: an optimized DaDianNao-like design [13], Eyeriss [15], SCNN [71], Pragmatic [3], and BitFusion [83]. We study 16b, 8b, and 1b/2b fixed-point quantized models.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {304–317},
numpages = {14},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322228,
author = {Skarlatos, Dimitrios and Yan, Mengjia and Gopireddy, Bhargava and Sprabery, Read and Torrellas, Josep and Fletcher, Christopher W.},
title = {MicroScope: Enabling Microarchitectural Replay Attacks},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322228},
doi = {10.1145/3307650.3322228},
abstract = {The popularity of hardware-based Trusted Execution Environments (TEEs) has recently skyrocketed with the introduction of Intel's Software Guard Extensions (SGX). In SGX, the user process is protected from supervisor software, such as the operating system, through an isolated execution environment called an enclave. Despite the isolation guarantees provided by TEEs, numerous microarchitectural side channel attacks have been demonstrated that bypass their defense mechanisms. But, not all hope is lost for defenders: many modern fine-grain, high-resolution side channels---e.g., execution unit port contention---introduce large amounts of noise, complicating the adversary's task to reliably extract secrets.In this work, we introduce Microarchitectural Replay Attacks, whereby an SGX adversary can denoise nearly arbitrary microarchitectural side channels in a single run of the victim, by causing the victim to repeatedly replay on a page faulting instruction. We design, implement, and demonstrate our ideas in a framework, called MicroScope, and use it to denoise notoriously noisy side channels. Our main result shows how MicroScope can denoise the execution unit port contention channel. Specifically, we show how Micro-Scope can reliably detect the presence or absence of as few as two divide instructions in a single logical run of the victim program. Such an attack could be used to detect subnormal input to individual floating-point instructions, or infer branch directions in an enclave despite today's countermeasures that flush the branch predictor at the enclave boundary. We also use MicroScope to single-step and denoise a cache-based attack on the OpenSSL implementation of AES. Finally, we discuss the broader implications of microarchitectural replay attacks---as well as discuss other mechanisms that can cause replays.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {318–331},
numpages = {14},
keywords = {side-channel, security, operating system, virtual memory},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3326635,
author = {Yan, Mengjia and Wen, Jen-Yang and Fletcher, Christopher W. and Torrellas, Josep},
title = {SecDir: A Secure Directory to Defeat Directory Side-Channel Attacks},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3326635},
doi = {10.1145/3307650.3326635},
abstract = {Directories for cache coherence have been recently shown to be vulnerable to conflict-based side-channel attacks. By forcing directory conflicts, an attacker can evict victim directory entries, which in turn trigger the eviction of victim cache lines from private caches. This evidence strongly suggests that directories need to be redesigned for security. The key to a secure directory is to block interference between processes. Sadly, in an environment with many cores, this is hard or expensive to do.This paper presents the first design of a scalable secure directory. We call it SecDir. SecDir takes part of the storage used by a conventional directory and re-assigns it to per-core private directory areas used in a victim-cache manner called Victim Directories (VDs). The partitioned nature of VDs prevents directory interference across cores, defeating directory side-channel attacks. The VD of a core is distributed, and holds as many entries as lines in the private L2 cache of the core. To minimize victim self-conflicts in a VD during an attack, a VD is organized as a cuckoo directory. Such a design also obscures the victim's conflict patterns from the attacker. For our evaluation, we model with simulations the directory of an Intel Skylake-X server with and without SecDir. Our results show that SecDir has a negligible performance overhead. Furthermore, SecDir is area-efficient.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {332–345},
numpages = {14},
keywords = {side-channel attacks, cuckoo hashing, cache-coherence directories},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322238,
author = {Deng, Shuwen and Xiong, Wenjie and Szefer, Jakub},
title = {Secure TLBs},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322238},
doi = {10.1145/3307650.3322238},
abstract = {This paper focuses on a new attack vector in modern processors: the timing-based side and covert channel attacks due to the Translation Look-aside Buffers (TLBs). This paper first presents a novel three-step modeling approach that is used to exhaustively enumerate all possible TLB timing-based vulnerabilities. Building on the three-step model, this paper then shows how to automatically generate micro security benchmarks that test for the TLB vulnerabilities. After showing the insecurity of standard TLBs, two new secure TLB designs are presented: a Static-Partition (SP) TLB and a Random-Fill (RF) TLB. The new secure TLBs are evaluated using the Rocket Core implementation of the RISC-V processor architecture enhanced with the two new designs. The three-step model and the security benchmarks are used to analyze the security of the new designs in simulation. Based on the analysis, the proposed secure TLBs can defend not only against the previously publicized attacks but also against other new timing-based attacks in TLBs found using the new three-step model. The performance overhead is evaluated on an FPGA-based setup, and, for example, shows that the RF TLB has less than 10% overhead while defending all the attacks.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {346–359},
numpages = {14},
keywords = {timing attack defenses, timing side and covert channel attacks, TLBs},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322246,
author = {Qureshi, Moinuddin K.},
title = {New Attacks and Defense for Encrypted-Address Cache},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322246},
doi = {10.1145/3307650.3322246},
abstract = {Conflict-based cache attacks can allow an adversary to infer the access pattern of a co-running application by orchestrating evictions via cache conflicts. Such attacks can be mitigated by randomizing the location of the lines in the cache. Our recent proposal, CEASER, makes cache randomization practical by accessing the cache using an encrypted address and periodically changing the encryption key. CEASER was analyzed with the state-of-the-art algorithm on forming eviction sets, and the analysis showed that CEASER with a Remap-Rate of 1% is sufficient to tolerate years of attack.In this paper, we present two new attacks that significantly push the state-of-the-art in forming eviction sets. Our first attack reduces the time required to form the eviction set from O(L2) to O(L), where L is the number of lines in the attack. This attack is 35x faster than the best-known attack and requires that the Remap-Rate of CEASER be increased to 35%. Our second attack exploits the replacement policy (we analyze LRU, RRIP, and Random) to form eviction set quickly and requires that the Remap-Rate of CEASER be increased to more than 100%, incurring impractical overheads.To improve the robustness of CEASER against these attacks in a practical manner, we propose Skewed-CEASER (CEASER-S), which divides the cache ways into multiple partitions and maps the cache line to be resident in a different set in each partition. This design significantly improves the robustness of CEASER, as the attacker must form an eviction set that can dislodge the line from multiple possible locations. We show that CEASER-S can tolerate years of attacks while retaining a Remap-Rate of 1%. CEASER-S incurs negligible slowdown (within 1%) and a storage overhead of less than 100 bytes for the newly added structures.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {360–371},
numpages = {12},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322265,
author = {Aga, Shaizeen and Narayanasamy, Satish},
title = {InvisiPage: Oblivious Demand Paging for Secure Enclaves},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322265},
doi = {10.1145/3307650.3322265},
abstract = {State-of-art secure processors like Intel SGX remain susceptible to leaking page-level address trace of an application via the page fault channel in which a malicious OS induces spurious page faults and deduces application's secrets from it. Prior works which fix this vulnerability do not provision for OS demand paging to be oblivious. In this work, we present InvisiPage which obfuscates page fault channel while simultaneously making OS demand paging oblivious. To do so, InvisiPage first carefully distributes page management actions between the application and the OS. Second, InvisiPage secures application's page management interactions with the OS using a novel construct which is derived from Oblivious RAM (ORAM) but is customized for page management. Finally, we lower overheads of our approach by reducing page management interactions with the OS via a novel memory partition. For a suite of cloud applications which process sensitive data we show that page fault channel can be tackled while enabling oblivious demand paging at low overheads.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {372–384},
numpages = {13},
keywords = {ORAM, page fault channel, Intel SGX},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

@inproceedings{10.1145/3307650.3322232,
author = {Lee, Eojin and Kang, Ingab and Lee, Sukhan and Suh, G. Edward and Ahn, Jung Ho},
title = {TWiCe: Preventing Row-Hammering by Exploiting Time Window Counters},
year = {2019},
isbn = {9781450366694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307650.3322232},
doi = {10.1145/3307650.3322232},
abstract = {Computer systems using DRAM are exposed to row-hammer (RH) attacks, which can flip data in a DRAM row without directly accessing a row but by frequently activating its adjacent ones. There have been a number of proposals to prevent RH, but they either incur large area overhead, suffer from noticeable performance drop on adversarial memory access patterns, or provide probabilistic protection with no capability to detect attacks.In this paper, we propose a new counter-based RH prevention solution named Time Window Counter (TWiCe) based row refresh, which accurately detects potential RH attacks only using a small number of counters with a minimal performance impact. We first make a key observation that the number of rows that can cause RH is limited by the maximum values of row activation frequency and DRAM cell retention time. We calculate the maximum number of required counter entries per DRAM bank, with which TWiCe prevents RH with a strong deterministic guarantee. We leverage pseudo-associative cache design and separate the TWiCe table to further reduce area and energy overheads. TWiCe incurs no performance overhead on normal DRAM operations and less than 0.7% area and energy overheads over contemporary DRAM devices. Our evaluation shows that TWiCe makes no more than 0.006% of additional DRAM row activations for adversarial memory access patterns including RH attack scenarios.},
booktitle = {Proceedings of the 46th International Symposium on Computer Architecture},
pages = {385–396},
numpages = {12},
location = {Phoenix, Arizona},
series = {ISCA '19}
}

