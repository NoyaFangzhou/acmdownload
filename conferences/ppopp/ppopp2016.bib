@article{10.1145/3016078.2851158,
author = {Tallada, Marc Gonzalez},
title = {Coarse Grain Parallelization of Deep Neural Networks},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851158},
doi = {10.1145/3016078.2851158},
abstract = {Deep neural networks (DNN) have recently achieved extraordinary results in domains like computer vision and speech recognition. An essential element for this success has been the introduction of high performance computing (HPC) techniques in the critical step of training the neural network. This paper describes the implementation and analysis of a network-agnostic and convergence-invariant coarse-grain parallelization of the DNN training algorithm. The coarse-grain parallelization is achieved through the exploitation of the batch-level parallelism. This strategy is independent from the support of specialized and optimized libraries. Therefore, the optimization is immediately available for accelerating the DNN training. The proposal is compatible with multi-GPU execution without altering the algorithm convergence rate. The parallelization has been implemented in Caffe, a state-of-the-art DNN framework. The paper describes the code transformations for the parallelization and we also identify the limiting performance factors of the approach. We show competitive performance results for two state-of-the-art computer vision datasets, MNIST and CIFAR-10. In particular, on a 16-core Xeon E5-2667v2 at 3.30GHz we observe speedups of 8\texttimes{} over the sequential execution, at similar performance levels of those obtained by the GPU optimized Caffe version in a NVIDIA K40 GPU.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {1},
numpages = {12},
keywords = {OpenMP, stochastic gradient descent, shared memory algorithms, deep learning, neural networks, coarse-grain parallelism}
}

@inproceedings{10.1145/2851141.2851158,
author = {Tallada, Marc Gonzalez},
title = {Coarse Grain Parallelization of Deep Neural Networks},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851158},
doi = {10.1145/2851141.2851158},
abstract = {Deep neural networks (DNN) have recently achieved extraordinary results in domains like computer vision and speech recognition. An essential element for this success has been the introduction of high performance computing (HPC) techniques in the critical step of training the neural network. This paper describes the implementation and analysis of a network-agnostic and convergence-invariant coarse-grain parallelization of the DNN training algorithm. The coarse-grain parallelization is achieved through the exploitation of the batch-level parallelism. This strategy is independent from the support of specialized and optimized libraries. Therefore, the optimization is immediately available for accelerating the DNN training. The proposal is compatible with multi-GPU execution without altering the algorithm convergence rate. The parallelization has been implemented in Caffe, a state-of-the-art DNN framework. The paper describes the code transformations for the parallelization and we also identify the limiting performance factors of the approach. We show competitive performance results for two state-of-the-art computer vision datasets, MNIST and CIFAR-10. In particular, on a 16-core Xeon E5-2667v2 at 3.30GHz we observe speedups of 8\texttimes{} over the sequential execution, at similar performance levels of those obtained by the GPU optimized Caffe version in a NVIDIA K40 GPU.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {1},
numpages = {12},
keywords = {stochastic gradient descent, deep learning, shared memory algorithms, coarse-grain parallelism, neural networks, OpenMP},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851163,
author = {Wang, Xiao and Sabne, Amit and Kisner, Sherman and Raghunathan, Anand and Bouman, Charles and Midkiff, Samuel},
title = {High Performance Model Based Image Reconstruction},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851163},
doi = {10.1145/3016078.2851163},
abstract = {Computed Tomography (CT) Image Reconstruction is an important technique used in a wide range of applications, ranging from explosive detection, medical imaging to scientific imaging. Among available reconstruction methods, Model Based Iterative Reconstruction (MBIR) produces higher quality images and allows for the use of more general CT scanner geometries than is possible with more commonly used methods. The high computational cost of MBIR, however, often makes it impractical in applications for which it would otherwise be ideal. This paper describes a new MBIR implementation that significantly reduces the computational cost of MBIR while retaining its benefits. It describes a novel organization of the scanner data into super-voxels (SV) that, combined with a super-voxel buffer (SVB), dramatically increase locality and prefetching, enable parallelism across SVs and lead to an average speedup of 187 on 20 cores.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {2},
numpages = {12},
keywords = {parallel algorithm, multicore, CT image reconstruction, applications, MBIR}
}

@inproceedings{10.1145/2851141.2851163,
author = {Wang, Xiao and Sabne, Amit and Kisner, Sherman and Raghunathan, Anand and Bouman, Charles and Midkiff, Samuel},
title = {High Performance Model Based Image Reconstruction},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851163},
doi = {10.1145/2851141.2851163},
abstract = {Computed Tomography (CT) Image Reconstruction is an important technique used in a wide range of applications, ranging from explosive detection, medical imaging to scientific imaging. Among available reconstruction methods, Model Based Iterative Reconstruction (MBIR) produces higher quality images and allows for the use of more general CT scanner geometries than is possible with more commonly used methods. The high computational cost of MBIR, however, often makes it impractical in applications for which it would otherwise be ideal. This paper describes a new MBIR implementation that significantly reduces the computational cost of MBIR while retaining its benefits. It describes a novel organization of the scanner data into super-voxels (SV) that, combined with a super-voxel buffer (SVB), dramatically increase locality and prefetching, enable parallelism across SVs and lead to an average speedup of 187 on 20 cores.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {2},
numpages = {12},
keywords = {applications, parallel algorithm, MBIR, CT image reconstruction, multicore},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851144,
author = {Agrawal, Sandeep R. and Dee, Christopher M. and Lebeck, Alvin R.},
title = {Exploiting Accelerators for Efficient High Dimensional Similarity Search},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851144},
doi = {10.1145/3016078.2851144},
abstract = {Similarity search finds the most similar matches in an object collection for a given query; making it an important problem across a wide range of disciplines such as web search, image recognition and protein sequencing. Practical implementations of High Dimensional Similarity Search (HDSS) search across billions of possible solutions for multiple queries in real time, making its performance and efficiency a significant challenge. Existing clusters and datacenters use commercial multicore hardware to perform search, which may not provide the optimal performance and performance per Watt.This work explores the performance, power and cost benefits of using throughput accelerators like GPUs to perform similarity search for query cohorts even under tight deadlines. We propose optimized implementations of similarity search for both the host and the accelerator. Augmenting existing Xeon servers with accelerators results in a 3\texttimes{} improvement in throughput per machine, resulting in a more than 2.5\texttimes{} reduction in cost of ownership, even for discounted Xeon servers. Replacing a Xeon based cluster with an accelerator based cluster for similarity search reduces the total cost of ownership by more than 6\texttimes{} to 16\texttimes{} while consuming significantly less power than an ARM based cluster.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {3},
numpages = {12},
keywords = {total cost of ownership, high throughput, energy efficiency, GPGPU}
}

@inproceedings{10.1145/2851141.2851144,
author = {Agrawal, Sandeep R. and Dee, Christopher M. and Lebeck, Alvin R.},
title = {Exploiting Accelerators for Efficient High Dimensional Similarity Search},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851144},
doi = {10.1145/2851141.2851144},
abstract = {Similarity search finds the most similar matches in an object collection for a given query; making it an important problem across a wide range of disciplines such as web search, image recognition and protein sequencing. Practical implementations of High Dimensional Similarity Search (HDSS) search across billions of possible solutions for multiple queries in real time, making its performance and efficiency a significant challenge. Existing clusters and datacenters use commercial multicore hardware to perform search, which may not provide the optimal performance and performance per Watt.This work explores the performance, power and cost benefits of using throughput accelerators like GPUs to perform similarity search for query cohorts even under tight deadlines. We propose optimized implementations of similarity search for both the host and the accelerator. Augmenting existing Xeon servers with accelerators results in a 3\texttimes{} improvement in throughput per machine, resulting in a more than 2.5\texttimes{} reduction in cost of ownership, even for discounted Xeon servers. Replacing a Xeon based cluster with an accelerator based cluster for similarity search reduces the total cost of ownership by more than 6\texttimes{} to 16\texttimes{} while consuming significantly less power than an ARM based cluster.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {3},
numpages = {12},
keywords = {energy efficiency, high throughput, GPGPU, total cost of ownership},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851153,
author = {Cruz, Flavio and Rocha, Ricardo and Goldstein, Seth Copen},
title = {Declarative Coordination of Graph-Based Parallel Programs},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851153},
doi = {10.1145/3016078.2851153},
abstract = {Declarative programming has been hailed as a promising approach to parallel programming since it makes it easier to reason about programs while hiding the implementation details of parallelism from the programmer. However, its advantage is also its disadvantage as it leaves the programmer with no straightforward way to optimize programs for performance. In this paper, we introduce Coordinated Linear Meld (CLM), a concurrent forward-chaining linear logic programming language, with a declarative way to coordinate the execution of parallel programs allowing the programmer to specify arbitrary scheduling and data partitioning policies. Our approach allows the programmer to write graph-based declarative programs and then optionally to use coordination to fine-tune parallel performance. In this paper we specify the set of coordination facts, discuss their implementation in a parallel virtual machine, and show---through example---how they can be used to optimize parallel execution. We compare the performance of CLM programs against the original uncoordinated Linear Meld and several other frameworks.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {4},
numpages = {12},
keywords = {linear logic, parallel programming}
}

@inproceedings{10.1145/2851141.2851153,
author = {Cruz, Flavio and Rocha, Ricardo and Goldstein, Seth Copen},
title = {Declarative Coordination of Graph-Based Parallel Programs},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851153},
doi = {10.1145/2851141.2851153},
abstract = {Declarative programming has been hailed as a promising approach to parallel programming since it makes it easier to reason about programs while hiding the implementation details of parallelism from the programmer. However, its advantage is also its disadvantage as it leaves the programmer with no straightforward way to optimize programs for performance. In this paper, we introduce Coordinated Linear Meld (CLM), a concurrent forward-chaining linear logic programming language, with a declarative way to coordinate the execution of parallel programs allowing the programmer to specify arbitrary scheduling and data partitioning policies. Our approach allows the programmer to write graph-based declarative programs and then optionally to use coordination to fine-tune parallel performance. In this paper we specify the set of coordination facts, discuss their implementation in a parallel virtual machine, and show---through example---how they can be used to optimize parallel execution. We compare the performance of CLM programs against the original uncoordinated Linear Meld and several other frameworks.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {4},
numpages = {12},
keywords = {parallel programming, linear logic},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851157,
author = {Denniston, Tyler and Kamil, Shoaib and Amarasinghe, Saman},
title = {Distributed Halide},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851157},
doi = {10.1145/3016078.2851157},
abstract = {Many image processing tasks are naturally expressed as a pipeline of small computational kernels known as stencils. Halide is a popular domain-specific language and compiler designed to implement image processing algorithms. Halide uses simple language constructs to express what to compute and a separate scheduling co-language for expressing when and where to perform the computation. This approach has demonstrated performance comparable to or better than hand-optimized code. Until now, however, Halide has been restricted to parallel shared memory execution, limiting its performance for memory-bandwidth-bound pipelines or large-scale image processing tasks.We present an extension to Halide to support distributed-memory parallel execution of complex stencil pipelines. These extensions compose with the existing scheduling constructs in Halide, allowing expression of complex computation and communication strategies. Existing Halide applications can be distributed with minimal changes, allowing programmers to explore the tradeoff between recomputation and communication with little effort. Approximately 10 new of lines code are needed even for a 200 line, 99 stage application. On nine image processing benchmarks, our extensions give up to a 1.4\texttimes{} speedup on a single node over regular multithreaded execution with the same number of cores, by mitigating the effects of non-uniform memory access. The distributed benchmarks achieve up to 18\texttimes{} speedup on a 16 node testing machine and up to 57\texttimes{} speedup on 64 nodes of the NERSC Cori supercomputer.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {5},
numpages = {12},
keywords = {distributed memory, image processing, stencils}
}

@inproceedings{10.1145/2851141.2851157,
author = {Denniston, Tyler and Kamil, Shoaib and Amarasinghe, Saman},
title = {Distributed Halide},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851157},
doi = {10.1145/2851141.2851157},
abstract = {Many image processing tasks are naturally expressed as a pipeline of small computational kernels known as stencils. Halide is a popular domain-specific language and compiler designed to implement image processing algorithms. Halide uses simple language constructs to express what to compute and a separate scheduling co-language for expressing when and where to perform the computation. This approach has demonstrated performance comparable to or better than hand-optimized code. Until now, however, Halide has been restricted to parallel shared memory execution, limiting its performance for memory-bandwidth-bound pipelines or large-scale image processing tasks.We present an extension to Halide to support distributed-memory parallel execution of complex stencil pipelines. These extensions compose with the existing scheduling constructs in Halide, allowing expression of complex computation and communication strategies. Existing Halide applications can be distributed with minimal changes, allowing programmers to explore the tradeoff between recomputation and communication with little effort. Approximately 10 new of lines code are needed even for a 200 line, 99 stage application. On nine image processing benchmarks, our extensions give up to a 1.4\texttimes{} speedup on a single node over regular multithreaded execution with the same number of cores, by mitigating the effects of non-uniform memory access. The distributed benchmarks achieve up to 18\texttimes{} speedup on a 16 node testing machine and up to 57\texttimes{} speedup on 64 nodes of the NERSC Cori supercomputer.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {5},
numpages = {12},
keywords = {image processing, distributed memory, stencils},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851142,
author = {Newton, Ryan R. and A\u{g}acan, \"{O}mer S. and Fogg, Peter and Tobin-Hochstadt, Sam},
title = {Parallel Type-Checking with Haskell Using Saturating LVars and Stream Generators},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851142},
doi = {10.1145/3016078.2851142},
abstract = {Given the sophistication of recent type systems, unification-based type-checking and inference can be a time-consuming phase of compilation---especially when union types are combined with subtyping. It is natural to consider improving performance through parallelism, but these algorithms are challenging to parallelize due to complicated control structure and difficulties representing data in a way that is both efficient and supports concurrency. We provide techniques that address these problems based on the LVish approach to deterministic-by-default parallel programming. We extend LVish with Saturating LVars, the first LVars implemented to release memory during the object's lifetime. Our design allows us to achieve a parallel speedup on worst-case (exponential) inputs of Hindley-Milner inference, and on the Typed Racket type-checking algorithm, which yields up an 8.46\texttimes{} parallel speedup on 14 cores for type-checking examples drawn from the Racket repository.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {6},
numpages = {12}
}

@inproceedings{10.1145/2851141.2851142,
author = {Newton, Ryan R. and A\u{g}acan, \"{O}mer S. and Fogg, Peter and Tobin-Hochstadt, Sam},
title = {Parallel Type-Checking with Haskell Using Saturating LVars and Stream Generators},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851142},
doi = {10.1145/2851141.2851142},
abstract = {Given the sophistication of recent type systems, unification-based type-checking and inference can be a time-consuming phase of compilation---especially when union types are combined with subtyping. It is natural to consider improving performance through parallelism, but these algorithms are challenging to parallelize due to complicated control structure and difficulties representing data in a way that is both efficient and supports concurrency. We provide techniques that address these problems based on the LVish approach to deterministic-by-default parallel programming. We extend LVish with Saturating LVars, the first LVars implemented to release memory during the object's lifetime. Our design allows us to achieve a parallel speedup on worst-case (exponential) inputs of Hindley-Milner inference, and on the Typed Racket type-checking algorithm, which yields up an 8.46\texttimes{} parallel speedup on 14 cores for type-checking examples drawn from the Racket repository.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {6},
numpages = {12},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851154,
author = {Wang, Lei and Yang, Fan and Zhuang, Liangji and Cui, Huimin and Lv, Fang and Feng, Xiaobing},
title = {Articulation Points Guided Redundancy Elimination for Betweenness Centrality},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851154},
doi = {10.1145/3016078.2851154},
abstract = {Betweenness centrality (BC) is an important metrics in graph analysis which indicates critical vertices in large-scale networks based on shortest path enumeration. Typically, a BC algorithm constructs a shortest-path DAG for each vertex to calculate its BC score. However, for emerging real-world graphs, even the state-of-the-art BC algorithm will introduce a number of redundancies, as suggested by the existence of articulation points. Articulation points imply some common sub-DAGs in the DAGs for different vertices, but existing algorithms do not leverage such information and miss the optimization opportunity.We propose a redundancy elimination approach, which identifies the common sub-DAGs shared between the DAGs for different vertices. Our approach leverages the articulation points and reuses the results of the common sub-DAGs in calculating the BC scores, which eliminates redundant computations. We implemented the approach as an algorithm with two-level parallelism and evaluated it on a multicore platform. Compared to the state-of-the-art implementation using shared memory, our approach achieves an average speedup of 4.6x across a variety of real-world graphs, with the traversal rates up to 45 ~ 2400 MTEPS (Millions of Traversed Edges per Second).},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {7},
numpages = {13},
keywords = {betweenness centrality, partial redundancy elimination, parallelism}
}

@inproceedings{10.1145/2851141.2851154,
author = {Wang, Lei and Yang, Fan and Zhuang, Liangji and Cui, Huimin and Lv, Fang and Feng, Xiaobing},
title = {Articulation Points Guided Redundancy Elimination for Betweenness Centrality},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851154},
doi = {10.1145/2851141.2851154},
abstract = {Betweenness centrality (BC) is an important metrics in graph analysis which indicates critical vertices in large-scale networks based on shortest path enumeration. Typically, a BC algorithm constructs a shortest-path DAG for each vertex to calculate its BC score. However, for emerging real-world graphs, even the state-of-the-art BC algorithm will introduce a number of redundancies, as suggested by the existence of articulation points. Articulation points imply some common sub-DAGs in the DAGs for different vertices, but existing algorithms do not leverage such information and miss the optimization opportunity.We propose a redundancy elimination approach, which identifies the common sub-DAGs shared between the DAGs for different vertices. Our approach leverages the articulation points and reuses the results of the common sub-DAGs in calculating the BC scores, which eliminates redundant computations. We implemented the approach as an algorithm with two-level parallelism and evaluated it on a multicore platform. Compared to the state-of-the-art implementation using shared memory, our approach achieves an average speedup of 4.6x across a variety of real-world graphs, with the traversal rates up to 45 ~ 2400 MTEPS (Millions of Traversed Edges per Second).},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {7},
numpages = {13},
keywords = {betweenness centrality, partial redundancy elimination, parallelism},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851161,
author = {Bloemen, Vincent and Laarman, Alfons and van de Pol, Jaco},
title = {Multi-Core on-the-Fly SCC Decomposition},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851161},
doi = {10.1145/3016078.2851161},
abstract = {The main advantages of Tarjan's strongly connected component (SCC) algorithm are its linear time complexity and ability to return SCCs on-the-fly, while traversing or even generating the graph. Until now, most parallel SCC algorithms sacrifice both: they run in quadratic worst-case time and/or require the full graph in advance.The current paper presents a novel parallel, on-the-fly SCC algorithm. It preserves the linear-time property by letting workers explore the graph randomly while carefully communicating partially completed SCCs. We prove that this strategy is correct. For efficiently communicating partial SCCs, we develop a concurrent, iterable disjoint set structure (combining the union-find data structure with a cyclic list).We demonstrate scalability on a 64-core machine using 75 real-world graphs (from model checking and explicit data graphs), synthetic graphs (combinations of trees, cycles and linear graphs), and random graphs. Previous work did not show speedups for graphs containing a large SCC. We observe that our parallel algorithm is typically 10-30\texttimes{} faster compared to Tarjan's algorithm for graphs containing a large SCC. Comparable performance (with respect to the current state-of-the-art) is obtained for graphs containing many small SCCs.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {8},
numpages = {12},
keywords = {digraph, union-find, SCC, graph, parallel, depth-first search, algorithm, keywords strongly connected components, multi-core}
}

@inproceedings{10.1145/2851141.2851161,
author = {Bloemen, Vincent and Laarman, Alfons and van de Pol, Jaco},
title = {Multi-Core on-the-Fly SCC Decomposition},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851161},
doi = {10.1145/2851141.2851161},
abstract = {The main advantages of Tarjan's strongly connected component (SCC) algorithm are its linear time complexity and ability to return SCCs on-the-fly, while traversing or even generating the graph. Until now, most parallel SCC algorithms sacrifice both: they run in quadratic worst-case time and/or require the full graph in advance.The current paper presents a novel parallel, on-the-fly SCC algorithm. It preserves the linear-time property by letting workers explore the graph randomly while carefully communicating partially completed SCCs. We prove that this strategy is correct. For efficiently communicating partial SCCs, we develop a concurrent, iterable disjoint set structure (combining the union-find data structure with a cyclic list).We demonstrate scalability on a 64-core machine using 75 real-world graphs (from model checking and explicit data graphs), synthetic graphs (combinations of trees, cycles and linear graphs), and random graphs. Previous work did not show speedups for graphs containing a large SCC. We observe that our parallel algorithm is typically 10-30\texttimes{} faster compared to Tarjan's algorithm for graphs containing a large SCC. Comparable performance (with respect to the current state-of-the-art) is obtained for graphs containing many small SCCs.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {8},
numpages = {12},
keywords = {multi-core, keywords strongly connected components, algorithm, SCC, depth-first search, union-find, parallel, graph, digraph},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851152,
author = {Kannan, Ramakrishnan and Ballard, Grey and Park, Haesun},
title = {A High-Performance Parallel Algorithm for Nonnegative Matrix Factorization},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851152},
doi = {10.1145/3016078.2851152},
abstract = {Non-negative matrix factorization (NMF) is the problem of determining two non-negative low rank factors W and H, for the given input matrix A, such that A ≈ WH. NMF is a useful tool for many applications in different domains such as topic modeling in text mining, background separation in video analysis, and community detection in social networks. Despite its popularity in the data mining community, there is a lack of efficient distributed algorithms to solve the problem for big data sets.We propose a high-performance distributed-memory parallel algorithm that computes the factorization by iteratively solving alternating non-negative least squares (NLS) subproblems for W and H. It maintains the data and factor matrices in memory (distributed across processors), uses MPI for interprocessor communication, and, in the dense case, provably minimizes communication costs (under mild assumptions). As opposed to previous implementations, our algorithm is also flexible: (1) it performs well for both dense and sparse matrices, and (2) it allows the user to choose any one of the multiple algorithms for solving the updates to low rank factors W and H within the alternating iterations. We demonstrate the scalability of our algorithm and compare it with baseline implementations, showing significant performance improvements.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {9},
numpages = {11}
}

@inproceedings{10.1145/2851141.2851152,
author = {Kannan, Ramakrishnan and Ballard, Grey and Park, Haesun},
title = {A High-Performance Parallel Algorithm for Nonnegative Matrix Factorization},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851152},
doi = {10.1145/2851141.2851152},
abstract = {Non-negative matrix factorization (NMF) is the problem of determining two non-negative low rank factors W and H, for the given input matrix A, such that A ≈ WH. NMF is a useful tool for many applications in different domains such as topic modeling in text mining, background separation in video analysis, and community detection in social networks. Despite its popularity in the data mining community, there is a lack of efficient distributed algorithms to solve the problem for big data sets.We propose a high-performance distributed-memory parallel algorithm that computes the factorization by iteratively solving alternating non-negative least squares (NLS) subproblems for W and H. It maintains the data and factor matrices in memory (distributed across processors), uses MPI for interprocessor communication, and, in the dense case, provably minimizes communication costs (under mild assumptions). As opposed to previous implementations, our algorithm is also flexible: (1) it performs well for both dense and sparse matrices, and (2) it allows the user to choose any one of the multiple algorithms for solving the updates to low rank factors W and H within the alternating iterations. We demonstrate the scalability of our algorithm and compare it with baseline implementations, showing significant performance improvements.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {9},
numpages = {11},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851167,
author = {Chowdhury, Rezaul and Ganapathi, Pramod and Tithi, Jesmin Jahan and Bachmeier, Charles and Kuszmaul, Bradley C. and Leiserson, Charles E. and Solar-Lezama, Armando and Tang, Yuan},
title = {AUTOGEN: Automatic Discovery of Cache-Oblivious Parallel Recursive Algorithms for Solving Dynamic Programs},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851167},
doi = {10.1145/3016078.2851167},
abstract = {We present AUTOGEN---an algorithm that for a wide class of dynamic programming (DP) problems automatically discovers highly efficient cache-oblivious parallel recursive divide-and-conquer algorithms from inefficient iterative descriptions of DP recurrences. AUTOGEN analyzes the set of DP table locations accessed by the iterative algorithm when run on a DP table of small size, and automatically identifies a recursive access pattern and a corresponding provably correct recursive algorithm for solving the DP recurrence. We use AUTOGEN to autodiscover efficient algorithms for several well-known problems. Our experimental results show that several autodiscovered algorithms significantly outperform parallel looping and tiled loop-based algorithms. Also these algorithms are less sensitive to fluctuations of memory and bandwidth compared with their looping counterparts, and their running times and energy profiles remain relatively more stable. To the best of our knowledge, AUTOGEN is the first algorithm that can automatically discover new nontrivial divide-and-conquer algorithms.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {10},
numpages = {12},
keywords = {recursive, energy-efficient, cache-oblivious, automatic discovery, parallel, cache-adaptive, AutoGen, divide-and-conquer, dynamic programming, cache-efficient}
}

@inproceedings{10.1145/2851141.2851167,
author = {Chowdhury, Rezaul and Ganapathi, Pramod and Tithi, Jesmin Jahan and Bachmeier, Charles and Kuszmaul, Bradley C. and Leiserson, Charles E. and Solar-Lezama, Armando and Tang, Yuan},
title = {AUTOGEN: Automatic Discovery of Cache-Oblivious Parallel Recursive Algorithms for Solving Dynamic Programs},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851167},
doi = {10.1145/2851141.2851167},
abstract = {We present AUTOGEN---an algorithm that for a wide class of dynamic programming (DP) problems automatically discovers highly efficient cache-oblivious parallel recursive divide-and-conquer algorithms from inefficient iterative descriptions of DP recurrences. AUTOGEN analyzes the set of DP table locations accessed by the iterative algorithm when run on a DP table of small size, and automatically identifies a recursive access pattern and a corresponding provably correct recursive algorithm for solving the DP recurrence. We use AUTOGEN to autodiscover efficient algorithms for several well-known problems. Our experimental results show that several autodiscovered algorithms significantly outperform parallel looping and tiled loop-based algorithms. Also these algorithms are less sensitive to fluctuations of memory and bandwidth compared with their looping counterparts, and their running times and energy profiles remain relatively more stable. To the best of our knowledge, AUTOGEN is the first algorithm that can automatically discover new nontrivial divide-and-conquer algorithms.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {10},
numpages = {12},
keywords = {cache-oblivious, cache-adaptive, parallel, divide-and-conquer, AutoGen, recursive, dynamic programming, cache-efficient, automatic discovery, energy-efficient},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851145,
author = {Wang, Yangzihao and Davidson, Andrew and Pan, Yuechao and Wu, Yuduo and Riffel, Andy and Owens, John D.},
title = {Gunrock: A High-Performance Graph Processing Library on the GPU},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851145},
doi = {10.1145/3016078.2851145},
abstract = {For large-scale graph analytics on the GPU, the irregularity of data access/control flow and the complexity of programming GPUs have been two significant challenges for developing a programmable high-performance graph library. "Gunrock," our high-level bulk-synchronous graph-processing system targeting the GPU, takes a new approach to abstracting GPU graph analytics: rather than designing an abstraction around computation, Gunrock instead implements a novel data-centric abstraction centered on operations on a vertex or edge frontier. Gunrock achieves a balance between performance and expressiveness by coupling high-performance GPU computing primitives and optimization strategies with a high-level programming model that allows programmers to quickly develop new graph primitives with small code size and minimal GPU programming knowledge. We evaluate Gunrock on five graph primitives (BFS, BC, SSSP, CC, and PageRank) and show that Gunrock has on average at least an order of magnitude speedup over Boost and PowerGraph, comparable performance to the fastest GPU hardwired primitives, and better performance than any other GPU high-level graph library.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {11},
numpages = {12}
}

@inproceedings{10.1145/2851141.2851145,
author = {Wang, Yangzihao and Davidson, Andrew and Pan, Yuechao and Wu, Yuduo and Riffel, Andy and Owens, John D.},
title = {Gunrock: A High-Performance Graph Processing Library on the GPU},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851145},
doi = {10.1145/2851141.2851145},
abstract = {For large-scale graph analytics on the GPU, the irregularity of data access/control flow and the complexity of programming GPUs have been two significant challenges for developing a programmable high-performance graph library. "Gunrock," our high-level bulk-synchronous graph-processing system targeting the GPU, takes a new approach to abstracting GPU graph analytics: rather than designing an abstraction around computation, Gunrock instead implements a novel data-centric abstraction centered on operations on a vertex or edge frontier. Gunrock achieves a balance between performance and expressiveness by coupling high-performance GPU computing primitives and optimization strategies with a high-level programming model that allows programmers to quickly develop new graph primitives with small code size and minimal GPU programming knowledge. We evaluate Gunrock on five graph primitives (BFS, BC, SSSP, CC, and PageRank) and show that Gunrock has on average at least an order of magnitude speedup over Boost and PowerGraph, comparable performance to the fastest GPU hardwired primitives, and better performance than any other GPU high-level graph library.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {11},
numpages = {12},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851169,
author = {Ashkiani, Saman and Davidson, Andrew and Meyer, Ulrich and Owens, John D.},
title = {GPU Multisplit},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851169},
doi = {10.1145/3016078.2851169},
abstract = {Multisplit is a broadly useful parallel primitive that permutes its input data into contiguous buckets or bins, where the function that categorizes an element into a bucket is provided by the programmer. Due to the lack of an efficient multisplit on GPUs, programmers often choose to implement multisplit with a sort. However, sort does more work than necessary to implement multisplit, and is thus inefficient. In this work, we provide a parallel model and multiple implementations for the multisplit problem. Our principal focus is multisplit for a small number of buckets. In our implementations, we exploit the computational hierarchy of the GPU to perform most of the work locally, with minimal usage of global operations. We also use warp-synchronous programming models to avoid branch divergence and reduce memory usage, as well as hierarchical reordering of input elements to achieve better coalescing of global memory accesses. On an NVIDIA K40c GPU, for key-only (key-value) multisplit, we demonstrate a 3.0-6.7x (4.4-8.0x) speedup over radix sort, and achieve a peak throughput of 10.0 G keys/s.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {12},
numpages = {13}
}

@inproceedings{10.1145/2851141.2851169,
author = {Ashkiani, Saman and Davidson, Andrew and Meyer, Ulrich and Owens, John D.},
title = {GPU Multisplit},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851169},
doi = {10.1145/2851141.2851169},
abstract = {Multisplit is a broadly useful parallel primitive that permutes its input data into contiguous buckets or bins, where the function that categorizes an element into a bucket is provided by the programmer. Due to the lack of an efficient multisplit on GPUs, programmers often choose to implement multisplit with a sort. However, sort does more work than necessary to implement multisplit, and is thus inefficient. In this work, we provide a parallel model and multiple implementations for the multisplit problem. Our principal focus is multisplit for a small number of buckets. In our implementations, we exploit the computational hierarchy of the GPU to perform most of the work locally, with minimal usage of global operations. We also use warp-synchronous programming models to avoid branch divergence and reduce memory usage, as well as hierarchical reordering of input elements to achieve better coalescing of global memory accesses. On an NVIDIA K40c GPU, for key-only (key-value) multisplit, we demonstrate a 3.0-6.7x (4.4-8.0x) speedup over radix sort, and achieve a peak throughput of 10.0 G keys/s.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {12},
numpages = {13},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851148,
author = {De Matteis, Tiziano and Mencagli, Gabriele},
title = {Keep Calm and React with Foresight: Strategies for Low-Latency and Energy-Efficient Elastic Data Stream Processing},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851148},
doi = {10.1145/3016078.2851148},
abstract = {This paper addresses the problem of designing scaling strategies for elastic data stream processing. Elasticity allows applications to rapidly change their configuration on-the-fly (e.g., the amount of used resources) in response to dynamic workload fluctuations. In this work we face this problem by adopting the Model Predictive Control technique, a control-theoretic method aimed at finding the optimal application configuration along a limited prediction horizon in the future by solving an online optimization problem. Our control strategies are designed to address latency constraints, using Queueing Theory models, and energy consumption by changing the number of used cores and the CPU frequency through the Dynamic Voltage and Frequency Scaling (DVFS) support available in the modern multicore CPUs. The proactive capabilities, in addition to the latency- and energy-awareness, represent the novel features of our approach. To validate our methodology, we develop a thorough set of experiments on a high-frequency trading application. The results demonstrate the high-degree of flexibility and configurability of our approach, and show the effectiveness of our elastic scaling strategies compared with existing state-of-the-art techniques used in similar scenarios.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {13},
numpages = {12},
keywords = {model predictive control, DVFS, data stream processing, elasticity, multicore programming}
}

@inproceedings{10.1145/2851141.2851148,
author = {De Matteis, Tiziano and Mencagli, Gabriele},
title = {Keep Calm and React with Foresight: Strategies for Low-Latency and Energy-Efficient Elastic Data Stream Processing},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851148},
doi = {10.1145/2851141.2851148},
abstract = {This paper addresses the problem of designing scaling strategies for elastic data stream processing. Elasticity allows applications to rapidly change their configuration on-the-fly (e.g., the amount of used resources) in response to dynamic workload fluctuations. In this work we face this problem by adopting the Model Predictive Control technique, a control-theoretic method aimed at finding the optimal application configuration along a limited prediction horizon in the future by solving an online optimization problem. Our control strategies are designed to address latency constraints, using Queueing Theory models, and energy consumption by changing the number of used cores and the CPU frequency through the Dynamic Voltage and Frequency Scaling (DVFS) support available in the modern multicore CPUs. The proactive capabilities, in addition to the latency- and energy-awareness, represent the novel features of our approach. To validate our methodology, we develop a thorough set of experiments on a high-frequency trading application. The results demonstrate the high-degree of flexibility and configurability of our approach, and show the effectiveness of our elastic scaling strategies compared with existing state-of-the-art techniques used in similar scenarios.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {13},
numpages = {12},
keywords = {multicore programming, elasticity, DVFS, model predictive control, data stream processing},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851151,
author = {Li, Jing and Agrawal, Kunal and Elnikety, Sameh and He, Yuxiong and Lee, I-Ting Angelina and Lu, Chenyang and McKinley, Kathryn S.},
title = {Work Stealing for Interactive Services to Meet Target Latency},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851151},
doi = {10.1145/3016078.2851151},
abstract = {Interactive web services increasingly drive critical business workloads such as search, advertising, games, shopping, and finance. Whereas optimizing parallel programs and distributed server systems have historically focused on average latency and throughput, the primary metric for interactive applications is instead consistent responsiveness, i.e., minimizing the number of requests that miss a target latency. This paper is the first to show how to generalize work-stealing, which is traditionally used to minimize the makespan of a single parallel job, to optimize for a target latency in interactive services with multiple parallel requests.We design a new adaptive work stealing policy, called tail-control, that reduces the number of requests that miss a target latency. It uses instantaneous request progress, system load, and a target latency to choose when to parallelize requests with stealing, when to admit new requests, and when to limit parallelism of large requests. We implement this approach in the Intel Thread Building Block (TBB) library and evaluate it on real-world workloads and synthetic workloads. The tail-control policy substantially reduces the number of requests exceeding the desired target latency and delivers up to 58% relative improvement over various baseline policies. This generalization of work stealing for multiple requests effectively optimizes the number of requests that complete within a target latency, a key metric for interactive services.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {14},
numpages = {13}
}

@inproceedings{10.1145/2851141.2851151,
author = {Li, Jing and Agrawal, Kunal and Elnikety, Sameh and He, Yuxiong and Lee, I-Ting Angelina and Lu, Chenyang and McKinley, Kathryn S.},
title = {Work Stealing for Interactive Services to Meet Target Latency},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851151},
doi = {10.1145/2851141.2851151},
abstract = {Interactive web services increasingly drive critical business workloads such as search, advertising, games, shopping, and finance. Whereas optimizing parallel programs and distributed server systems have historically focused on average latency and throughput, the primary metric for interactive applications is instead consistent responsiveness, i.e., minimizing the number of requests that miss a target latency. This paper is the first to show how to generalize work-stealing, which is traditionally used to minimize the makespan of a single parallel job, to optimize for a target latency in interactive services with multiple parallel requests.We design a new adaptive work stealing policy, called tail-control, that reduces the number of requests that miss a target latency. It uses instantaneous request progress, system load, and a target latency to choose when to parallelize requests with stealing, when to admit new requests, and when to limit parallelism of large requests. We implement this approach in the Intel Thread Building Block (TBB) library and evaluate it on real-world workloads and synthetic workloads. The tail-control policy substantially reduces the number of requests exceeding the desired target latency and delivers up to 58% relative improvement over various baseline policies. This generalization of work stealing for multiple requests effectively optimizes the number of requests that complete within a target latency, a key metric for interactive services.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {14},
numpages = {13},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851147,
author = {Steele, Guy L. and Tristan, Jean-Baptiste},
title = {Adding Approximate Counters},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851147},
doi = {10.1145/3016078.2851147},
abstract = {We describe a general framework for adding the values of two approximate counters to produce a new approximate counter value whose expected estimated value is equal to the sum of the expected estimated values of the given approximate counters. (To the best of our knowledge, this is the first published description of any algorithm for adding two approximate counters.) We then work out implementation details for five different kinds of approximate counter and provide optimized pseudocode. For three of them, we present proofs that the variance of a counter value produced by adding two counter values in this way is bounded, and in fact is no worse, or not much worse, than the variance of the value of a single counter to which the same total number of increment operations have been applied. Addition of approximate counters is useful in massively parallel divide-and-conquer algorithms that use a distributed representation for large arrays of counters. We describe two machine-learning algorithms for topic modeling that use millions of integer counters, and confirm that replacing the integer counters with approximate counters is effective, speeding up a GPU-based implementation by over 65% and a CPU-based by nearly 50%, as well as reducing memory requirements, without degrading their statistical effectiveness.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {15},
numpages = {12},
keywords = {multithreading, approximate counters, statistical counters, divide and conquer, parallel computing, distributed computing}
}

@inproceedings{10.1145/2851141.2851147,
author = {Steele, Guy L. and Tristan, Jean-Baptiste},
title = {Adding Approximate Counters},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851147},
doi = {10.1145/2851141.2851147},
abstract = {We describe a general framework for adding the values of two approximate counters to produce a new approximate counter value whose expected estimated value is equal to the sum of the expected estimated values of the given approximate counters. (To the best of our knowledge, this is the first published description of any algorithm for adding two approximate counters.) We then work out implementation details for five different kinds of approximate counter and provide optimized pseudocode. For three of them, we present proofs that the variance of a counter value produced by adding two counter values in this way is bounded, and in fact is no worse, or not much worse, than the variance of the value of a single counter to which the same total number of increment operations have been applied. Addition of approximate counters is useful in massively parallel divide-and-conquer algorithms that use a distributed representation for large arrays of counters. We describe two machine-learning algorithms for topic modeling that use millions of integer counters, and confirm that replacing the integer counters with approximate counters is effective, speeding up a GPU-based implementation by over 65% and a CPU-based by nearly 50%, as well as reducing memory requirements, without degrading their statistical effectiveness.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {15},
numpages = {12},
keywords = {divide and conquer, multithreading, distributed computing, statistical counters, parallel computing, approximate counters},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851168,
author = {Yang, Chaoran and Mellor-Crummey, John},
title = {A Wait-Free Queue as Fast as Fetch-and-Add},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851168},
doi = {10.1145/3016078.2851168},
abstract = {Concurrent data structures that have fast and predictable performance are of critical importance for harnessing the power of multicore processors, which are now ubiquitous. Although wait-free objects, whose operations complete in a bounded number of steps, were devised more than two decades ago, wait-free objects that can deliver scalable high performance are still rare.In this paper, we present the first wait-free FIFO queue based on fetch-and-add (FAA). While compare-and-swap (CAS) based non-blocking algorithms may perform poorly due to work wasted by CAS failures, algorithms that coordinate using FAA, which is guaranteed to succeed, can in principle perform better under high contention. Along with FAA, our queue uses a custom epoch-based scheme to reclaim memory; on x86 architectures, it requires no extra memory fences on our algorithm's typical execution path. An empirical study of our new FAA-based wait-free FIFO queue under high contention on four different architectures with many hardware threads shows that it outperforms prior queue designs that lack a wait-free progress guarantee. Surprisingly, at the highest level of contention, the throughput of our queue is often as high as that of a microbenchmark that only performs FAA. As a result, our fast wait-free queue implementation is useful in practice on most multi-core systems today. We believe that our design can serve as an example of how to construct other fast wait-free objects.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {16},
numpages = {13},
keywords = {non-blocking queue, wait-free, fast-path-slow-path}
}

@inproceedings{10.1145/2851141.2851168,
author = {Yang, Chaoran and Mellor-Crummey, John},
title = {A Wait-Free Queue as Fast as Fetch-and-Add},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851168},
doi = {10.1145/2851141.2851168},
abstract = {Concurrent data structures that have fast and predictable performance are of critical importance for harnessing the power of multicore processors, which are now ubiquitous. Although wait-free objects, whose operations complete in a bounded number of steps, were devised more than two decades ago, wait-free objects that can deliver scalable high performance are still rare.In this paper, we present the first wait-free FIFO queue based on fetch-and-add (FAA). While compare-and-swap (CAS) based non-blocking algorithms may perform poorly due to work wasted by CAS failures, algorithms that coordinate using FAA, which is guaranteed to succeed, can in principle perform better under high contention. Along with FAA, our queue uses a custom epoch-based scheme to reclaim memory; on x86 architectures, it requires no extra memory fences on our algorithm's typical execution path. An empirical study of our new FAA-based wait-free FIFO queue under high contention on four different architectures with many hardware threads shows that it outperforms prior queue designs that lack a wait-free progress guarantee. Surprisingly, at the highest level of contention, the throughput of our queue is often as high as that of a microbenchmark that only performs FAA. As a result, our fast wait-free queue implementation is useful in practice on most multi-core systems today. We believe that our design can serve as an example of how to construct other fast wait-free objects.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {16},
numpages = {13},
keywords = {non-blocking queue, wait-free, fast-path-slow-path},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851155,
author = {Haider, Syed Kamran and Hasenplaugh, William and Alistarh, Dan},
title = {Lease/Release: Architectural Support for Scaling Contended Data Structures},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851155},
doi = {10.1145/3016078.2851155},
abstract = {High memory contention is generally agreed to be a worst-case scenario for concurrent data structures. There has been a significant amount of research effort spent investigating designs which minimize contention, and several programming techniques have been proposed to mitigate its effects. However, there are currently few architectural mechanisms to allow scaling contended data structures at high thread counts.In this paper, we investigate hardware support for scalable contended data structures. We propose Lease/Release, a simple addition to standard directory-based MSI cache coherence protocols, allowing participants to lease memory, at the granularity of cache lines, by delaying coherence messages for a short, bounded period of time. Our analysis shows that Lease/Release can significantly reduce the overheads of contention for both non-blocking (lock-free) and lock-based data structure implementations, while ensuring that no deadlocks are introduced. We validate Lease/Release empirically on the Graphite multiprocessor simulator, on a range of data structures, including queue, stack, and priority queue implementations, as well as on transactional applications. Results show that Lease/Release consistently improves both throughput and energy usage, by up to 5x, both for lock-free and lock-based data structure designs.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {17},
numpages = {12}
}

@inproceedings{10.1145/2851141.2851155,
author = {Haider, Syed Kamran and Hasenplaugh, William and Alistarh, Dan},
title = {Lease/Release: Architectural Support for Scaling Contended Data Structures},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851155},
doi = {10.1145/2851141.2851155},
abstract = {High memory contention is generally agreed to be a worst-case scenario for concurrent data structures. There has been a significant amount of research effort spent investigating designs which minimize contention, and several programming techniques have been proposed to mitigate its effects. However, there are currently few architectural mechanisms to allow scaling contended data structures at high thread counts.In this paper, we investigate hardware support for scalable contended data structures. We propose Lease/Release, a simple addition to standard directory-based MSI cache coherence protocols, allowing participants to lease memory, at the granularity of cache lines, by delaying coherence messages for a short, bounded period of time. Our analysis shows that Lease/Release can significantly reduce the overheads of contention for both non-blocking (lock-free) and lock-based data structure implementations, while ensuring that no deadlocks are introduced. We validate Lease/Release empirically on the Graphite multiprocessor simulator, on a range of data structures, including queue, stack, and priority queue implementations, as well as on transactional applications. Results show that Lease/Release consistently improves both throughput and energy usage, by up to 5x, both for lock-free and lock-based data structure designs.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {17},
numpages = {12},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851146,
author = {Guerraoui, Rachid and Trigonakis, Vasileios},
title = {Optimistic Concurrency with OPTIK},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851146},
doi = {10.1145/3016078.2851146},
abstract = {We introduce OPTIK, a new practical design pattern for designing and implementing fast and scalable concurrent data structures. OPTIK relies on the commonly-used technique of version numbers for detecting conflicting concurrent operations. We show how to implement the OPTIK pattern using the novel concept of OPTIK locks. These locks enable the use of version numbers for implementing very efficient optimistic concurrent data structures. Existing state-of-the-art lock-based data structures acquire the lock and then check for conflicts. In contrast, with OPTIK locks, we merge the lock acquisition with the detection of conflicting concurrency in a single atomic step, similarly to lock-free algorithms. We illustrate the power of our OPTIK pattern and its implementation by introducing four new algorithms and by optimizing four state-of-the-art algorithms for linked lists, skip lists, hash tables, and queues. Our results show that concurrent data structures built using OPTIK are more scalable than the state of the art.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {18},
numpages = {12}
}

@inproceedings{10.1145/2851141.2851146,
author = {Guerraoui, Rachid and Trigonakis, Vasileios},
title = {Optimistic Concurrency with OPTIK},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851146},
doi = {10.1145/2851141.2851146},
abstract = {We introduce OPTIK, a new practical design pattern for designing and implementing fast and scalable concurrent data structures. OPTIK relies on the commonly-used technique of version numbers for detecting conflicting concurrent operations. We show how to implement the OPTIK pattern using the novel concept of OPTIK locks. These locks enable the use of version numbers for implementing very efficient optimistic concurrent data structures. Existing state-of-the-art lock-based data structures acquire the lock and then check for conflicts. In contrast, with OPTIK locks, we merge the lock acquisition with the detection of conflicting concurrency in a single atomic step, similarly to lock-free algorithms. We illustrate the power of our OPTIK pattern and its implementation by introducing four new algorithms and by optimizing four state-of-the-art algorithms for linked lists, skip lists, hash tables, and queues. Our results show that concurrent data structures built using OPTIK are more scalable than the state of the art.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {18},
numpages = {12},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851162,
author = {Dice, Dave and Kogan, Alex and Lev, Yossi},
title = {Refined Transactional Lock Elision},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851162},
doi = {10.1145/3016078.2851162},
abstract = {Transactional lock elision (TLE) is a well-known technique that exploits hardware transactional memory (HTM) to introduce concurrency into lock-based software. It achieves that by attempting to execute a critical section protected by a lock in an atomic hardware transaction, reverting to the lock if these attempts fail. One significant drawback of TLE is that it disables hardware speculation once there is a thread running under lock. In this paper we present two algorithms that rely on existing compiler support for transactional programs and allow threads to speculate concurrently on HTM along with a thread holding the lock. We demonstrate the benefit of our algorithms over TLE and other related approaches with an in-depth analysis of a number of benchmarks and a wide range of workloads, including an AVL tree-based micro-benchmark and ccTSA, a real sequence assembler application.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {19},
numpages = {12},
keywords = {transactional lock elision, concurrency, hardware transactional memory}
}

@inproceedings{10.1145/2851141.2851162,
author = {Dice, Dave and Kogan, Alex and Lev, Yossi},
title = {Refined Transactional Lock Elision},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851162},
doi = {10.1145/2851141.2851162},
abstract = {Transactional lock elision (TLE) is a well-known technique that exploits hardware transactional memory (HTM) to introduce concurrency into lock-based software. It achieves that by attempting to execute a critical section protected by a lock in an atomic hardware transaction, reverting to the lock if these attempts fail. One significant drawback of TLE is that it disables hardware speculation once there is a thread running under lock. In this paper we present two algorithms that rely on existing compiler support for transactional programs and allow threads to speculate concurrently on HTM along with a thread holding the lock. We demonstrate the benefit of our algorithms over TLE and other related approaches with an in-depth analysis of a number of benchmarks and a wide range of workloads, including an AVL tree-based micro-benchmark and ccTSA, a real sequence assembler application.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {19},
numpages = {12},
keywords = {concurrency, hardware transactional memory, transactional lock elision},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851143,
author = {Cao, Man and Zhang, Minjia and Sengupta, Aritra and Bond, Michael D.},
title = {Drinking from Both Glasses: Combining Pessimistic and Optimistic Tracking of Cross-Thread Dependences},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851143},
doi = {10.1145/3016078.2851143},
abstract = {It is notoriously challenging to develop parallel software systems that are both scalable and correct. Runtime support for parallelism---such as multithreaded record &amp; replay, data race detectors, transactional memory, and enforcement of stronger memory models---helps achieve these goals, but existing commodity solutions slow programs substantially in order to track (i.e., detect or control) an execution's cross-thread dependences accurately. Prior work tracks cross-thread dependences either "pessimistically," slowing every program access, or "optimistically," allowing for lightweight instrumentation of most accesses but dramatically slowing accesses involved in cross-thread dependences.This paper seeks to hybridize pessimistic and optimistic tracking, which is challenging because there exists a fundamental mismatch between pessimistic and optimistic tracking. We address this challenge based on insights about how dependence tracking and program synchronization interact, and introduce a novel approach called hybrid tracking. Hybrid tracking is suitable for building efficient runtime support, which we demonstrate by building hybrid-tracking-based versions of a dependence recorder and a region serializability enforcer. An adaptive, profile-based policy makes runtime decisions about switching between pessimistic and optimistic tracking. Our evaluation shows that hybrid tracking enables runtime support to overcome the performance limitations of both pessimistic and optimistic tracking alone.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {20},
numpages = {13}
}

@inproceedings{10.1145/2851141.2851143,
author = {Cao, Man and Zhang, Minjia and Sengupta, Aritra and Bond, Michael D.},
title = {Drinking from Both Glasses: Combining Pessimistic and Optimistic Tracking of Cross-Thread Dependences},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851143},
doi = {10.1145/2851141.2851143},
abstract = {It is notoriously challenging to develop parallel software systems that are both scalable and correct. Runtime support for parallelism---such as multithreaded record &amp; replay, data race detectors, transactional memory, and enforcement of stronger memory models---helps achieve these goals, but existing commodity solutions slow programs substantially in order to track (i.e., detect or control) an execution's cross-thread dependences accurately. Prior work tracks cross-thread dependences either "pessimistically," slowing every program access, or "optimistically," allowing for lightweight instrumentation of most accesses but dramatically slowing accesses involved in cross-thread dependences.This paper seeks to hybridize pessimistic and optimistic tracking, which is challenging because there exists a fundamental mismatch between pessimistic and optimistic tracking. We address this challenge based on insights about how dependence tracking and program synchronization interact, and introduce a novel approach called hybrid tracking. Hybrid tracking is suitable for building efficient runtime support, which we demonstrate by building hybrid-tracking-based versions of a dependence recorder and a region serializability enforcer. An adaptive, profile-based policy makes runtime decisions about switching between pessimistic and optimistic tracking. Our evaluation shows that hybrid tracking enables runtime support to overcome the performance limitations of both pessimistic and optimistic tracking alone.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {20},
numpages = {13},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851160,
author = {Wang, Tianzheng and Chabbi, Milind and Kimura, Hideaki},
title = {Be My Guest: MCS Lock Now Welcomes Guests},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851160},
doi = {10.1145/3016078.2851160},
abstract = {The MCS lock is one of the most prevalent queuing locks. It provides fair scheduling and high performance on massively parallel systems. However, the MCS lock mandates a bring-your-own-context policy: each lock user must provide an additional context (i.e., a queue node) to interact with the lock. This paper proposes MCSg, a variant of the MCS lock that relaxes this restriction.Our key observation is that not all lock users are created equal. We analyzed how locks are used in massively-parallel modern systems, such as NUMA-aware operating systems and databases. We found that such systems often have a small number of "regular" code paths that enter the lock very frequently. Such code paths are the primary beneficiary of the high scalability of MCS locks.However, there are also many "guest" code paths that infrequently enter the lock and do not need the same degree of fairness to access the lock (e.g., background tasks that only run periodically with lower priority). These guest users, which are typically spread out in various modules of the software, prefer context-free locks, such as ticket locks.MCSg provides these guests a context-free interface while regular users still enjoy the benefits provided by MCS. It can also be used as a drop-in replacement of MCS for more advanced locks, such as cohort locking. We also propose MCSg++, an extended version of MCSg, which avoids guest starvation and non-FIFO behaviors that might happen with MCSg.Our evaluation using microbenchmarks and the TPC-C database benchmark on a 16-socket, 240-core server shows that both MCSg and MCSg++ preserve the benefits of MCS for regular users while providing a context-free interface for guests.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {21},
numpages = {12},
keywords = {throughput, fairness, MCS, latency, locking API, queued locks, scalability, spin locks}
}

@inproceedings{10.1145/2851141.2851160,
author = {Wang, Tianzheng and Chabbi, Milind and Kimura, Hideaki},
title = {Be My Guest: MCS Lock Now Welcomes Guests},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851160},
doi = {10.1145/2851141.2851160},
abstract = {The MCS lock is one of the most prevalent queuing locks. It provides fair scheduling and high performance on massively parallel systems. However, the MCS lock mandates a bring-your-own-context policy: each lock user must provide an additional context (i.e., a queue node) to interact with the lock. This paper proposes MCSg, a variant of the MCS lock that relaxes this restriction.Our key observation is that not all lock users are created equal. We analyzed how locks are used in massively-parallel modern systems, such as NUMA-aware operating systems and databases. We found that such systems often have a small number of "regular" code paths that enter the lock very frequently. Such code paths are the primary beneficiary of the high scalability of MCS locks.However, there are also many "guest" code paths that infrequently enter the lock and do not need the same degree of fairness to access the lock (e.g., background tasks that only run periodically with lower priority). These guest users, which are typically spread out in various modules of the software, prefer context-free locks, such as ticket locks.MCSg provides these guests a context-free interface while regular users still enjoy the benefits provided by MCS. It can also be used as a drop-in replacement of MCS for more advanced locks, such as cohort locking. We also propose MCSg++, an extended version of MCSg, which avoids guest starvation and non-FIFO behaviors that might happen with MCSg.Our evaluation using microbenchmarks and the TPC-C database benchmark on a 16-socket, 240-core server shows that both MCSg and MCSg++ preserve the benefits of MCS for regular users while providing a context-free interface for guests.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {21},
numpages = {12},
keywords = {queued locks, scalability, throughput, fairness, spin locks, locking API, MCS, latency},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851166,
author = {Chabbi, Milind and Mellor-Crummey, John},
title = {Contention-Conscious, Locality-Preserving Locks},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851166},
doi = {10.1145/3016078.2851166},
abstract = {Over the last decade, the growing use of cache-coherent NUMA architectures has spurred the development of numerous locality-preserving mutual exclusion algorithms. NUMA-aware locks such as HCLH, HMCS, and cohort locks exploit locality of reference among nearby threads to deliver high lock throughput under high contention. However, the hierarchical nature of these locality-aware locks increases latency, which reduces the throughput of uncontended or lightly-contended critical sections. To date, no lock design for NUMA systems has delivered both low latency under low contention and high throughput under high contention.In this paper, we describe the design and evaluation of an adaptive mutual exclusion scheme (AHMCS lock), which employs several orthogonal strategies---a hierarchical MCS (HMCS) lock for high throughput under high contention, Lamport's fast path approach for low latency under low contention, an adaptation mechanism that employs hysteresis to balance latency and throughput under moderate contention, and hardware transactional memory for lowest latency in the absence of contention. The result is a top performing lock that has most properties of an ideal mutual exclusion algorithm. AHMCS exploits the strengths of multiple contention management techniques to deliver high performance over a broad range of contention levels. Our empirical evaluations demonstrate the effectiveness of AHMCS over prior art.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {22},
numpages = {14},
keywords = {dynamic locks, hierarchical locks, spin locks, NUMA}
}

@inproceedings{10.1145/2851141.2851166,
author = {Chabbi, Milind and Mellor-Crummey, John},
title = {Contention-Conscious, Locality-Preserving Locks},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851166},
doi = {10.1145/2851141.2851166},
abstract = {Over the last decade, the growing use of cache-coherent NUMA architectures has spurred the development of numerous locality-preserving mutual exclusion algorithms. NUMA-aware locks such as HCLH, HMCS, and cohort locks exploit locality of reference among nearby threads to deliver high lock throughput under high contention. However, the hierarchical nature of these locality-aware locks increases latency, which reduces the throughput of uncontended or lightly-contended critical sections. To date, no lock design for NUMA systems has delivered both low latency under low contention and high throughput under high contention.In this paper, we describe the design and evaluation of an adaptive mutual exclusion scheme (AHMCS lock), which employs several orthogonal strategies---a hierarchical MCS (HMCS) lock for high throughput under high contention, Lamport's fast path approach for low latency under low contention, an adaptation mechanism that employs hysteresis to balance latency and throughput under moderate contention, and hardware transactional memory for lowest latency in the absence of contention. The result is a top performing lock that has most properties of an ideal mutual exclusion algorithm. AHMCS exploits the strengths of multiple contention management techniques to deliver high performance over a broad range of contention levels. Our empirical evaluations demonstrate the effectiveness of AHMCS over prior art.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {22},
numpages = {14},
keywords = {dynamic locks, NUMA, hierarchical locks, spin locks},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851164,
author = {Kalikar, Saurabh and Nasre, Rupesh},
title = {DomLock: A New Multi-Granularity Locking Technique for Hierarchies},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851164},
doi = {10.1145/3016078.2851164},
abstract = {We present efficient locking mechanisms for hierarchical data structures. Several applications work on an abstract hierarchy of objects, and a parallel execution on this hierarchy necessitates synchronization across workers operating on different parts of the hierarchy. Existing synchronization mechanisms are either too coarse, too inefficient, or too ad hoc, resulting in reduced or unpredictable amount of concurrency. We propose a new locking approach based on the structural properties of the underlying hierarchy. We show that the developed techniques are efficient even when the hierarchy is an arbitrary graph, and are applicable even when the hierarchy involves mutation. Theoretically, we present our approach as a locking-cost-minimizing instance of a generic algebraic model of synchronization for hierarchical data structures. Using STMBench7, we illustrate considerable reduction in the locking cost, resulting in an average throughput improvement of 42%.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {23},
numpages = {12},
keywords = {dominators, graphs, hierarchical data structure, object graphs, locking, synchronization, trees}
}

@inproceedings{10.1145/2851141.2851164,
author = {Kalikar, Saurabh and Nasre, Rupesh},
title = {DomLock: A New Multi-Granularity Locking Technique for Hierarchies},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851164},
doi = {10.1145/2851141.2851164},
abstract = {We present efficient locking mechanisms for hierarchical data structures. Several applications work on an abstract hierarchy of objects, and a parallel execution on this hierarchy necessitates synchronization across workers operating on different parts of the hierarchy. Existing synchronization mechanisms are either too coarse, too inefficient, or too ad hoc, resulting in reduced or unpredictable amount of concurrency. We propose a new locking approach based on the structural properties of the underlying hierarchy. We show that the developed techniques are efficient even when the hierarchy is an arbitrary graph, and are applicable even when the hierarchy involves mutation. Theoretically, we present our approach as a locking-cost-minimizing instance of a generic algebraic model of synchronization for hierarchical data structures. Using STMBench7, we illustrate considerable reduction in the locking cost, resulting in an average throughput improvement of 42%.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {23},
numpages = {12},
keywords = {object graphs, trees, locking, hierarchical data structure, synchronization, graphs, dominators},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851150,
author = {Ritson, Carl G. and Owens, Scott},
title = {Benchmarking Weak Memory Models},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851150},
doi = {10.1145/3016078.2851150},
abstract = {To achieve good multi-core performance, modern microprocessors have weak memory models, rather than enforce sequential consistency. This gives the programmer a wide scope for choosing exactly how to implement various aspects of inter-thread communication through the system's shared memory. However, these choices come with both semantic and performance consequences, often in tension with each other. In this paper, we focus on the performance side, and define techniques for evaluating the impact of various choices in using weak memory models, such as where to put fences, and which fences to use. We make no attempt to judge certain strategies as best or most efficient, and instead provide the techniques that will allow the programmer to understand the performance implications when identifying and resolving any semantic/performance trade-offs. In particular, our technique supports the reasoned selection of macrobenchmarks to use in investigating trade-offs in using weak memory models. We demonstrate our technique on both synthetic benchmarks and real-world applications for the Linux Kernel and OpenJDK Hotspot Virtual Machine on the ARMv8 and POWERv7 architectures.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {24},
numpages = {11},
keywords = {benchmarking, memory models, concurrency, performance}
}

@inproceedings{10.1145/2851141.2851150,
author = {Ritson, Carl G. and Owens, Scott},
title = {Benchmarking Weak Memory Models},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851150},
doi = {10.1145/2851141.2851150},
abstract = {To achieve good multi-core performance, modern microprocessors have weak memory models, rather than enforce sequential consistency. This gives the programmer a wide scope for choosing exactly how to implement various aspects of inter-thread communication through the system's shared memory. However, these choices come with both semantic and performance consequences, often in tension with each other. In this paper, we focus on the performance side, and define techniques for evaluating the impact of various choices in using weak memory models, such as where to put fences, and which fences to use. We make no attempt to judge certain strategies as best or most efficient, and instead provide the techniques that will allow the programmer to understand the performance implications when identifying and resolving any semantic/performance trade-offs. In particular, our technique supports the reasoned selection of macrobenchmarks to use in investigating trade-offs in using weak memory models. We demonstrate our technique on both synthetic benchmarks and real-world applications for the Linux Kernel and OpenJDK Hotspot Virtual Machine on the ARMv8 and POWERv7 architectures.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {24},
numpages = {11},
keywords = {benchmarking, concurrency, memory models, performance},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851165,
author = {Narayanaswamy, Ganesh and Joshi, Saurabh and Kroening, Daniel},
title = {The Virtues of Conflict: Analysing Modern Concurrency},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851165},
doi = {10.1145/3016078.2851165},
abstract = {Modern shared memory multiprocessors permit reordering of memory operations for performance reasons. These reorderings are often a source of subtle bugs in programs written for such architectures. Traditional approaches to verify weak memory programs often rely on interleaving semantics, which is prone to state space explosion, and thus severely limits the scalability of the analysis. In recent times, there has been a renewed interest in modelling dynamic executions of weak memory programs using partial orders. However, such an approach typically requires ad-hoc mechanisms to correctly capture the data and control-flow choices/conflicts present in real-world programs. In this work, we propose a novel, conflict-aware, composable, truly concurrent semantics for programs written using C/C++ for modern weak memory architectures. We exploit our symbolic semantics based on general event structures to build an efficient decision procedure that detects assertion violations in bounded multi-threaded programs. Using a large, representative set of benchmarks, we show that our conflict-aware semantics outperforms the state-of-the-art partial-order based approaches.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {25},
numpages = {12},
keywords = {verification, software, concurrency, weak consistency models}
}

@inproceedings{10.1145/2851141.2851165,
author = {Narayanaswamy, Ganesh and Joshi, Saurabh and Kroening, Daniel},
title = {The Virtues of Conflict: Analysing Modern Concurrency},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851165},
doi = {10.1145/2851141.2851165},
abstract = {Modern shared memory multiprocessors permit reordering of memory operations for performance reasons. These reorderings are often a source of subtle bugs in programs written for such architectures. Traditional approaches to verify weak memory programs often rely on interleaving semantics, which is prone to state space explosion, and thus severely limits the scalability of the analysis. In recent times, there has been a renewed interest in modelling dynamic executions of weak memory programs using partial orders. However, such an approach typically requires ad-hoc mechanisms to correctly capture the data and control-flow choices/conflicts present in real-world programs. In this work, we propose a novel, conflict-aware, composable, truly concurrent semantics for programs written using C/C++ for modern weak memory architectures. We exploit our symbolic semantics based on general event structures to build an efficient decision procedure that detects assertion violations in bounded multi-threaded programs. Using a large, representative set of benchmarks, we show that our conflict-aware semantics outperforms the state-of-the-art partial-order based approaches.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {25},
numpages = {12},
keywords = {verification, concurrency, software, weak consistency models},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851170,
author = {Perrin, Matthieu and Mostefaoui, Achour and Jard, Claude},
title = {Causal Consistency: Beyond Memory},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851170},
doi = {10.1145/3016078.2851170},
abstract = {In distributed systems where strong consistency is costly when not impossible, causal consistency provides a valuable abstraction to represent program executions as partial orders. In addition to the sequential program order of each computing entity, causal order also contains the semantic links between the events that affect the shared objects -- messages emission and reception in a communication channel, reads and writes on a shared register. Usual approaches based on semantic links are very difficult to adapt to other data types such as queues or counters because they require a specific analysis of causal dependencies for each data type. This paper presents a new approach to define causal consistency for any abstract data type based on sequential specifications. It explores, formalizes and studies the differences between three variations of causal consistency and highlights them in the light of PRAM, eventual consistency and sequential consistency: weak causal consistency, that captures the notion of causality preservation when focusing on convergence; causal convergence that mixes weak causal consistency and convergence; and causal consistency, that coincides with causal memory when applied to shared memory.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {26},
numpages = {12},
keywords = {weak causal consistency, pipelined consistency, consistency criteria, causal consistency, sequential consistency, shared objects}
}

@inproceedings{10.1145/2851141.2851170,
author = {Perrin, Matthieu and Mostefaoui, Achour and Jard, Claude},
title = {Causal Consistency: Beyond Memory},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851170},
doi = {10.1145/2851141.2851170},
abstract = {In distributed systems where strong consistency is costly when not impossible, causal consistency provides a valuable abstraction to represent program executions as partial orders. In addition to the sequential program order of each computing entity, causal order also contains the semantic links between the events that affect the shared objects -- messages emission and reception in a communication channel, reads and writes on a shared register. Usual approaches based on semantic links are very difficult to adapt to other data types such as queues or counters because they require a specific analysis of causal dependencies for each data type. This paper presents a new approach to define causal consistency for any abstract data type based on sequential specifications. It explores, formalizes and studies the differences between three variations of causal consistency and highlights them in the light of PRAM, eventual consistency and sequential consistency: weak causal consistency, that captures the notion of causality preservation when focusing on convergence; causal convergence that mixes weak causal consistency and convergence; and causal consistency, that coincides with causal memory when applied to shared memory.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {26},
numpages = {12},
keywords = {weak causal consistency, causal consistency, consistency criteria, pipelined consistency, sequential consistency, shared objects},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851159,
author = {Chatzopoulos, Georgios and Dragojevi\'{c}, Aleksandar and Guerraoui, Rachid},
title = {ESTIMA: Extrapolating Scalability of in-Memory Applications},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851159},
doi = {10.1145/3016078.2851159},
abstract = {This paper presents ESTIMA, an easy-to-use tool for extrapolating the scalability of in-memory applications. ESTIMA is designed to perform a simple, yet important task: given the performance of an application on a small machine with a handful of cores, ESTIMA extrapolates its scalability to a larger machine with more cores, while requiring minimum input from the user. The key idea underlying ESTIMA is the use of stalled cycles (e.g. cycles that the processor spends waiting for various events, such as cache misses or waiting on a lock). ESTIMA measures stalled cycles on a few cores and extrapolates them to more cores, estimating the amount of waiting in the system. ESTIMA can be effectively used to predict the scalability of in-memory applications. For instance, using measurements of memcached and SQLite on a desktop machine, we obtain accurate predictions of their scalability on a server. Our extensive evaluation on a large number of in-memory benchmarks shows that ESTIMA has generally low prediction errors.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {27},
numpages = {11}
}

@inproceedings{10.1145/2851141.2851159,
author = {Chatzopoulos, Georgios and Dragojevi\'{c}, Aleksandar and Guerraoui, Rachid},
title = {ESTIMA: Extrapolating Scalability of in-Memory Applications},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851159},
doi = {10.1145/2851141.2851159},
abstract = {This paper presents ESTIMA, an easy-to-use tool for extrapolating the scalability of in-memory applications. ESTIMA is designed to perform a simple, yet important task: given the performance of an application on a small machine with a handful of cores, ESTIMA extrapolates its scalability to a larger machine with more cores, while requiring minimum input from the user. The key idea underlying ESTIMA is the use of stalled cycles (e.g. cycles that the processor spends waiting for various events, such as cache misses or waiting on a lock). ESTIMA measures stalled cycles on a few cores and extrapolates them to more cores, estimating the amount of waiting in the system. ESTIMA can be effectively used to predict the scalability of in-memory applications. For instance, using measurements of memcached and SQLite on a desktop machine, we obtain accurate predictions of their scalability on a server. Our extensive evaluation on a large number of in-memory benchmarks shows that ESTIMA has generally low prediction errors.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {27},
numpages = {11},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851156,
author = {Muddukrishna, Ananya and Jonsson, Peter A. and Podobas, Artur and Brorsson, Mats},
title = {Grain Graphs: OpenMP Performance Analysis Made Easy},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851156},
doi = {10.1145/3016078.2851156},
abstract = {Average programmers struggle to solve performance problems in OpenMP programs with tasks and parallel for-loops. Existing performance analysis tools visualize OpenMP task performance from the runtime system's perspective where task execution is interleaved with other tasks in an unpredictable order. Problems with OpenMP parallel for-loops are similarly difficult to resolve since tools only visualize aggregate thread-level statistics such as load imbalance without zooming into a per-chunk granularity. The runtime system/threads oriented visualization provides poor support for understanding problems with task and chunk execution time, parallelism, and memory hierarchy utilization, forcing average programmers to rely on experts or use tedious trial-and-error tuning methods for performance. We present grain graphs, a new OpenMP performance analysis method that visualizes grains -- computation performed by a task or a parallel for-loop chunk instance -- and highlights problems such as low parallelism, work inflation and poor parallelization benefit at the grain level. We demonstrate that grain graphs can quickly reveal performance problems that are difficult to detect and characterize in fine detail using existing visualizations in standard OpenMP programs, simplifying OpenMP performance analysis. This enables average programmers to make portable optimizations for poor performing OpenMP programs, reducing pressure on experts and removing the need for tedious trial-and-error tuning.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {28},
numpages = {13},
keywords = {task-based programs, OpenMP, performance visualization, performance analysis}
}

@inproceedings{10.1145/2851141.2851156,
author = {Muddukrishna, Ananya and Jonsson, Peter A. and Podobas, Artur and Brorsson, Mats},
title = {Grain Graphs: OpenMP Performance Analysis Made Easy},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851156},
doi = {10.1145/2851141.2851156},
abstract = {Average programmers struggle to solve performance problems in OpenMP programs with tasks and parallel for-loops. Existing performance analysis tools visualize OpenMP task performance from the runtime system's perspective where task execution is interleaved with other tasks in an unpredictable order. Problems with OpenMP parallel for-loops are similarly difficult to resolve since tools only visualize aggregate thread-level statistics such as load imbalance without zooming into a per-chunk granularity. The runtime system/threads oriented visualization provides poor support for understanding problems with task and chunk execution time, parallelism, and memory hierarchy utilization, forcing average programmers to rely on experts or use tedious trial-and-error tuning methods for performance. We present grain graphs, a new OpenMP performance analysis method that visualizes grains -- computation performed by a task or a parallel for-loop chunk instance -- and highlights problems such as low parallelism, work inflation and poor parallelization benefit at the grain level. We demonstrate that grain graphs can quickly reveal performance problems that are difficult to detect and characterize in fine detail using existing visualizations in standard OpenMP programs, simplifying OpenMP performance analysis. This enables average programmers to make portable optimizations for poor performing OpenMP programs, reducing pressure on experts and removing the need for tedious trial-and-error tuning.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {28},
numpages = {13},
keywords = {performance analysis, task-based programs, performance visualization, OpenMP},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851149,
author = {Machado, Nuno and Lucia, Brandon and Rodrigues, Lu\'{\i}s},
title = {Production-Guided Concurrency Debugging},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851149},
doi = {10.1145/3016078.2851149},
abstract = {Concurrency bugs that stem from schedule-dependent branches are hard to understand and debug, because their root causes imply not only different event orderings, but also changes in the control-flow between failing and non-failing executions. We present Cortex: a system that helps exposing and understanding concurrency bugs that result from schedule-dependent branches, without relying on information from failing executions. Cortex preemptively exposes failing executions by perturbing the order of events and control-flow behavior in non-failing schedules from production runs of a program. By leveraging this information from production runs, Cortex synthesizes executions to guide the search for failing schedules. Production-guided search helps cope with the large execution search space by targeting failing executions that are similar to observed non-failing executions. Evaluation on popular benchmarks shows that Cortex is able to expose failing schedules with only a few perturbations to non-failing executions, and takes a practical amount of time.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {29},
numpages = {12}
}

@inproceedings{10.1145/2851141.2851149,
author = {Machado, Nuno and Lucia, Brandon and Rodrigues, Lu\'{\i}s},
title = {Production-Guided Concurrency Debugging},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851149},
doi = {10.1145/2851141.2851149},
abstract = {Concurrency bugs that stem from schedule-dependent branches are hard to understand and debug, because their root causes imply not only different event orderings, but also changes in the control-flow between failing and non-failing executions. We present Cortex: a system that helps exposing and understanding concurrency bugs that result from schedule-dependent branches, without relying on information from failing executions. Cortex preemptively exposes failing executions by perturbing the order of events and control-flow behavior in non-failing schedules from production runs of a program. By leveraging this information from production runs, Cortex synthesizes executions to guide the search for failing schedules. Production-guided search helps cope with the large execution search space by targeting failing executions that are similar to observed non-failing executions. Evaluation on popular benchmarks shows that Cortex is able to expose failing schedules with only a few perturbations to non-failing executions, and takes a practical amount of time.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {29},
numpages = {12},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851194,
author = {Farooqui, Naila and Barik, Rajkishore and Lewis, Brian T. and Shpeisman, Tatiana and Schwan, Karsten},
title = {Affinity-Aware Work-Stealing for Integrated CPU-GPU Processors},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851194},
doi = {10.1145/3016078.2851194},
abstract = {Recent integrated CPU-GPU processors like Intel's Broadwell and AMD's Kaveri support hardware CPU-GPU shared virtual memory, atomic operations, and memory coherency. This enables fine-grained CPU-GPU work-stealing, but architectural differences between the CPU and GPU hurt the performance of traditionally-implemented work-stealing on such processors. These architectural differences include different clock frequencies, atomic operation costs, and cache and shared memory latencies. This paper describes a preliminary implementation of our work-stealing scheduler, Libra, which includes techniques to deal with these architectural differences in integrated CPU-GPU processors. Libra's affinity-aware techniques achieve significant performance gains over classically-implemented work-stealing. We show preliminary results using a diverse set of nine regular and irregular workloads running on an Intel Broadwell Core-M processor. Libra currently achieves up to a 2\texttimes{} performance improvement over classical work-stealing, with a 20% average improvement.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {30},
numpages = {2}
}

@inproceedings{10.1145/2851141.2851194,
author = {Farooqui, Naila and Barik, Rajkishore and Lewis, Brian T. and Shpeisman, Tatiana and Schwan, Karsten},
title = {Affinity-Aware Work-Stealing for Integrated CPU-GPU Processors},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851194},
doi = {10.1145/2851141.2851194},
abstract = {Recent integrated CPU-GPU processors like Intel's Broadwell and AMD's Kaveri support hardware CPU-GPU shared virtual memory, atomic operations, and memory coherency. This enables fine-grained CPU-GPU work-stealing, but architectural differences between the CPU and GPU hurt the performance of traditionally-implemented work-stealing on such processors. These architectural differences include different clock frequencies, atomic operation costs, and cache and shared memory latencies. This paper describes a preliminary implementation of our work-stealing scheduler, Libra, which includes techniques to deal with these architectural differences in integrated CPU-GPU processors. Libra's affinity-aware techniques achieve significant performance gains over classically-implemented work-stealing. We show preliminary results using a diverse set of nine regular and irregular workloads running on an Intel Broadwell Core-M processor. Libra currently achieves up to a 2\texttimes{} performance improvement over classical work-stealing, with a 20% average improvement.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {30},
numpages = {2},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851195,
author = {Gindraud, Fran\c{c}ois and Rastello, Fabrice and Cohen, Albert and Broquedis, Fran\c{c}ois},
title = {An Interval Constrained Memory Allocator for the Givy GAS Runtime},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851195},
doi = {10.1145/3016078.2851195},
abstract = {The shared memory model helps parallel programming productivity, but it also has a high hardware cost and imposes scalability constraints. Ultimately, higher performance will use distributed memories, which scales better but requires programmers to manually transfer data between local memories, which is a complex task. Distributed memories are also more energy efficient than shared memories, and are used in a family of embedded computing solutions called multi processor system on chip (MPSoC).},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {31},
numpages = {2},
keywords = {distributed shared memory, memory allocation, global address space}
}

@inproceedings{10.1145/2851141.2851195,
author = {Gindraud, Fran\c{c}ois and Rastello, Fabrice and Cohen, Albert and Broquedis, Fran\c{c}ois},
title = {An Interval Constrained Memory Allocator for the Givy GAS Runtime},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851195},
doi = {10.1145/2851141.2851195},
abstract = {The shared memory model helps parallel programming productivity, but it also has a high hardware cost and imposes scalability constraints. Ultimately, higher performance will use distributed memories, which scales better but requires programmers to manually transfer data between local memories, which is a complex task. Distributed memories are also more energy efficient than shared memories, and are used in a family of embedded computing solutions called multi processor system on chip (MPSoC).},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {31},
numpages = {2},
keywords = {global address space, memory allocation, distributed shared memory},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851178,
author = {Chang, Li-Wen and El Hajj, Izzat and Kim, Hee-Seok and G\'{o}mez-Luna, Juan and Dakkak, Abdul and Hwu, Wen-mei},
title = {A Programming System for Future Proofing Performance Critical Libraries},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851178},
doi = {10.1145/3016078.2851178},
abstract = {We present Tangram, a programming system for writing performance-portable programs. The language enables programmers to write computation and composition codelets, supported by tuning knobs and primitives for expressing data parallelism and work decomposition. The compiler and runtime use a set of techniques such as hierarchical composition, coarsening, data placement, tuning, and runtime selection based on input characteristics and micro-profiling. The resulting performance is competitive with optimized vendor libraries.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {32},
numpages = {2}
}

@inproceedings{10.1145/2851141.2851178,
author = {Chang, Li-Wen and El Hajj, Izzat and Kim, Hee-Seok and G\'{o}mez-Luna, Juan and Dakkak, Abdul and Hwu, Wen-mei},
title = {A Programming System for Future Proofing Performance Critical Libraries},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851178},
doi = {10.1145/2851141.2851178},
abstract = {We present Tangram, a programming system for writing performance-portable programs. The language enables programmers to write computation and composition codelets, supported by tuning knobs and primitives for expressing data parallelism and work decomposition. The compiler and runtime use a set of techniques such as hierarchical composition, coarsening, data placement, tuning, and runtime selection based on input characteristics and micro-profiling. The resulting performance is competitive with optimized vendor libraries.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {32},
numpages = {2},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851196,
author = {Nielsen, Jesper Puge and Karlsson, Sven},
title = {A Scalable Lock-Free Hash Table with Open Addressing},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851196},
doi = {10.1145/3016078.2851196},
abstract = {Concurrent data structures synchronized with locks do not scale well with the number of threads. As more scalable alternatives, concurrent data structures and algorithms based on widely available, however advanced, atomic operations have been proposed. These data structures allow for correct and concurrent operations without any locks. In this paper, we present a new fully lock-free open addressed hash table with a simpler design than prior published work. We split hash table insertions into two atomic phases: first inserting a value ignoring other concurrent operations, then in the second phase resolve any duplicate or conflicting values.Our hash table has a constant and low memory usage that is less than existing lock-free hash tables at a fill level of 33% and above. The hash table exhibits good cache locality. Compared to prior art, our hash table results in 16% and 15% fewer L1 and L2 cache misses respectively, leading to 21% fewer memory stall cycles. Our experiments show that our hash table scales close to linearly with the number of threads and outperforms, in throughput, other lock-free hash tables by 19%.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {33},
numpages = {2}
}

@inproceedings{10.1145/2851141.2851196,
author = {Nielsen, Jesper Puge and Karlsson, Sven},
title = {A Scalable Lock-Free Hash Table with Open Addressing},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851196},
doi = {10.1145/2851141.2851196},
abstract = {Concurrent data structures synchronized with locks do not scale well with the number of threads. As more scalable alternatives, concurrent data structures and algorithms based on widely available, however advanced, atomic operations have been proposed. These data structures allow for correct and concurrent operations without any locks. In this paper, we present a new fully lock-free open addressed hash table with a simpler design than prior published work. We split hash table insertions into two atomic phases: first inserting a value ignoring other concurrent operations, then in the second phase resolve any duplicate or conflicting values.Our hash table has a constant and low memory usage that is less than existing lock-free hash tables at a fill level of 33% and above. The hash table exhibits good cache locality. Compared to prior art, our hash table results in 16% and 15% fewer L1 and L2 cache misses respectively, leading to 21% fewer memory stall cycles. Our experiments show that our hash table scales close to linearly with the number of threads and outperforms, in throughput, other lock-free hash tables by 19%.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {33},
numpages = {2},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851188,
author = {Maier, Tobias and Sanders, Peter and Dementiev, Roman},
title = {Concurrent Hash Tables: Fast <i>and</i> General?(!)},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851188},
doi = {10.1145/3016078.2851188},
abstract = {Concurrent hash tables are one of the most important concurrent data structures with numerous applications. Since hash table accesses can dominate the execution time of the overall application, we need implementations that achieve good speedup. Unfortunately, currently available concurrent hashing libraries turn out to be far away from this requirement in particular when contention on some elements occurs.Our starting point for better performing data structures is a fast and simple lock-free concurrent hash table based on linear probing that is limited to word-sized key-value types and does not support dynamic size adaptation. We explain how to lift these limitations in a provably scalable way and demonstrate that dynamic growing has a performance overhead comparable to the same generalization in sequential hash tables.We perform extensive experiments comparing the performance of our implementations with six of the most widely used concurrent hash tables. Ours are considerably faster than the best algorithms with similar restrictions and an order of magnitude faster than the best more general tables. In some extreme cases, the difference even approaches four orders of magnitude.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {34},
numpages = {2},
keywords = {experimental, concurrency, lock-freedom}
}

@inproceedings{10.1145/2851141.2851188,
author = {Maier, Tobias and Sanders, Peter and Dementiev, Roman},
title = {Concurrent Hash Tables: Fast <i>and</i> General?(!)},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851188},
doi = {10.1145/2851141.2851188},
abstract = {Concurrent hash tables are one of the most important concurrent data structures with numerous applications. Since hash table accesses can dominate the execution time of the overall application, we need implementations that achieve good speedup. Unfortunately, currently available concurrent hashing libraries turn out to be far away from this requirement in particular when contention on some elements occurs.Our starting point for better performing data structures is a fast and simple lock-free concurrent hash table based on linear probing that is limited to word-sized key-value types and does not support dynamic size adaptation. We explain how to lift these limitations in a provably scalable way and demonstrate that dynamic growing has a performance overhead comparable to the same generalization in sequential hash tables.We perform extensive experiments comparing the performance of our implementations with six of the most widely used concurrent hash tables. Ours are considerably faster than the best algorithms with similar restrictions and an order of magnitude faster than the best more general tables. In some extreme cases, the difference even approaches four orders of magnitude.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {34},
numpages = {2},
keywords = {experimental, lock-freedom, concurrency},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851181,
author = {Prades, Javier and Rea\~{n}o, Carlos and Silla, Federico},
title = {CUDA Acceleration for Xen Virtual Machines in Infiniband Clusters with RCUDA},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851181},
doi = {10.1145/3016078.2851181},
abstract = {Many data centers currently use virtual machines (VMs) to achieve a more efficient usage of hardware resources. However, current virtualization solutions, such as Xen, do not easily provide graphics processing unit (GPU) accelerators to applications running in the virtualized domain with the flexibility usually required in data centers (i.e., managing virtual GPU instances and concurrently sharing them among several VMs). Remote GPU virtualization frameworks such as the rCUDA solution may address this problem.In this work we analyze the use of the rCUDA framework to accelerate scientific applications running inside Xen VMs. Results show that the use of the rCUDA framework is a feasible approach, featuring a very low overhead if an InfiniBand fabric is already present in the cluster.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {35},
numpages = {2},
keywords = {CUDA, HPC, InfiniBand, virtualization, rCUDA, xen}
}

@inproceedings{10.1145/2851141.2851181,
author = {Prades, Javier and Rea\~{n}o, Carlos and Silla, Federico},
title = {CUDA Acceleration for Xen Virtual Machines in Infiniband Clusters with RCUDA},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851181},
doi = {10.1145/2851141.2851181},
abstract = {Many data centers currently use virtual machines (VMs) to achieve a more efficient usage of hardware resources. However, current virtualization solutions, such as Xen, do not easily provide graphics processing unit (GPU) accelerators to applications running in the virtualized domain with the flexibility usually required in data centers (i.e., managing virtual GPU instances and concurrently sharing them among several VMs). Remote GPU virtualization frameworks such as the rCUDA solution may address this problem.In this work we analyze the use of the rCUDA framework to accelerate scientific applications running inside Xen VMs. Results show that the use of the rCUDA framework is a feasible approach, featuring a very low overhead if an InfiniBand fabric is already present in the cluster.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {35},
numpages = {2},
keywords = {virtualization, InfiniBand, CUDA, HPC, rCUDA, xen},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851186,
author = {Umar, Ibrahim and Anshus, Otto J. and Ha, Phuong H.},
title = {Effect of Portable Fine-Grained Locality on Energy Efficiency and Performance in Concurrent Search Trees},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851186},
doi = {10.1145/3016078.2851186},
abstract = {Recent research has suggested that improving fine-grained data-locality is one of the main approaches to improving energy efficiency and performance. However, no previous research has investigated the effect of the approach on these metrices in the case of concurrent data structures.This paper investigates how fine-grained data locality influences energy efficiency and performance in concurrent search trees, a crucial data structure that is widely used in several important systems. We conduct a set of experiments on three lock-based concurrent search trees: DeltaTree, a portable fine-grained locality-aware concurrent search tree; CBTree, a coarse-grained locality-aware B+tree; and BST-TK, a locality-oblivious concurrent search tree. We run the experiments on a commodity x86 platform and an embedded ARM platform. The experimental results show that DeltaTree has 13--25% better energy efficiency and 10--22% more operations/second on the x86 and ARM platforms, respectively. The results confirm that portable fine-grained locality can improve energy efficiency and performance in concurrent search trees.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {36},
numpages = {2},
keywords = {energy efficient computing systems, performance evaluation, multicore processors, concurrent algorithms, data locality, power analysis, memory systems}
}

@inproceedings{10.1145/2851141.2851186,
author = {Umar, Ibrahim and Anshus, Otto J. and Ha, Phuong H.},
title = {Effect of Portable Fine-Grained Locality on Energy Efficiency and Performance in Concurrent Search Trees},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851186},
doi = {10.1145/2851141.2851186},
abstract = {Recent research has suggested that improving fine-grained data-locality is one of the main approaches to improving energy efficiency and performance. However, no previous research has investigated the effect of the approach on these metrices in the case of concurrent data structures.This paper investigates how fine-grained data locality influences energy efficiency and performance in concurrent search trees, a crucial data structure that is widely used in several important systems. We conduct a set of experiments on three lock-based concurrent search trees: DeltaTree, a portable fine-grained locality-aware concurrent search tree; CBTree, a coarse-grained locality-aware B+tree; and BST-TK, a locality-oblivious concurrent search tree. We run the experiments on a commodity x86 platform and an embedded ARM platform. The experimental results show that DeltaTree has 13--25% better energy efficiency and 10--22% more operations/second on the x86 and ARM platforms, respectively. The results confirm that portable fine-grained locality can improve energy efficiency and performance in concurrent search trees.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {36},
numpages = {2},
keywords = {energy efficient computing systems, data locality, power analysis, concurrent algorithms, memory systems, performance evaluation, multicore processors},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851175,
author = {Parikh, Hrushit and Deodhar, Vinit and Gavrilovska, Ada and Pande, Santosh},
title = {Efficient Distributed Workstealing via Matchmaking},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851175},
doi = {10.1145/3016078.2851175},
abstract = {Many classes of high-performance applications and combinatorial problems exhibit large degree of runtime load variability. One approach to achieving balanced resource use is to over decompose the problem on fine-grained tasks that are then dynamically balanced using approaches such as workstealing. Existing work stealing techniques for such irregular applications, running on large clusters, exhibit high overheads due to potential untimely interruption of busy nodes, excessive communication messages and delays experienced by idle nodes in finding work due to repeated failed steals. We contend that the fundamental problem of distributed work-stealing is of rapidly bringing together work producers and consumers. In response, we develop an algorithm that performs timely, lightweight and highly efficient matchmaking between work producers and consumers which results in accurate load balance. Experimental evaluations show that our scheduler is able to outperform other distributed work stealing schedulers, and to achieve scale beyond what is possible with current approaches.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {37},
numpages = {2},
keywords = {work-stealing, irregular applications, scheduling}
}

@inproceedings{10.1145/2851141.2851175,
author = {Parikh, Hrushit and Deodhar, Vinit and Gavrilovska, Ada and Pande, Santosh},
title = {Efficient Distributed Workstealing via Matchmaking},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851175},
doi = {10.1145/2851141.2851175},
abstract = {Many classes of high-performance applications and combinatorial problems exhibit large degree of runtime load variability. One approach to achieving balanced resource use is to over decompose the problem on fine-grained tasks that are then dynamically balanced using approaches such as workstealing. Existing work stealing techniques for such irregular applications, running on large clusters, exhibit high overheads due to potential untimely interruption of busy nodes, excessive communication messages and delays experienced by idle nodes in finding work due to repeated failed steals. We contend that the fundamental problem of distributed work-stealing is of rapidly bringing together work producers and consumers. In response, we develop an algorithm that performs timely, lightweight and highly efficient matchmaking between work producers and consumers which results in accurate load balance. Experimental evaluations show that our scheduler is able to outperform other distributed work stealing schedulers, and to achieve scale beyond what is possible with current approaches.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {37},
numpages = {2},
keywords = {scheduling, irregular applications, work-stealing},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851182,
author = {Luo, Hao and Chen, Guoyang and Li, Pengcheng and Ding, Chen and Shen, Xipeng},
title = {Data-Centric Combinatorial Optimization of Parallel Code},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851182},
doi = {10.1145/3016078.2851182},
abstract = {Memory performance is one essential factor for tapping into the full potential of the massive parallelism of GPU. It has motivated some recent efforts in GPU cache modeling. This paper presents a new data-centric way to model the performance of a system with heterogeneous memory resources. The new model is composable, meaning it can predict the performance difference due to placing data differently by profiling the execution just once.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {38},
numpages = {2},
keywords = {locality metrics, footprint, locality modeling}
}

@inproceedings{10.1145/2851141.2851182,
author = {Luo, Hao and Chen, Guoyang and Li, Pengcheng and Ding, Chen and Shen, Xipeng},
title = {Data-Centric Combinatorial Optimization of Parallel Code},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851182},
doi = {10.1145/2851141.2851182},
abstract = {Memory performance is one essential factor for tapping into the full potential of the massive parallelism of GPU. It has motivated some recent efforts in GPU cache modeling. This paper presents a new data-centric way to model the performance of a system with heterogeneous memory resources. The new model is composable, meaning it can predict the performance difference due to placing data differently by profiling the execution just once.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {38},
numpages = {2},
keywords = {locality metrics, locality modeling, footprint},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851183,
author = {Maleki, Saeed and Nguyen, Donald and Lenharth, Andrew and Garzar\'{a}n, Mar\'{\i}a and Padua, David and Pingali, Keshav},
title = {DSMR: A Shared and Distributed Memory Algorithm for Single-Source Shortest Path Problem},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851183},
doi = {10.1145/3016078.2851183},
abstract = {The Single-Source Shortest Path (SSSP) problem is to find the shortest paths from a source vertex to all other vertices in a graph. In this paper, we introduce the Dijkstra Strip-Mined Relaxation (DSMR) algorithm, an efficient parallel SSSP algorithm for shared and distributed memory systems. Our results show that, DSMR is faster than parallel Δ-Stepping by a factor of up-to 1.66.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {39},
numpages = {2}
}

@inproceedings{10.1145/2851141.2851183,
author = {Maleki, Saeed and Nguyen, Donald and Lenharth, Andrew and Garzar\'{a}n, Mar\'{\i}a and Padua, David and Pingali, Keshav},
title = {DSMR: A Shared and Distributed Memory Algorithm for Single-Source Shortest Path Problem},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851183},
doi = {10.1145/2851141.2851183},
abstract = {The Single-Source Shortest Path (SSSP) problem is to find the shortest paths from a source vertex to all other vertices in a graph. In this paper, we introduce the Dijkstra Strip-Mined Relaxation (DSMR) algorithm, an efficient parallel SSSP algorithm for shared and distributed memory systems. Our results show that, DSMR is faster than parallel Δ-Stepping by a factor of up-to 1.66.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {39},
numpages = {2},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851184,
author = {Salucci, Luca and Bonetta, Daniele and Marr, Stefan and Binder, Walter},
title = {Generic Messages: Capability-Based Shared Memory Parallelism for Event-Loop Systems},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851184},
doi = {10.1145/3016078.2851184},
abstract = {Systems based on event-loops have been popularized by Node.JS, and are becoming a key technology in the domain of cloud computing. Despite their popularity, such systems support only share-nothing parallelism via message passing between parallel entities usually called workers. In this paper, we introduce a novel parallel programming abstraction called Generic Messages (GEMs), which enables shared-memory parallelism for share-nothing event-based systems. A key characteristic of GEMs is that they enable workers to share state by specifying how the state can be accessed once it is shared. We call this aspect of the GEMs model capability-based parallelism.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {40},
numpages = {2},
keywords = {event-loop systems, Node.JS, shared memory, generic messages}
}

@inproceedings{10.1145/2851141.2851184,
author = {Salucci, Luca and Bonetta, Daniele and Marr, Stefan and Binder, Walter},
title = {Generic Messages: Capability-Based Shared Memory Parallelism for Event-Loop Systems},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851184},
doi = {10.1145/2851141.2851184},
abstract = {Systems based on event-loops have been popularized by Node.JS, and are becoming a key technology in the domain of cloud computing. Despite their popularity, such systems support only share-nothing parallelism via message passing between parallel entities usually called workers. In this paper, we introduce a novel parallel programming abstraction called Generic Messages (GEMs), which enables shared-memory parallelism for share-nothing event-based systems. A key characteristic of GEMs is that they enable workers to share state by specifying how the state can be accessed once it is shared. We call this aspect of the GEMs model capability-based parallelism.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {40},
numpages = {2},
keywords = {event-loop systems, shared memory, generic messages, Node.JS},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851174,
author = {Liu, Jianqiao and Hegde, Nikhil and Kulkarni, Milind},
title = {Hybrid CPU-GPU Scheduling and Execution of Tree Traversals},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851174},
doi = {10.1145/3016078.2851174},
abstract = {GPUs offer the promise of massive, power-efficient parallelism. However, exploiting this parallelism requires code to be carefully structured to deal with the limitations of the SIMT execution model. In recent years, there has been much interest in mapping irregular applications to GPUs: applications with unpredictable, data-dependent behaviors. While most of the work in this space has focused on ad hoc implementations of specific algorithms, recent work has looked at generic techniques for mapping a large class of tree traversal algorithms to GPUs, through careful restructuring of the tree traversal algorithms to make them behave more regularly. Unfortunately, even this general approach for GPU execution of tree traversal algorithms is reliant on ad hoc, handwritten, algorithm-specific scheduling (i.e., assignment of threads to warps) to achieve high performance.The key challenge of scheduling is that it is a highly irregular process, that requires the inspection of thread behavior and then careful sorting of the threads into warps. In this paper, we present a novel scheduling and execution technique for tree traversal algorithms that is both general and automatic. The key novelty is a hybrid approach: the GPU partially executes tasks to inspect thread behavior and transmits information back to the CPU, which uses that information to perform the scheduling itself, before executing the remaining, carefully scheduled, portion of the traversals on the GPU. We applied this framework to five tree traversal algorithms, achieving significant speedups over optimized GPU code that does not perform application-specific scheduling. Further, we show that in many cases, our hybrid approach is able to deliver better performance even than GPU code that uses hand-tuned, application-specific scheduling.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {41},
numpages = {2},
keywords = {irregular applications, tree traversal, heterogeneous architectures, scheduling}
}

@inproceedings{10.1145/2851141.2851174,
author = {Liu, Jianqiao and Hegde, Nikhil and Kulkarni, Milind},
title = {Hybrid CPU-GPU Scheduling and Execution of Tree Traversals},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851174},
doi = {10.1145/2851141.2851174},
abstract = {GPUs offer the promise of massive, power-efficient parallelism. However, exploiting this parallelism requires code to be carefully structured to deal with the limitations of the SIMT execution model. In recent years, there has been much interest in mapping irregular applications to GPUs: applications with unpredictable, data-dependent behaviors. While most of the work in this space has focused on ad hoc implementations of specific algorithms, recent work has looked at generic techniques for mapping a large class of tree traversal algorithms to GPUs, through careful restructuring of the tree traversal algorithms to make them behave more regularly. Unfortunately, even this general approach for GPU execution of tree traversal algorithms is reliant on ad hoc, handwritten, algorithm-specific scheduling (i.e., assignment of threads to warps) to achieve high performance.The key challenge of scheduling is that it is a highly irregular process, that requires the inspection of thread behavior and then careful sorting of the threads into warps. In this paper, we present a novel scheduling and execution technique for tree traversal algorithms that is both general and automatic. The key novelty is a hybrid approach: the GPU partially executes tasks to inspect thread behavior and transmits information back to the CPU, which uses that information to perform the scheduling itself, before executing the remaining, carefully scheduled, portion of the traversals on the GPU. We applied this framework to five tree traversal algorithms, achieving significant speedups over optimized GPU code that does not perform application-specific scheduling. Further, we show that in many cases, our hybrid approach is able to deliver better performance even than GPU code that uses hand-tuned, application-specific scheduling.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {41},
numpages = {2},
keywords = {heterogeneous architectures, irregular applications, tree traversal, scheduling},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851173,
author = {Ramachandran, Arunmoezhi and Mittal, Neeraj},
title = {Improving Efficacy of Internal Binary Search Trees Using Local Recovery},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851173},
doi = {10.1145/3016078.2851173},
abstract = {Binary Search Tree (BST) is an important data structure for managing ordered data. Many algorithms---blocking as well as non-blocking---have been proposed for concurrent manipulation of a binary search tree in an asynchronous shared memory system that supports search, insert and delete operations based on both external and internal representations of a search tree.An important step in executing an operation on a tree is to traverse the tree from top-to-down in order to locate the operation's window. A process may need to perform this traversal several times to handle any failures occurring due to other processes performing conflicting actions on the tree. Most concurrent algorithms that have proposed so far use a na\"{\i}ve approach and simply restart the traversal from the root of the tree.In this work, we present a new approach to recover from such failures more efficiently in a concurrent binary search tree based on internal representation using local recovery by restarting the traversal from the "middle" of the tree in order to locate an operation's window. Our approach is sufficiently general in the sense that it can be applied to a variety of concurrent binary search trees based on both blocking and non-blocking approaches.Using experimental evaluation, we demonstrate that our local recovery approach can yield significant speed-ups of up to 69% for many concurrent algorithms.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {42},
numpages = {2},
keywords = {binary search tree, concurrent data structure, local recovery, internal representation}
}

@inproceedings{10.1145/2851141.2851173,
author = {Ramachandran, Arunmoezhi and Mittal, Neeraj},
title = {Improving Efficacy of Internal Binary Search Trees Using Local Recovery},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851173},
doi = {10.1145/2851141.2851173},
abstract = {Binary Search Tree (BST) is an important data structure for managing ordered data. Many algorithms---blocking as well as non-blocking---have been proposed for concurrent manipulation of a binary search tree in an asynchronous shared memory system that supports search, insert and delete operations based on both external and internal representations of a search tree.An important step in executing an operation on a tree is to traverse the tree from top-to-down in order to locate the operation's window. A process may need to perform this traversal several times to handle any failures occurring due to other processes performing conflicting actions on the tree. Most concurrent algorithms that have proposed so far use a na\"{\i}ve approach and simply restart the traversal from the root of the tree.In this work, we present a new approach to recover from such failures more efficiently in a concurrent binary search tree based on internal representation using local recovery by restarting the traversal from the "middle" of the tree in order to locate an operation's window. Our approach is sufficiently general in the sense that it can be applied to a variety of concurrent binary search trees based on both blocking and non-blocking approaches.Using experimental evaluation, we demonstrate that our local recovery approach can yield significant speed-ups of up to 69% for many concurrent algorithms.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {42},
numpages = {2},
keywords = {local recovery, internal representation, concurrent data structure, binary search tree},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851190,
author = {Merrill, Duane and Garland, Michael},
title = {Merge-Based Sparse Matrix-Vector Multiplication (SpMV) Using the CSR Storage Format},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851190},
doi = {10.1145/3016078.2851190},
abstract = {We present a perfectly balanced, "merge-based" parallel method for computing sparse matrix-vector products (SpMV). Our algorithm operates directly upon the Compressed Sparse Row (CSR) sparse matrix format, a predominant in-memory representation for general-purpose sparse linear algebra computations. Our CsrMV performs an equitable multi-partitioning of the input dataset, ensuring that no single thread can be overwhelmed by assignment to (a) arbitrarily-long rows or (b) an arbitrarily-large number of zero-length rows. This parallel decomposition requires neither offline preprocessing nor specialized/ancillary data formats. We evaluate our method on both CPU and GPU microarchitecture across an enormous corpus of diverse real world matrix datasets. We show that traditional CsrMV methods are inconsistent performers subject to order-of-magnitude slowdowns, whereas the performance response of our method is substantially impervious to row-length heterogeneity.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {43},
numpages = {2},
keywords = {merge-path, merge, parallel decomposition, SpMV, GPU, sparse matrix, segmented reduction, sparse graph}
}

@inproceedings{10.1145/2851141.2851190,
author = {Merrill, Duane and Garland, Michael},
title = {Merge-Based Sparse Matrix-Vector Multiplication (SpMV) Using the CSR Storage Format},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851190},
doi = {10.1145/2851141.2851190},
abstract = {We present a perfectly balanced, "merge-based" parallel method for computing sparse matrix-vector products (SpMV). Our algorithm operates directly upon the Compressed Sparse Row (CSR) sparse matrix format, a predominant in-memory representation for general-purpose sparse linear algebra computations. Our CsrMV performs an equitable multi-partitioning of the input dataset, ensuring that no single thread can be overwhelmed by assignment to (a) arbitrarily-long rows or (b) an arbitrarily-large number of zero-length rows. This parallel decomposition requires neither offline preprocessing nor specialized/ancillary data formats. We evaluate our method on both CPU and GPU microarchitecture across an enormous corpus of diverse real world matrix datasets. We show that traditional CsrMV methods are inconsistent performers subject to order-of-magnitude slowdowns, whereas the performance response of our method is substantially impervious to row-length heterogeneity.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {43},
numpages = {2},
keywords = {segmented reduction, sparse matrix, parallel decomposition, SpMV, GPU, sparse graph, merge, merge-path},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851193,
author = {Drebes, Andi and Pop, Antoniu and Heydemann, Karine and Drach, Nathalie and Cohen, Albert},
title = {NUMA-Aware Scheduling and Memory Allocation for Data-Flow Task-Parallel Applications},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851193},
doi = {10.1145/3016078.2851193},
abstract = {Dynamic task parallelism is a popular programming model on shared-memory systems. Compared to data parallel loop-based concurrency, it promises enhanced scalability, load balancing and locality. These promises, however, are undermined by non-uniform memory access (NUMA) systems. We show that it is possible to preserve the uniform hardware abstraction of contemporary task-parallel programming models, for both computing and memory resources, while achieving near-optimal data locality. Our run-time algorithms for NUMA-aware task and data placement are fully automatic, application-independent, performance-portable across NUMA machines, and adapt to dynamic changes. Placement decisions use information about inter-task data dependences and reuse. This information is readily available in the run-time systems of modern task-parallel programming frameworks, and from the operating system regarding the placement of previously allocated memory. Our algorithms take advantage of data-flow style task parallelism, where the privatization of task data enhances scalability through the elimination of false dependences and enables fine-grained dynamic control over the placement of application data. We demonstrate that the benefits of dynamically managing data placement outweigh the privatization cost, even when comparing with target-specific optimizations through static, NUMA-aware data interleaving. Our implementation and the experimental evaluation on a set of high-performance benchmarks executing on a 192-core system with 24 NUMA nodes show that the fraction of local memory accesses can be increased to more than 99%, resulting in a speedup of up to 5\texttimes{} compared to a NUMA-aware hierarchical work-stealing baseline.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {44},
numpages = {2}
}

@inproceedings{10.1145/2851141.2851193,
author = {Drebes, Andi and Pop, Antoniu and Heydemann, Karine and Drach, Nathalie and Cohen, Albert},
title = {NUMA-Aware Scheduling and Memory Allocation for Data-Flow Task-Parallel Applications},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851193},
doi = {10.1145/2851141.2851193},
abstract = {Dynamic task parallelism is a popular programming model on shared-memory systems. Compared to data parallel loop-based concurrency, it promises enhanced scalability, load balancing and locality. These promises, however, are undermined by non-uniform memory access (NUMA) systems. We show that it is possible to preserve the uniform hardware abstraction of contemporary task-parallel programming models, for both computing and memory resources, while achieving near-optimal data locality. Our run-time algorithms for NUMA-aware task and data placement are fully automatic, application-independent, performance-portable across NUMA machines, and adapt to dynamic changes. Placement decisions use information about inter-task data dependences and reuse. This information is readily available in the run-time systems of modern task-parallel programming frameworks, and from the operating system regarding the placement of previously allocated memory. Our algorithms take advantage of data-flow style task parallelism, where the privatization of task data enhances scalability through the elimination of false dependences and enables fine-grained dynamic control over the placement of application data. We demonstrate that the benefits of dynamically managing data placement outweigh the privatization cost, even when comparing with target-specific optimizations through static, NUMA-aware data interleaving. Our implementation and the experimental evaluation on a set of high-performance benchmarks executing on a 192-core system with 24 NUMA nodes show that the fraction of local memory accesses can be increased to more than 99%, resulting in a speedup of up to 5\texttimes{} compared to a NUMA-aware hierarchical work-stealing baseline.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {44},
numpages = {2},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851189,
author = {Mohamedin, Mohamed and Palmieri, Roberto and Peluso, Sebastiano and Ravindran, Binoy},
title = {On Designing NUMA-Aware Concurrency Control for Scalable Transactional Memory},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851189},
doi = {10.1145/3016078.2851189},
abstract = {NUMA architectures posed the challenge of rethinking parallel applications due to the non-homogeneity introduced by their design, and their real benefits are limited to the characteristics of the particular workload. We name as partitionable transactional workloads such workloads that may be able to exploit the distributed nature of NUMA, such as transactional workloads where data and accesses can be easily partitioned among the so called NUMA zones. However, in case those workloads require the synchronization on shared data, we have to face the issue of exploiting the NUMA architecture also in the concurrency control for their transactions. Therefore in this paper we present a NUMA-aware concurrency control for transactional memory that we designed for promoting scalability in scenarios where both the transactional workload is prone to scale, and the characteristics of the underlying memory model are inherently non-uniform, such as NUMA architectures.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {45},
numpages = {2},
keywords = {scalability, NUMA, transactional memory}
}

@inproceedings{10.1145/2851141.2851189,
author = {Mohamedin, Mohamed and Palmieri, Roberto and Peluso, Sebastiano and Ravindran, Binoy},
title = {On Designing NUMA-Aware Concurrency Control for Scalable Transactional Memory},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851189},
doi = {10.1145/2851141.2851189},
abstract = {NUMA architectures posed the challenge of rethinking parallel applications due to the non-homogeneity introduced by their design, and their real benefits are limited to the characteristics of the particular workload. We name as partitionable transactional workloads such workloads that may be able to exploit the distributed nature of NUMA, such as transactional workloads where data and accesses can be easily partitioned among the so called NUMA zones. However, in case those workloads require the synchronization on shared data, we have to face the issue of exploiting the NUMA architecture also in the concurrency control for their transactions. Therefore in this paper we present a NUMA-aware concurrency control for transactional memory that we designed for promoting scalability in scenarios where both the transactional workload is prone to scale, and the characteristics of the underlying memory model are inherently non-uniform, such as NUMA architectures.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {45},
numpages = {2},
keywords = {scalability, transactional memory, NUMA},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851191,
author = {Saad, Mohamed M. and Palmieri, Roberto and Ravindran, Binoy},
title = {On Ordering Transaction Commit},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851191},
doi = {10.1145/3016078.2851191},
abstract = {In this poster paper, we briefly introduce an effective solution to address the problem of committing transactions enforcing a predefined order. To do that, we overview the design of two algorithms that deploy a cooperative transaction execution that circumvents the transaction isolation constraint in favor of propagating written values among conflicting transactions. A preliminary implementation shows that even in the presence of data conflicts, the proposed algorithms outperform other competitors, significantly.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {46},
numpages = {2},
keywords = {commitment ordering, transactional memory}
}

@inproceedings{10.1145/2851141.2851191,
author = {Saad, Mohamed M. and Palmieri, Roberto and Ravindran, Binoy},
title = {On Ordering Transaction Commit},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851191},
doi = {10.1145/2851141.2851191},
abstract = {In this poster paper, we briefly introduce an effective solution to address the problem of committing transactions enforcing a predefined order. To do that, we overview the design of two algorithms that deploy a cooperative transaction execution that circumvents the transaction isolation constraint in favor of propagating written values among conflicting transactions. A preliminary implementation shows that even in the presence of data conflicts, the proposed algorithms outperform other competitors, significantly.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {46},
numpages = {2},
keywords = {transactional memory, commitment ordering},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851179,
author = {Qian, Xuehai and Sen, Koushik and Hargrove, Paul and Iancu, Costin},
title = {OPR: Deterministic Group Replay for One-Sided Communication},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851179},
doi = {10.1145/3016078.2851179},
abstract = {The ability to reproduce a parallel execution is desirable for debugging and program reliability purposes. In debugging (13), the programmer needs to manually step back in time, while for resilience (6) this is automatically performed by the the application upon failure. To be useful, replay has to faithfully reproduce the original execution. For parallel programs the main challenge is inferring and maintaining the order of conflicting operations (data races). Deterministic record and replay (R&amp;R) techniques have been developed for multithreaded shared memory programs (5), as well as distributed memory programs (14). Our main interest is techniques for large scale scientific (3; 4) programming models.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {47},
numpages = {2}
}

@inproceedings{10.1145/2851141.2851179,
author = {Qian, Xuehai and Sen, Koushik and Hargrove, Paul and Iancu, Costin},
title = {OPR: Deterministic Group Replay for One-Sided Communication},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851179},
doi = {10.1145/2851141.2851179},
abstract = {The ability to reproduce a parallel execution is desirable for debugging and program reliability purposes. In debugging (13), the programmer needs to manually step back in time, while for resilience (6) this is automatically performed by the the application upon failure. To be useful, replay has to faithfully reproduce the original execution. For parallel programs the main challenge is inferring and maintaining the order of conflicting operations (data races). Deterministic record and replay (R&amp;R) techniques have been developed for multithreaded shared memory programs (5), as well as distributed memory programs (14). Our main interest is techniques for large scale scientific (3; 4) programming models.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {47},
numpages = {2},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851187,
author = {Rabozzi, Marco and Mazzucchelli, Matteo and Cordone, Roberto and Fumarola, Giovanni Matteo and Santambrogio, Marco D.},
title = {Preemption-Aware Planning on Big-Data Systems},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851187},
doi = {10.1145/3016078.2851187},
abstract = {Recent developments in Big Data frameworks are moving towards reservation based approaches as a mean to manage the increasingly complex mix of computations, whereas preemption techniques are employed to meet strict jobs deadlines. Within this work we propose and evaluate a new planning algorithm in the context of reservation based scheduling. Our approach is able to achieve high cluster utilization while minimizing the need for preemption that causes system overheads and planning mispredictions.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {48},
numpages = {2}
}

@inproceedings{10.1145/2851141.2851187,
author = {Rabozzi, Marco and Mazzucchelli, Matteo and Cordone, Roberto and Fumarola, Giovanni Matteo and Santambrogio, Marco D.},
title = {Preemption-Aware Planning on Big-Data Systems},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851187},
doi = {10.1145/2851141.2851187},
abstract = {Recent developments in Big Data frameworks are moving towards reservation based approaches as a mean to manage the increasingly complex mix of computations, whereas preemption techniques are employed to meet strict jobs deadlines. Within this work we propose and evaluate a new planning algorithm in the context of reservation based scheduling. Our approach is able to achieve high cluster utilization while minimizing the need for preemption that causes system overheads and planning mispredictions.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {48},
numpages = {2},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851185,
author = {Chen, Yifeng and Huang, Kun and Wang, Bei and Li, Guohui and Cui, Xiang},
title = {Samsara Parallel: A Non-BSP Parallel-in-Time Model},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851185},
doi = {10.1145/3016078.2851185},
abstract = {Many time-dependent problems like molecular dynamics of protein folding require a large number of time steps. The latencies and overheads of common-purpose clusters with accelerators are too big for high-frequency iteration. We introduce an algorithmic model called Samsara Parallel (or SP) which, unlike BSP, relies on asynchronous communications and can repeatedly return to earlier time steps to refine the precision of computation. This also extends a line of research called Parallel-in-Time in computational chemistry and physics.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {49},
numpages = {2},
keywords = {parallel-in-time, BSP, molecular dynamics}
}

@inproceedings{10.1145/2851141.2851185,
author = {Chen, Yifeng and Huang, Kun and Wang, Bei and Li, Guohui and Cui, Xiang},
title = {Samsara Parallel: A Non-BSP Parallel-in-Time Model},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851185},
doi = {10.1145/2851141.2851185},
abstract = {Many time-dependent problems like molecular dynamics of protein folding require a large number of time steps. The latencies and overheads of common-purpose clusters with accelerators are too big for high-frequency iteration. We introduce an algorithmic model called Samsara Parallel (or SP) which, unlike BSP, relies on asynchronous communications and can repeatedly return to earlier time steps to refine the precision of computation. This also extends a line of research called Parallel-in-Time in computational chemistry and physics.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {49},
numpages = {2},
keywords = {molecular dynamics, parallel-in-time, BSP},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851176,
author = {Zhang, Mingzhe and Lau, Francis C. M. and Wang, Cho-Li and Cheng, Luwei and Chen, Haibo},
title = {Scalable Adaptive NUMA-Aware Lock: Combining Local Locking and Remote Locking for Efficient Concurrency},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851176},
doi = {10.1145/3016078.2851176},
abstract = {Scalable locking is a key building block for scalable multi-threaded software. Its performance is especially critical in multi-socket, multi-core machines with non-uniform memory access (NUMA). Previous schemes such as local locking and remote locking only perform well under a certain level of contention, and often require non-trivial tuning for a particular configuration. Besides, for large NUMA systems, because of unmanaged lock server's nomination, current distance-first NUMA policies cannot perform satisfactorily.In this work, we propose SANL, a locking scheme that can deliver high performance under various contention levels by adaptively switching between the local and the remote lock scheme. Furthermore, we introduce a new NUMA policy for the remote lock that jointly considers node distances and server utilization when choosing lock servers. A comparison with seven representative locking schemes shows that SANL outperforms the others in most contention situations. In one group test, SANL is 3.7 times faster than RCL lock and 17 times faster than POSIX mutex.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {50},
numpages = {2}
}

@inproceedings{10.1145/2851141.2851176,
author = {Zhang, Mingzhe and Lau, Francis C. M. and Wang, Cho-Li and Cheng, Luwei and Chen, Haibo},
title = {Scalable Adaptive NUMA-Aware Lock: Combining Local Locking and Remote Locking for Efficient Concurrency},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851176},
doi = {10.1145/2851141.2851176},
abstract = {Scalable locking is a key building block for scalable multi-threaded software. Its performance is especially critical in multi-socket, multi-core machines with non-uniform memory access (NUMA). Previous schemes such as local locking and remote locking only perform well under a certain level of contention, and often require non-trivial tuning for a particular configuration. Besides, for large NUMA systems, because of unmanaged lock server's nomination, current distance-first NUMA policies cannot perform satisfactorily.In this work, we propose SANL, a locking scheme that can deliver high performance under various contention levels by adaptively switching between the local and the remote lock scheme. Furthermore, we introduce a new NUMA policy for the remote lock that jointly considers node distances and server utilization when choosing lock servers. A comparison with seven representative locking schemes shows that SANL outperforms the others in most contention situations. In one group test, SANL is 3.7 times faster than RCL lock and 17 times faster than POSIX mutex.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {50},
numpages = {2},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851177,
author = {Hegde, Nikhil and Liu, Jianqiao and Kulkarni, Milind},
title = {SPIRIT: A Runtime System for Distributed Irregular Tree Applications},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851177},
doi = {10.1145/3016078.2851177},
abstract = {Repeated, depth-first traversal of trees is a common algorithmic pattern in an important set of applications from diverse domains such as cosmological simulations, data mining, and computer graphics. As these applications operate over massive data sets, it is often necessary to distribute the trees to process all of the data.In this work, we introduce SPIRIT, a runtime system to ease the writing of distributed tree applications. SPIRIT automates the challenging tasks of tree distribution, optimizing communication and parallelizing independent computations. The common algorithmic pattern in tree traversals is exploited to effectively schedule parallel computations and improve locality. As a result, pipeline parallelism in distributed traversals is identified, which is complemented by load-balancing, and locality-enhancing, message aggregation optimizations. Evaluation of SPIRIT on tree traversal in Point Correlation (PC) shows a scalable system, achieving speedups upto 38x on a 16-node, 64 process system compared to a 1-node, baseline configuration. We also find that SPIRIT results in substantially less communication and achieves significant performance improvements over implementations in other distributed graph systems.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {51},
numpages = {2},
keywords = {irregular programs, distributed computing}
}

@inproceedings{10.1145/2851141.2851177,
author = {Hegde, Nikhil and Liu, Jianqiao and Kulkarni, Milind},
title = {SPIRIT: A Runtime System for Distributed Irregular Tree Applications},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851177},
doi = {10.1145/2851141.2851177},
abstract = {Repeated, depth-first traversal of trees is a common algorithmic pattern in an important set of applications from diverse domains such as cosmological simulations, data mining, and computer graphics. As these applications operate over massive data sets, it is often necessary to distribute the trees to process all of the data.In this work, we introduce SPIRIT, a runtime system to ease the writing of distributed tree applications. SPIRIT automates the challenging tasks of tree distribution, optimizing communication and parallelizing independent computations. The common algorithmic pattern in tree traversals is exploited to effectively schedule parallel computations and improve locality. As a result, pipeline parallelism in distributed traversals is identified, which is complemented by load-balancing, and locality-enhancing, message aggregation optimizations. Evaluation of SPIRIT on tree traversal in Point Correlation (PC) shows a scalable system, achieving speedups upto 38x on a 16-node, 64 process system compared to a 1-node, baseline configuration. We also find that SPIRIT results in substantially less communication and achieves significant performance improvements over implementations in other distributed graph systems.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {51},
numpages = {2},
keywords = {irregular programs, distributed computing},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851171,
author = {Ramalhete, Pedro and Correia, Andreia},
title = {Tidex: A Mutual Exclusion Lock},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851171},
doi = {10.1145/3016078.2851171},
abstract = {Several basic mutual exclusion lock algorithms are known, with one of the simplest being the Ticket Lock. We present a new mutual exclusion lock with properties similar to the Ticket Lock but using atomic_exchange() instead of atomic_fetch_add() that can be more efficient on systems without a native instruction for atomic_fetch_add(), or in which the native instruction for atomic_exchange() is faster than the one for atomic_fetch_add(). Similarly to the Ticket Lock, our lock has small memory foot print, is extremely simple, respects FIFO order, and provides starvation freedom in architectures that implement atomic_exchange() as a single instruction, like x86.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {52},
numpages = {2},
keywords = {mutual exclusion, ticket lock, locks}
}

@inproceedings{10.1145/2851141.2851171,
author = {Ramalhete, Pedro and Correia, Andreia},
title = {Tidex: A Mutual Exclusion Lock},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851171},
doi = {10.1145/2851141.2851171},
abstract = {Several basic mutual exclusion lock algorithms are known, with one of the simplest being the Ticket Lock. We present a new mutual exclusion lock with properties similar to the Ticket Lock but using atomic_exchange() instead of atomic_fetch_add() that can be more efficient on systems without a native instruction for atomic_fetch_add(), or in which the native instruction for atomic_exchange() is faster than the one for atomic_fetch_add(). Similarly to the Ticket Lock, our lock has small memory foot print, is extremely simple, respects FIFO order, and provides starvation freedom in architectures that implement atomic_exchange() as a single instruction, like x86.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {52},
numpages = {2},
keywords = {ticket lock, locks, mutual exclusion},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851172,
author = {Mastoras, Aristeidis and Gross, Thomas R.},
title = {Unifying Fixed Code and Fixed Data Mapping of Load-Imbalanced Pipelined Loops},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851172},
doi = {10.1145/3016078.2851172},
abstract = {Some loops with cross-iteration dependences can execute in parallel by pipelining. The loop body is partitioned into stages such that the data dependences are not violated and then the stages are mapped onto threads. Two well-known mapping techniques are fixed code and fixed data; they achieve high performance for load-balanced loops, but they fail to perform well for load-imbalanced loops. In this article, we present a novel hybrid mapping that eliminates drawbacks of both prior mapping techniques and enables dynamic scheduling of stages.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {53},
numpages = {2},
keywords = {multi-threading, pipeline model, dynamic scheduling, mapping}
}

@inproceedings{10.1145/2851141.2851172,
author = {Mastoras, Aristeidis and Gross, Thomas R.},
title = {Unifying Fixed Code and Fixed Data Mapping of Load-Imbalanced Pipelined Loops},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851172},
doi = {10.1145/2851141.2851172},
abstract = {Some loops with cross-iteration dependences can execute in parallel by pipelining. The loop body is partitioned into stages such that the data dependences are not violated and then the stages are mapped onto threads. Two well-known mapping techniques are fixed code and fixed data; they achieve high performance for load-balanced loops, but they fail to perform well for load-imbalanced loops. In this article, we present a novel hybrid mapping that eliminates drawbacks of both prior mapping techniques and enables dynamic scheduling of stages.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {53},
numpages = {2},
keywords = {pipeline model, mapping, dynamic scheduling, multi-threading},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851180,
author = {Kurt, Mehmet Can and Ren, Bin and Krishnamoorthy, Sriram and Agrawal, Gagan},
title = {User-Assisted Storage Reuse Determination for Dynamic Task Graphs},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851180},
doi = {10.1145/3016078.2851180},
abstract = {Models based on task graphs that operate on single-assignment data are attractive in several ways, but also require nuanced algorithms for scheduling and memory management for efficient execution. In this paper, we consider memory-efficient dynamic scheduling of task graphs, and present a novel approach for dynamically recycling the memory locations assigned to data items as they are produced by tasks.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {54},
numpages = {2},
keywords = {memory management, dynamic task graphs}
}

@inproceedings{10.1145/2851141.2851180,
author = {Kurt, Mehmet Can and Ren, Bin and Krishnamoorthy, Sriram and Agrawal, Gagan},
title = {User-Assisted Storage Reuse Determination for Dynamic Task Graphs},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851180},
doi = {10.1145/2851141.2851180},
abstract = {Models based on task graphs that operate on single-assignment data are attractive in several ways, but also require nuanced algorithms for scheduling and memory management for efficient execution. In this paper, we consider memory-efficient dynamic scheduling of task graphs, and present a novel approach for dynamically recycling the memory locations assigned to data items as they are produced by tasks.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {54},
numpages = {2},
keywords = {dynamic task graphs, memory management},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851192,
author = {Rehman, Waqas Ur and Ayub, Muhammad Sohaib and Siddiqui, Junaid Haroon},
title = {Verification of MPI Java Programs Using Software Model Checking},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851192},
doi = {10.1145/3016078.2851192},
abstract = {Development of concurrent software requires the programmer to be aware of non-determinism, data races, and deadlocks. MPI (message passing interface) is a popular standard for writing message oriented distributed applications. Some messages in MPI systems can be processed by one of the many machines and in many possible orders. This non-determinism can affect the result of an MPI application. The alternate results may or may not be correct. To verify MPI applications, we need to check all these possible orderings and use an application specific oracle to decide if these orderings give correct output. MPJ Express is an open source Java implementation of the MPI standard. We developed a Java based model of MPJ Express, where processes are modeled as threads, and which can run unmodified MPI Java programs on a single system. This enabled us to adapt the Java PathFinder explicit state software model checker (JPF) using a custom listener to verify our model running real MPI Java programs. We evaluated our approach using small examples where model checking revealed message orders that would result in incorrect system behavior.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {55},
numpages = {2},
keywords = {Java PathFinder(JPF), message passing interface in Java (MPJ), model checking}
}

@inproceedings{10.1145/2851141.2851192,
author = {Rehman, Waqas Ur and Ayub, Muhammad Sohaib and Siddiqui, Junaid Haroon},
title = {Verification of MPI Java Programs Using Software Model Checking},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851192},
doi = {10.1145/2851141.2851192},
abstract = {Development of concurrent software requires the programmer to be aware of non-determinism, data races, and deadlocks. MPI (message passing interface) is a popular standard for writing message oriented distributed applications. Some messages in MPI systems can be processed by one of the many machines and in many possible orders. This non-determinism can affect the result of an MPI application. The alternate results may or may not be correct. To verify MPI applications, we need to check all these possible orderings and use an application specific oracle to decide if these orderings give correct output. MPJ Express is an open source Java implementation of the MPI standard. We developed a Java based model of MPJ Express, where processes are modeled as threads, and which can run unmodified MPI Java programs on a single system. This enabled us to adapt the Java PathFinder explicit state software model checker (JPF) using a custom listener to verify our model running real MPI Java programs. We evaluated our approach using small examples where model checking revealed message orders that would result in incorrect system behavior.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {55},
numpages = {2},
keywords = {message passing interface in Java (MPJ), model checking, Java PathFinder(JPF)},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

