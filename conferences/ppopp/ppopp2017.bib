@article{10.1145/3155284.3018773,
author = {Steele, Guy L.},
title = {It's Time for a New Old Language},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018773},
doi = {10.1145/3155284.3018773},
abstract = {The most popular programming language in computer science has no compiler or interpreter. Its definition is not written down in any one place. It has changed a lot over the decades, and those changes have introduced ambiguities and inconsistencies. Today, dozens of variations are in use, and its complexity has reached the point where it needs to be re-explained, at least in part, every time it is used. Much effort has been spent in hand-translating between this language and other languages that do have compilers. The language is quite amenable to parallel computation, but this fact has gone unexploited.In this talk we will summarize the history of the language, highlight the variations and some of the problems that have arisen, and propose specific solutions. We suggest that it is high time that this language be given a complete formal specification, and that compilers, IDEs, and proof-checkers be created to support it, so that all the best tools and techniques of our trade may be applied to it also.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {1},
numpages = {1},
keywords = {programming languages, compilers, specifications}
}

@inproceedings{10.1145/3018743.3018773,
author = {Steele, Guy L.},
title = {It's Time for a New Old Language},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018773},
doi = {10.1145/3018743.3018773},
abstract = {The most popular programming language in computer science has no compiler or interpreter. Its definition is not written down in any one place. It has changed a lot over the decades, and those changes have introduced ambiguities and inconsistencies. Today, dozens of variations are in use, and its complexity has reached the point where it needs to be re-explained, at least in part, every time it is used. Much effort has been spent in hand-translating between this language and other languages that do have compilers. The language is quite amenable to parallel computation, but this fact has gone unexploited.In this talk we will summarize the history of the language, highlight the variations and some of the problems that have arisen, and propose specific solutions. We suggest that it is high time that this language be given a complete formal specification, and that compilers, IDEs, and proof-checkers be created to support it, so that all the best tools and techniques of our trade may be applied to it also.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {1},
numpages = {1},
keywords = {compilers, specifications, programming languages},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018748,
author = {Chen, Guoyang and Zhao, Yue and Shen, Xipeng and Zhou, Huiyang},
title = {EffiSha: A Software Framework for Enabling Effficient Preemptive Scheduling of GPU},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018748},
doi = {10.1145/3155284.3018748},
abstract = {Modern GPUs are broadly adopted in many multitasking environments, including data centers and smartphones. However, the current support for the scheduling of multiple GPU kernels (from different applications) is limited, forming a major barrier for GPU to meet many practical needs. This work for the first time demonstrates that on existing GPUs, efficient preemptive scheduling of GPU kernels is possible even without special hardware support. Specifically, it presents EffiSha, a pure software framework that enables preemptive scheduling of GPU kernels with very low overhead. The enabled preemptive scheduler offers flexible support of kernels of different priorities, and demonstrates significant potential for reducing the average turnaround time and improving the system overall throughput of programs that time share a modern GPU.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {3–16},
numpages = {14},
keywords = {software framework, os, average turnaround time, overall system throughput, preemptive scheduling, gpu}
}

@inproceedings{10.1145/3018743.3018748,
author = {Chen, Guoyang and Zhao, Yue and Shen, Xipeng and Zhou, Huiyang},
title = {EffiSha: A Software Framework for Enabling Effficient Preemptive Scheduling of GPU},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018748},
doi = {10.1145/3018743.3018748},
abstract = {Modern GPUs are broadly adopted in many multitasking environments, including data centers and smartphones. However, the current support for the scheduling of multiple GPU kernels (from different applications) is limited, forming a major barrier for GPU to meet many practical needs. This work for the first time demonstrates that on existing GPUs, efficient preemptive scheduling of GPU kernels is possible even without special hardware support. Specifically, it presents EffiSha, a pure software framework that enables preemptive scheduling of GPU kernels with very low overhead. The enabled preemptive scheduler offers flexible support of kernels of different priorities, and demonstrates significant potential for reducing the average turnaround time and improving the system overall throughput of programs that time share a modern GPU.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {3–16},
numpages = {14},
keywords = {gpu, average turnaround time, preemptive scheduling, overall system throughput, software framework, os},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018753,
author = {Cohen, Nachshon and Tal, Arie and Petrank, Erez},
title = {Layout Lock: A Scalable Locking Paradigm for Concurrent Data Layout Modifications},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018753},
doi = {10.1145/3155284.3018753},
abstract = {Data-structures can benefit from dynamic data layout modifications when the size or the shape of the data structure changes during the execution, or when different phases in the program execute different workloads. However, in a modern multi-core environment, layout modifications involve costly synchronization overhead. In this paper we propose a novel layout lock that incurs a negligible overhead for reads and a small overhead for updates of the data structure. We then demonstrate the benefits of layout changes and also the advantages of the layout lock as its supporting synchronization mechanism for two data structures. In particular, we propose a concurrent binary search tree, and a concurrent array set, that benefit from concurrent layout modifications using the proposed layout lock. Experience demonstrates performance advantages and integration simplicity.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {17–29},
numpages = {13},
keywords = {concurrent data structures, data layout, synchronization}
}

@inproceedings{10.1145/3018743.3018753,
author = {Cohen, Nachshon and Tal, Arie and Petrank, Erez},
title = {Layout Lock: A Scalable Locking Paradigm for Concurrent Data Layout Modifications},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018753},
doi = {10.1145/3018743.3018753},
abstract = {Data-structures can benefit from dynamic data layout modifications when the size or the shape of the data structure changes during the execution, or when different phases in the program execute different workloads. However, in a modern multi-core environment, layout modifications involve costly synchronization overhead. In this paper we propose a novel layout lock that incurs a negligible overhead for reads and a small overhead for updates of the data structure. We then demonstrate the benefits of layout changes and also the advantages of the layout lock as its supporting synchronization mechanism for two data structures. In particular, we propose a concurrent binary search tree, and a concurrent array set, that benefit from concurrent layout modifications using the proposed layout lock. Experience demonstrates performance advantages and integration simplicity.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {17–29},
numpages = {13},
keywords = {synchronization, concurrent data structures, data layout},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018755,
author = {Zhang, Xiuxia and Tan, Guangming and Xue, Shuangbai and Li, Jiajia and Zhou, Keren and Chen, Mingyu},
title = {Understanding the GPU Microarchitecture to Achieve Bare-Metal Performance Tuning},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018755},
doi = {10.1145/3155284.3018755},
abstract = {In this paper, we present a methodology to understand GPU microarchitectural features and improve performance for compute-intensive kernels. The methodology relies on a reverse engineering approach to crack the GPU ISA encodings in order to build a GPU assembler. An assembly microbenchmark suite correlates microarchitectural features with their performance factors to uncover instruction-level and memory hierarchy preferences. We use SGEMM as a running example to show the ways to achieve bare-metal performance tuning. The performance boost is achieved by tuning FFMA throughput by activating dual-issue, eliminating register bank conflicts, adding non-FFMA instructions with little penalty, and choosing proper width of global/shared load instructions. On NVIDIA Kepler K20m, we develop a faster SGEMM with 3.1Tflop/s performance and 88% efficiency; the performance is 15% higher than cuBLAS7.0. Applying these optimizations to convolution, the implementation gains 39%-62% performance improvement compared with cuDNN4.0. The toolchain is an attempt to automatically crack different GPU ISA encodings and build an assembler adaptively for the purpose of performance enhancements to applications on GPUs.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {31–43},
numpages = {13},
keywords = {assembler, convolution, gpu, sgemm, performance, reverse-engineering gpu isa encoding}
}

@inproceedings{10.1145/3018743.3018755,
author = {Zhang, Xiuxia and Tan, Guangming and Xue, Shuangbai and Li, Jiajia and Zhou, Keren and Chen, Mingyu},
title = {Understanding the GPU Microarchitecture to Achieve Bare-Metal Performance Tuning},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018755},
doi = {10.1145/3018743.3018755},
abstract = {In this paper, we present a methodology to understand GPU microarchitectural features and improve performance for compute-intensive kernels. The methodology relies on a reverse engineering approach to crack the GPU ISA encodings in order to build a GPU assembler. An assembly microbenchmark suite correlates microarchitectural features with their performance factors to uncover instruction-level and memory hierarchy preferences. We use SGEMM as a running example to show the ways to achieve bare-metal performance tuning. The performance boost is achieved by tuning FFMA throughput by activating dual-issue, eliminating register bank conflicts, adding non-FFMA instructions with little penalty, and choosing proper width of global/shared load instructions. On NVIDIA Kepler K20m, we develop a faster SGEMM with 3.1Tflop/s performance and 88% efficiency; the performance is 15% higher than cuBLAS7.0. Applying these optimizations to convolution, the implementation gains 39%-62% performance improvement compared with cuDNN4.0. The toolchain is an attempt to automatically crack different GPU ISA encodings and build an assembler adaptively for the purpose of performance enhancements to applications on GPUs.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {31–43},
numpages = {13},
keywords = {gpu, performance, assembler, convolution, reverse-engineering gpu isa encoding, sgemm},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018749,
author = {Ou, Peizhao and Demsky, Brian},
title = {Checking Concurrent Data Structures Under the C/C++11 Memory Model},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018749},
doi = {10.1145/3155284.3018749},
abstract = {Concurrent data structures often provide better performance on multi-core processors but are significantly more difficult to design and test than their sequential counterparts. The C/C++11 standard introduced a weak memory model with support for low-level atomic operations such as compare and swap (CAS). While low-level atomic operations can significantly improve the performance of concurrent data structures, they introduce non-intuitive behaviors that can increase the difficulty of developing code.In this paper, we develop a correctness model for concurrent data structures that make use of atomic operations. Based on this correctness model, we present CDSSPEC, a specification checker for concurrent data structures under the C/C++11 memory model. We have evaluated CDSSPEC on 10 concurrent data structures, among which CDSSPEC detected 3 known bugs and 93% of the injected bugs.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {45–59},
numpages = {15},
keywords = {concurrent data structure specifications, concurrent data structure, relaxed memory models, concurrent data structure correctness, c/c++11}
}

@inproceedings{10.1145/3018743.3018749,
author = {Ou, Peizhao and Demsky, Brian},
title = {Checking Concurrent Data Structures Under the C/C++11 Memory Model},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018749},
doi = {10.1145/3018743.3018749},
abstract = {Concurrent data structures often provide better performance on multi-core processors but are significantly more difficult to design and test than their sequential counterparts. The C/C++11 standard introduced a weak memory model with support for low-level atomic operations such as compare and swap (CAS). While low-level atomic operations can significantly improve the performance of concurrent data structures, they introduce non-intuitive behaviors that can increase the difficulty of developing code.In this paper, we develop a correctness model for concurrent data structures that make use of atomic operations. Based on this correctness model, we present CDSSPEC, a specification checker for concurrent data structures under the C/C++11 memory model. We have evaluated CDSSPEC on 10 concurrent data structures, among which CDSSPEC detected 3 known bugs and 93% of the injected bugs.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {45–59},
numpages = {15},
keywords = {concurrent data structure correctness, c/c++11, relaxed memory models, concurrent data structure, concurrent data structure specifications},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018768,
author = {Chabbi, Milind and Amer, Abdelhalim and Wen, Shasha and Liu, Xu},
title = {An Efficient Abortable-Locking Protocol for Multi-Level NUMA Systems},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018768},
doi = {10.1145/3155284.3018768},
abstract = {The popularity of Non-Uniform Memory Access (NUMA) architectures has led to numerous locality-preserving hierarchical lock designs, such as HCLH, HMCS, and cohort locks. Locality-preserving locks trade fairness for higher throughput. Hence, some instances of acquisitions can incur long latencies, which may be intolerable for certain applications. Few locks admit a waiting thread to abandon its protocol on a timeout. State-of-the-art abortable locks are not fully locality aware, introduce high overheads, and unsuitable for frequent aborts. Enhancing locality-aware locks with lightweight timeout capability is critical for their adoption. In this paper, we design and evaluate the HMCS-T lock, a Hierarchical MCS (HMCS) lock variant that admits a timeout. HMCS-T maintains the locality benefits of HMCS while ensuring aborts to be lightweight. HMCS-T offers the progress guarantee missing in most abortable queuing locks. Our evaluations show that HMCS-T offers the timeout feature at a moderate overhead over its HMCS analog. HMCS-T, used in an MPI runtime lock, mitigated the poor scalability of an MPI+OpenMP BFS code and resulted in 4.3x superior scaling.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {61–74},
numpages = {14},
keywords = {abortable lock, numa, timeout lock, mcs lock, spin lock, synchronization, hierarchical lock, queuing lock}
}

@inproceedings{10.1145/3018743.3018768,
author = {Chabbi, Milind and Amer, Abdelhalim and Wen, Shasha and Liu, Xu},
title = {An Efficient Abortable-Locking Protocol for Multi-Level NUMA Systems},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018768},
doi = {10.1145/3018743.3018768},
abstract = {The popularity of Non-Uniform Memory Access (NUMA) architectures has led to numerous locality-preserving hierarchical lock designs, such as HCLH, HMCS, and cohort locks. Locality-preserving locks trade fairness for higher throughput. Hence, some instances of acquisitions can incur long latencies, which may be intolerable for certain applications. Few locks admit a waiting thread to abandon its protocol on a timeout. State-of-the-art abortable locks are not fully locality aware, introduce high overheads, and unsuitable for frequent aborts. Enhancing locality-aware locks with lightweight timeout capability is critical for their adoption. In this paper, we design and evaluate the HMCS-T lock, a Hierarchical MCS (HMCS) lock variant that admits a timeout. HMCS-T maintains the locality benefits of HMCS while ensuring aborts to be lightweight. HMCS-T offers the progress guarantee missing in most abortable queuing locks. Our evaluations show that HMCS-T offers the timeout feature at a moderate overhead over its HMCS analog. HMCS-T, used in an MPI runtime lock, mitigated the poor scalability of an MPI+OpenMP BFS code and resulted in 4.3x superior scaling.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {61–74},
numpages = {14},
keywords = {synchronization, numa, hierarchical lock, spin lock, timeout lock, abortable lock, mcs lock, queuing lock},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018762,
author = {Acar, Umut A. and Ben-David, Naama and Rainey, Mike},
title = {Contention in Structured Concurrency: Provably Efficient Dynamic Non-Zero Indicators for Nested Parallelism},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018762},
doi = {10.1145/3155284.3018762},
abstract = {Over the past two decades, many concurrent data structures have been designed and implemented. Nearly all such work analyzes concurrent data structures empirically, omitting asymptotic bounds on their efficiency, partly because of the complexity of the analysis needed, and partly because of the difficulty of obtaining relevant asymptotic bounds: when the analysis takes into account important practical factors, such as contention, it is difficult or even impossible to prove desirable bounds.In this paper, we show that considering structured concurrency or relaxed concurrency models can enable establishing strong bounds, also for contention. To this end, we first present a dynamic relaxed counter data structure that indicates the non-zero status of the counter. Our data structure extends a recently proposed data structure, called SNZI, allowing our structure to grow dynamically in response to the increasing degree of concurrency in the system.Using the dynamic SNZI data structure, we then present a concurrent data structure for series-parallel directed acyclic graphs (sp-dags), a key data structure widely used in the implementation of modern parallel programming languages. The key component of sp-dags is an in-counter data structure that is an instance of our dynamic SNZI. We analyze the efficiency of our concurrent sp-dags and in-counter data structures under nested-parallel computing paradigm. This paradigm offers a structured model for concurrency. Under this model, we prove that our data structures require amortized (1) shared memory steps, including contention. We present an implementation and an experimental evaluation that suggests that the sp-dags data structure is practical and can perform well in practice.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {75–88},
numpages = {14},
keywords = {structured concurrency, concurrent data structures, nested parallelism, contention, series-parallel dags, snzi, concurrency, indicators, contention bounds, analytical bounds, non-blocking data structures}
}

@inproceedings{10.1145/3018743.3018762,
author = {Acar, Umut A. and Ben-David, Naama and Rainey, Mike},
title = {Contention in Structured Concurrency: Provably Efficient Dynamic Non-Zero Indicators for Nested Parallelism},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018762},
doi = {10.1145/3018743.3018762},
abstract = {Over the past two decades, many concurrent data structures have been designed and implemented. Nearly all such work analyzes concurrent data structures empirically, omitting asymptotic bounds on their efficiency, partly because of the complexity of the analysis needed, and partly because of the difficulty of obtaining relevant asymptotic bounds: when the analysis takes into account important practical factors, such as contention, it is difficult or even impossible to prove desirable bounds.In this paper, we show that considering structured concurrency or relaxed concurrency models can enable establishing strong bounds, also for contention. To this end, we first present a dynamic relaxed counter data structure that indicates the non-zero status of the counter. Our data structure extends a recently proposed data structure, called SNZI, allowing our structure to grow dynamically in response to the increasing degree of concurrency in the system.Using the dynamic SNZI data structure, we then present a concurrent data structure for series-parallel directed acyclic graphs (sp-dags), a key data structure widely used in the implementation of modern parallel programming languages. The key component of sp-dags is an in-counter data structure that is an instance of our dynamic SNZI. We analyze the efficiency of our concurrent sp-dags and in-counter data structures under nested-parallel computing paradigm. This paradigm offers a structured model for concurrency. Under this model, we prove that our data structures require amortized (1) shared memory steps, including contention. We present an implementation and an experimental evaluation that suggests that the sp-dags data structure is practical and can perform well in practice.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {75–88},
numpages = {14},
keywords = {nested parallelism, concurrency, structured concurrency, indicators, snzi, contention bounds, series-parallel dags, contention, analytical bounds, concurrent data structures, non-blocking data structures},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018767,
author = {Sato, Kento and Ahn, Dong H. and Laguna, Ignacio and Lee, Gregory L. and Schulz, Martin and Chambreau, Christopher M.},
title = {Noise Injection Techniques to Expose Subtle and Unintended Message Races},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018767},
doi = {10.1145/3155284.3018767},
abstract = {Debugging intermittently occurring bugs within MPI applications is challenging, and message races, a condition in which two or more sends race to match with a receive, are one of the common root causes. Many debugging tools have been proposed to help programmers resolve them, but their runtime interference perturbs the timing such that subtle races often cannot be reproduced with debugging tools. We present novel noise injection techniques to expose message races even under a tool's control. We first formalize this race problem in the context of non-deterministic parallel applications and use this analysis to determine an effective noise-injection strategy to uncover them. We codified these techniques in NINJA (Noise INJection Agent) that exposes these races without modification to the application. Our evaluations on synthetic cases as well as a real-world bug in Hypre-2.10.1 show that NINJA significantly helps expose races.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {89–101},
numpages = {13},
keywords = {debugging, non-determinism, mpi}
}

@inproceedings{10.1145/3018743.3018767,
author = {Sato, Kento and Ahn, Dong H. and Laguna, Ignacio and Lee, Gregory L. and Schulz, Martin and Chambreau, Christopher M.},
title = {Noise Injection Techniques to Expose Subtle and Unintended Message Races},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018767},
doi = {10.1145/3018743.3018767},
abstract = {Debugging intermittently occurring bugs within MPI applications is challenging, and message races, a condition in which two or more sends race to match with a receive, are one of the common root causes. Many debugging tools have been proposed to help programmers resolve them, but their runtime interference perturbs the timing such that subtle races often cannot be reproduced with debugging tools. We present novel noise injection techniques to expose message races even under a tool's control. We first formalize this race problem in the context of non-deterministic parallel applications and use this analysis to determine an effective noise-injection strategy to uncover them. We codified these techniques in NINJA (Noise INJection Agent) that exposes these races without modification to the application. Our evaluations on synthetic cases as well as a real-world bug in Hypre-2.10.1 show that NINJA significantly helps expose races.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {89–101},
numpages = {13},
keywords = {mpi, debugging, non-determinism},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018759,
author = {Luo, Hao and Li, Pengcheng and Ding, Chen},
title = {Thread Data Sharing in Cache: Theory and Measurement},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018759},
doi = {10.1145/3155284.3018759},
abstract = {On modern multi-core processors, independent workloads often interfere with each other by competing for shared cache space. However, for multi-threaded workloads, where a single copy of data can be accessed by multiple threads, the threads can cooperatively share cache. Because data sharing consolidates the collective working set of threads, the effective size of shared cache becomes larger than it would have been when data are not shared. This paper presents a new theory of data sharing. It includes (1) a new metric called the shared footprint to mathematically compute the amount of data shared by any group of threads in any size cache, and (2) a linear-time algorithm to measure shared footprint by scanning the memory trace of a multi-threaded program. The paper presents the practical implementation and evaluates the new theory using 14 PARSEC and SPEC OMP benchmarks, including an example use of shared footprint in program optimization.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {103–115},
numpages = {13},
keywords = {model, multithreading, cache, performance, locality}
}

@inproceedings{10.1145/3018743.3018759,
author = {Luo, Hao and Li, Pengcheng and Ding, Chen},
title = {Thread Data Sharing in Cache: Theory and Measurement},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018759},
doi = {10.1145/3018743.3018759},
abstract = {On modern multi-core processors, independent workloads often interfere with each other by competing for shared cache space. However, for multi-threaded workloads, where a single copy of data can be accessed by multiple threads, the threads can cooperatively share cache. Because data sharing consolidates the collective working set of threads, the effective size of shared cache becomes larger than it would have been when data are not shared. This paper presents a new theory of data sharing. It includes (1) a new metric called the shared footprint to mathematically compute the amount of data shared by any group of threads in any size cache, and (2) a linear-time algorithm to measure shared footprint by scanning the memory trace of a multi-threaded program. The paper presents the practical implementation and evaluates the new theory using 14 PARSEC and SPEC OMP benchmarks, including an example use of shared footprint in program optimization.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {103–115},
numpages = {13},
keywords = {performance, locality, cache, model, multithreading},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018763,
author = {Ren, Bin and Krishnamoorthy, Sriram and Agrawal, Kunal and Kulkarni, Milind},
title = {Exploiting Vector and Multicore Parallelism for Recursive, Data- and Task-Parallel Programs},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018763},
doi = {10.1145/3155284.3018763},
abstract = {Modern hardware contains parallel execution resources that are well-suited for data-parallelism-vector units-and task parallelism-multicores. However, most work on parallel scheduling focuses on one type of hardware or the other. In this work, we present a scheduling framework that allows for a unified treatment of task- and data-parallelism. Our key insight is an abstraction, task blocks, that uniformly handles data-parallel iterations and task-parallel tasks, allowing them to be scheduled on vector units or executed independently as multicores. Our framework allows us to define schedulers that can dynamically select between executing task- blocks on vector units or multicores. We show that these schedulers are asymptotically optimal, and deliver the maximum amount of parallelism available in computation trees. To evaluate our schedulers, we develop program transformations that can convert mixed data- and task-parallel pro- grams into task block-based programs. Using a prototype instantiation of our scheduling framework, we show that, on an 8-core system, we can simultaneously exploit vector and multicore parallelism to achieve 14\texttimes{}-108\texttimes{} speedup over sequential baselines.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {117–130},
numpages = {14},
keywords = {task parallelism, general scheduler, data parallelism}
}

@inproceedings{10.1145/3018743.3018763,
author = {Ren, Bin and Krishnamoorthy, Sriram and Agrawal, Kunal and Kulkarni, Milind},
title = {Exploiting Vector and Multicore Parallelism for Recursive, Data- and Task-Parallel Programs},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018763},
doi = {10.1145/3018743.3018763},
abstract = {Modern hardware contains parallel execution resources that are well-suited for data-parallelism-vector units-and task parallelism-multicores. However, most work on parallel scheduling focuses on one type of hardware or the other. In this work, we present a scheduling framework that allows for a unified treatment of task- and data-parallelism. Our key insight is an abstraction, task blocks, that uniformly handles data-parallel iterations and task-parallel tasks, allowing them to be scheduled on vector units or executed independently as multicores. Our framework allows us to define schedulers that can dynamically select between executing task- blocks on vector units or multicores. We show that these schedulers are asymptotically optimal, and deliver the maximum amount of parallelism available in computation trees. To evaluate our schedulers, we develop program transformations that can convert mixed data- and task-parallel pro- grams into task block-based programs. Using a prototype instantiation of our scheduling framework, we show that, on an 8-core system, we can simultaneously exploit vector and multicore parallelism to achieve 14\texttimes{}-108\texttimes{} speedup over sequential baselines.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {117–130},
numpages = {14},
keywords = {task parallelism, data parallelism, general scheduler},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018770,
author = {Shudler, Sergei and Calotoiu, Alexandru and Hoefler, Torsten and Wolf, Felix},
title = {Isoefficiency in Practice: Configuring and Understanding the Performance of Task-Based Applications},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018770},
doi = {10.1145/3155284.3018770},
abstract = {Task-based programming offers an elegant way to express units of computation and the dependencies among them, making it easier to distribute the computational load evenly across multiple cores. However, this separation of problem decomposition and parallelism requires a sufficiently large input problem to achieve satisfactory efficiency on a given number of cores. Unfortunately, finding a good match between input size and core count usually requires significant experimentation, which is expensive and sometimes even impractical. In this paper, we propose an automated empirical method for finding the isoefficiency function of a task-based program, binding efficiency, core count, and the input size in one analytical expression. This allows the latter two to be adjusted according to given (realistic) efficiency objectives. Moreover, we not only find (i) the actual isoefficiency function but also (ii) the function one would yield if the program execution was free of resource contention and (iii) an upper bound that could only be reached if the program was able to maintain its average parallelism throughout its execution. The difference between the three helps to explain low efficiency, and in particular, it helps to differentiate between resource contention and structural conflicts related to task dependencies or scheduling. The insights gained can be used to co-design programs and shared system resources.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {131–143},
numpages = {13},
keywords = {co-design, performance modeling, isoefficiency, tasking, parallel programming, performance analysis}
}

@inproceedings{10.1145/3018743.3018770,
author = {Shudler, Sergei and Calotoiu, Alexandru and Hoefler, Torsten and Wolf, Felix},
title = {Isoefficiency in Practice: Configuring and Understanding the Performance of Task-Based Applications},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018770},
doi = {10.1145/3018743.3018770},
abstract = {Task-based programming offers an elegant way to express units of computation and the dependencies among them, making it easier to distribute the computational load evenly across multiple cores. However, this separation of problem decomposition and parallelism requires a sufficiently large input problem to achieve satisfactory efficiency on a given number of cores. Unfortunately, finding a good match between input size and core count usually requires significant experimentation, which is expensive and sometimes even impractical. In this paper, we propose an automated empirical method for finding the isoefficiency function of a task-based program, binding efficiency, core count, and the input size in one analytical expression. This allows the latter two to be adjusted according to given (realistic) efficiency objectives. Moreover, we not only find (i) the actual isoefficiency function but also (ii) the function one would yield if the program execution was free of resource contention and (iii) an upper bound that could only be reached if the program was able to maintain its average parallelism throughout its execution. The difference between the three helps to explain low efficiency, and in particular, it helps to differentiate between resource contention and structural conflicts related to task dependencies or scheduling. The insights gained can be used to co-design programs and shared system resources.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {131–143},
numpages = {13},
keywords = {tasking, parallel programming, performance analysis, co-design, isoefficiency, performance modeling},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018764,
author = {Utterback, Robert and Agrawal, Kunal and Lee, I-Ting Angelina and Kulkarni, Milind},
title = {Processor-Oblivious Record and Replay},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018764},
doi = {10.1145/3155284.3018764},
abstract = {Record-and-replay systems are useful tools for debugging non-deterministic parallel programs by first recording an execution and then replaying that execution to produce the same access pattern. Existing record-and-replay systems generally target thread-based execution models, and record the behaviors and interleavings of individual threads. Dynamic multithreaded languages and libraries, such as the Cilk family, OpenMP, TBB, etc., do not have a notion of threads. Instead, these languages provide a processor-oblivious model of programming, where programs expose task-parallelism using high-level constructs such as spawn/sync without regard to the number of threads/cores available to run the program. Thread-based record-and-replay would violate the processor-oblivious nature of these programs, as they incorporate the number of threads into the recorded information, constraining the replayed execution to the same number of threads.In this paper, we present a processor-oblivious record-and-replay scheme for such languages where record and replay can use different number of processors and both are scheduled using work stealing. We provide theoretical guarantees for our record and replay scheme --- namely that record is optimal for programs with one lock and replay is near-optimal for all cases. In addition, we implemented this scheme in the Cilk Plus runtime system and our evaluation indicates that processor-obliviousness does not cause substantial overheads.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {145–161},
numpages = {17},
keywords = {dynamic program analysis, work stealing, deterministic replay, reproducible debugging}
}

@inproceedings{10.1145/3018743.3018764,
author = {Utterback, Robert and Agrawal, Kunal and Lee, I-Ting Angelina and Kulkarni, Milind},
title = {Processor-Oblivious Record and Replay},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018764},
doi = {10.1145/3018743.3018764},
abstract = {Record-and-replay systems are useful tools for debugging non-deterministic parallel programs by first recording an execution and then replaying that execution to produce the same access pattern. Existing record-and-replay systems generally target thread-based execution models, and record the behaviors and interleavings of individual threads. Dynamic multithreaded languages and libraries, such as the Cilk family, OpenMP, TBB, etc., do not have a notion of threads. Instead, these languages provide a processor-oblivious model of programming, where programs expose task-parallelism using high-level constructs such as spawn/sync without regard to the number of threads/cores available to run the program. Thread-based record-and-replay would violate the processor-oblivious nature of these programs, as they incorporate the number of threads into the recorded information, constraining the replayed execution to the same number of threads.In this paper, we present a processor-oblivious record-and-replay scheme for such languages where record and replay can use different number of processors and both are scheduled using work stealing. We provide theoretical guarantees for our record and replay scheme --- namely that record is optimal for programs with one lock and replay is near-optimal for all cases. In addition, we implemented this scheme in the Cilk Plus runtime system and our evaluation indicates that processor-obliviousness does not cause substantial overheads.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {145–161},
numpages = {17},
keywords = {work stealing, reproducible debugging, deterministic replay, dynamic program analysis},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018744,
author = {Prajapati, Nirmal and Ranasinghe, Waruna and Rajopadhye, Sanjay and Andonov, Rumen and Djidjev, Hristo and Grosser, Tobias},
title = {Simple, Accurate, Analytical Time Modeling and Optimal Tile Size Selection for GPGPU Stencils},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018744},
doi = {10.1145/3155284.3018744},
abstract = {Stencil computations are an important class of compute and data intensive programs that occur widely in scientific and engineeringapplications. A number of tools use sophisticated tiling, parallelization, and memory mapping strategies, and generate code that relies on vendor-supplied compilers. This code has a number of parameters, such as tile sizes, that are then tuned via empirical exploration.We develop a model that guides such a choice. Our model is a simple set of analytical functions that predict the execution time of the generated code. It is deliberately optimistic, since tile sizes and, moreover, the optimistic assumptions are intended to enable we are targeting modeling and parameter selections yielding highly tuned codes.We experimentally validate the model on a number of 2D and 3D stencil codes, and show that the root mean square error in the execution time is less than 10% for the subset of the codes that achieve performance within 20% of the best. Furthermore, based on using our model, we are able to predict tile sizes that achieve a further improvement of 9% on average.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {163–177},
numpages = {15},
keywords = {performance prediction, hybrid hexagonal classic tiling, gpgpu, stencils, polyhedral method, analytical models}
}

@inproceedings{10.1145/3018743.3018744,
author = {Prajapati, Nirmal and Ranasinghe, Waruna and Rajopadhye, Sanjay and Andonov, Rumen and Djidjev, Hristo and Grosser, Tobias},
title = {Simple, Accurate, Analytical Time Modeling and Optimal Tile Size Selection for GPGPU Stencils},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018744},
doi = {10.1145/3018743.3018744},
abstract = {Stencil computations are an important class of compute and data intensive programs that occur widely in scientific and engineeringapplications. A number of tools use sophisticated tiling, parallelization, and memory mapping strategies, and generate code that relies on vendor-supplied compilers. This code has a number of parameters, such as tile sizes, that are then tuned via empirical exploration.We develop a model that guides such a choice. Our model is a simple set of analytical functions that predict the execution time of the generated code. It is deliberately optimistic, since tile sizes and, moreover, the optimistic assumptions are intended to enable we are targeting modeling and parameter selections yielding highly tuned codes.We experimentally validate the model on a number of 2D and 3D stencil codes, and show that the root mean square error in the execution time is less than 10% for the subset of the codes that achieve performance within 20% of the best. Furthermore, based on using our model, we are able to predict tile sizes that achieve a further improvement of 9% on average.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {163–177},
numpages = {15},
keywords = {polyhedral method, analytical models, gpgpu, hybrid hexagonal classic tiling, stencils, performance prediction},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018760,
author = {Jiang, Peng and Agrawal, Gagan},
title = {Combining SIMD and Many/Multi-Core Parallelism for Finite State Machines with Enumerative Speculation},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018760},
doi = {10.1145/3155284.3018760},
abstract = {Finite State Machine (FSM) is the key kernel behind many popular applications, including regular expression matching, text tokenization, and Huffman decoding. Parallelizing FSMs is extremely difficult because of the strong dependencies and unpredictable memory accesses. Previous efforts have largely focused on multi-core parallelization, and used different approaches, including {em speculative} and {em enumerative} execution, both of which have been effective but also have limitations. With increasing width and improving flexibility in SIMD instruction sets, this paper focuses on combining SIMD and multi/many-core parallelism for FSMs. We have developed a novel strategy, called {em enumerative speculation}. Instead of speculating on a single state as in speculative execution or enumerating all possible states as in enumerative execution, our strategy speculates transitions from several possible states, reducing the prediction overheads of speculation approach and the large amount of redundant work in the enumerative approach. A simple lookback approach produces a set of guessed states to achieve high speculation success rates in our enumerative speculation. We evaluate our method with four popular FSM applications: Huffman decoding, regular expression matching, HTML tokenization, and Div7. We obtain up to 2.5x speedup using SIMD on one core and up to 95x combining SIMD with 60 cores of an Intel Xeon Phi. On a single core, we outperform the best single-state speculative execution version by an average of 1.6x, and in combining SIMD and many-core parallelism, outperform enumerative execution by an average of 2x.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {179–191},
numpages = {13},
keywords = {enumerative speculation, simd, finite state machines}
}

@inproceedings{10.1145/3018743.3018760,
author = {Jiang, Peng and Agrawal, Gagan},
title = {Combining SIMD and Many/Multi-Core Parallelism for Finite State Machines with Enumerative Speculation},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018760},
doi = {10.1145/3018743.3018760},
abstract = {Finite State Machine (FSM) is the key kernel behind many popular applications, including regular expression matching, text tokenization, and Huffman decoding. Parallelizing FSMs is extremely difficult because of the strong dependencies and unpredictable memory accesses. Previous efforts have largely focused on multi-core parallelization, and used different approaches, including {em speculative} and {em enumerative} execution, both of which have been effective but also have limitations. With increasing width and improving flexibility in SIMD instruction sets, this paper focuses on combining SIMD and multi/many-core parallelism for FSMs. We have developed a novel strategy, called {em enumerative speculation}. Instead of speculating on a single state as in speculative execution or enumerating all possible states as in enumerative execution, our strategy speculates transitions from several possible states, reducing the prediction overheads of speculation approach and the large amount of redundant work in the enumerative approach. A simple lookback approach produces a set of guessed states to achieve high speculation success rates in our enumerative speculation. We evaluate our method with four popular FSM applications: Huffman decoding, regular expression matching, HTML tokenization, and Div7. We obtain up to 2.5x speedup using SIMD on one core and up to 95x combining SIMD with 60 cores of an Intel Xeon Phi. On a single core, we outperform the best single-state speculative execution version by an average of 1.6x, and in combining SIMD and many-core parallelism, outperform enumerative execution by an average of 2x.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {179–191},
numpages = {13},
keywords = {finite state machines, simd, enumerative speculation},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018769,
author = {Awan, Ammar Ahmad and Hamidouche, Khaled and Hashmi, Jahanzeb Maqbool and Panda, Dhabaleswar K.},
title = {S-Caffe: Co-Designing MPI Runtimes and Caffe for Scalable Deep Learning on Modern GPU Clusters},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018769},
doi = {10.1145/3155284.3018769},
abstract = {Availability of large data sets like ImageNet and massively parallel computation support in modern HPC devices like NVIDIA GPUs have fueled a renewed interest in Deep Learning (DL) algorithms. This has triggered the development of DL frameworks like Caffe, Torch, TensorFlow, and CNTK. However, most DL frameworks have been limited to a single node. In order to scale out DL frameworks and bring HPC capabilities to the DL arena, we propose, S-Caffe; a scalable and distributed Caffe adaptation for modern multi-GPU clusters. With an in-depth analysis of new requirements brought forward by the DL frameworks and limitations of current communication runtimes, we present a co-design of the Caffe framework and the MVAPICH2-GDR MPI runtime. Using the co-design methodology, we modify Caffe's workflow to maximize the overlap of computation and communication with multi-stage data propagation and gradient aggregation schemes. We bring DL-Awareness to the MPI runtime by proposing a hierarchical reduction design that benefits from CUDA-Aware features and provides up to a massive 133x speedup over OpenMPI and 2.6x speedup over MVAPICH2 for 160 GPUs. S-Caffe successfully scales up to 160 K-80 GPUs for GoogLeNet (ImageNet) with a speedup of 2.5x over 32 GPUs. To the best of our knowledge, this is the first framework that scales up to 160 GPUs. Furthermore, even for single node training, S-Caffe shows an improvement of 14% and 9% over Nvidia's optimized Caffe for 8 and 16 GPUs, respectively. In addition, S-Caffe achieves up to 1395 samples per second for the AlexNet model, which is comparable to the performance of Microsoft CNTK.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {193–205},
numpages = {13},
keywords = {mpi_reduce, caffe, deep learning, cuda-aware mpi, distributed training}
}

@inproceedings{10.1145/3018743.3018769,
author = {Awan, Ammar Ahmad and Hamidouche, Khaled and Hashmi, Jahanzeb Maqbool and Panda, Dhabaleswar K.},
title = {S-Caffe: Co-Designing MPI Runtimes and Caffe for Scalable Deep Learning on Modern GPU Clusters},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018769},
doi = {10.1145/3018743.3018769},
abstract = {Availability of large data sets like ImageNet and massively parallel computation support in modern HPC devices like NVIDIA GPUs have fueled a renewed interest in Deep Learning (DL) algorithms. This has triggered the development of DL frameworks like Caffe, Torch, TensorFlow, and CNTK. However, most DL frameworks have been limited to a single node. In order to scale out DL frameworks and bring HPC capabilities to the DL arena, we propose, S-Caffe; a scalable and distributed Caffe adaptation for modern multi-GPU clusters. With an in-depth analysis of new requirements brought forward by the DL frameworks and limitations of current communication runtimes, we present a co-design of the Caffe framework and the MVAPICH2-GDR MPI runtime. Using the co-design methodology, we modify Caffe's workflow to maximize the overlap of computation and communication with multi-stage data propagation and gradient aggregation schemes. We bring DL-Awareness to the MPI runtime by proposing a hierarchical reduction design that benefits from CUDA-Aware features and provides up to a massive 133x speedup over OpenMPI and 2.6x speedup over MVAPICH2 for 160 GPUs. S-Caffe successfully scales up to 160 K-80 GPUs for GoogLeNet (ImageNet) with a speedup of 2.5x over 32 GPUs. To the best of our knowledge, this is the first framework that scales up to 160 GPUs. Furthermore, even for single node training, S-Caffe shows an improvement of 14% and 9% over Nvidia's optimized Caffe for 8 and 16 GPUs, respectively. In addition, S-Caffe achieves up to 1395 samples per second for the AlexNet model, which is comparable to the performance of Microsoft CNTK.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {193–205},
numpages = {13},
keywords = {mpi_reduce, distributed training, deep learning, cuda-aware mpi, caffe},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018765,
author = {Sabne, Amit and Wang, Xiao and Kisner, Sherman J. and Bouman, Charles A. and Raghunathan, Anand and Midkiff, Samuel P.},
title = {Model-Based Iterative CT Image Reconstruction on GPUs},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018765},
doi = {10.1145/3155284.3018765},
abstract = {Computed Tomography (CT) Image Reconstruction is an important technique used in a variety of domains, including medical imaging, electron microscopy, non-destructive testing and transportation security. Model-based Iterative Reconstruction (MBIR) using Iterative Coordinate Descent (ICD) is a CT algorithm that produces state-of-the-art results in terms of image quality. However, MBIR is highly computationally intensive and challenging to parallelize, and has traditionally been viewed as impractical in applications where reconstruction time is critical. We present the first GPU-based algorithm for ICD-based MBIR. The algorithm leverages the recently-proposed concept of SuperVoxels, and efficiently exploits the three levels of parallelism available in MBIR to better utilize the GPU hardware resources. We also explore data layout transformations to obtain more coalesced accesses and several GPU-specific optimizations for MBIR that boost performance. Across a suite of 3200 test cases, our GPU implementation obtains a geometric mean speedup of 4.43X over a state-of-the-art multi-core implementation on a 16-core iso-power CPU.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {207–220},
numpages = {14},
keywords = {model based iterative reconstruction, graphics processing units, computed tomography, iterative coordinate descent}
}

@inproceedings{10.1145/3018743.3018765,
author = {Sabne, Amit and Wang, Xiao and Kisner, Sherman J. and Bouman, Charles A. and Raghunathan, Anand and Midkiff, Samuel P.},
title = {Model-Based Iterative CT Image Reconstruction on GPUs},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018765},
doi = {10.1145/3018743.3018765},
abstract = {Computed Tomography (CT) Image Reconstruction is an important technique used in a variety of domains, including medical imaging, electron microscopy, non-destructive testing and transportation security. Model-based Iterative Reconstruction (MBIR) using Iterative Coordinate Descent (ICD) is a CT algorithm that produces state-of-the-art results in terms of image quality. However, MBIR is highly computationally intensive and challenging to parallelize, and has traditionally been viewed as impractical in applications where reconstruction time is critical. We present the first GPU-based algorithm for ICD-based MBIR. The algorithm leverages the recently-proposed concept of SuperVoxels, and efficiently exploits the three levels of parallelism available in MBIR to better utilize the GPU hardware resources. We also explore data layout transformations to obtain more coalesced accesses and several GPU-specific optimizations for MBIR that boost performance. Across a suite of 3200 test cases, our GPU implementation obtains a geometric mean speedup of 4.43X over a state-of-the-art multi-core implementation on a 16-core iso-power CPU.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {207–220},
numpages = {14},
keywords = {iterative coordinate descent, model based iterative reconstruction, computed tomography, graphics processing units},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018754,
author = {Yeh, Tsung Tai and Sabne, Amit and Sakdhnagool, Putt and Eigenmann, Rudolf and Rogers, Timothy G.},
title = {Pagoda: Fine-Grained GPU Resource Virtualization for Narrow Tasks},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018754},
doi = {10.1145/3155284.3018754},
abstract = {Massively multithreaded GPUs achieve high throughput by running thousands of threads in parallel. To fully utilize the hardware, workloads spawn work to the GPU in bulk by launching large tasks, where each task is a kernel that contains thousands of threads that occupy the entire GPU.GPUs face severe underutilization and their performance benefits vanish if the tasks are narrow, i.e., they contain &lt; 500 threads. Latency-sensitive applications in network, signal, and image processing that generate a large number of tasks with relatively small inputs are examples of such limited parallelism.This paper presents Pagoda, a runtime system that virtualizes GPU resources, using an OS-like daemon kernel called MasterKernel. Tasks are spawned from the CPU onto Pagoda as they become available, and are scheduled by the MasterKernel at the warp granularity. Experimental results demonstrate that Pagoda achieves a geometric mean speedup of 5.70x over PThreads running on a 20-core CPU, 1.51x over CUDA-HyperQ, and 1.69x over GeMTC, the state-of- the-art runtime GPU task scheduling system.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {221–234},
numpages = {14},
keywords = {gpu runtime system, task parallelism, utilization}
}

@inproceedings{10.1145/3018743.3018754,
author = {Yeh, Tsung Tai and Sabne, Amit and Sakdhnagool, Putt and Eigenmann, Rudolf and Rogers, Timothy G.},
title = {Pagoda: Fine-Grained GPU Resource Virtualization for Narrow Tasks},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018754},
doi = {10.1145/3018743.3018754},
abstract = {Massively multithreaded GPUs achieve high throughput by running thousands of threads in parallel. To fully utilize the hardware, workloads spawn work to the GPU in bulk by launching large tasks, where each task is a kernel that contains thousands of threads that occupy the entire GPU.GPUs face severe underutilization and their performance benefits vanish if the tasks are narrow, i.e., they contain &lt; 500 threads. Latency-sensitive applications in network, signal, and image processing that generate a large number of tasks with relatively small inputs are examples of such limited parallelism.This paper presents Pagoda, a runtime system that virtualizes GPU resources, using an OS-like daemon kernel called MasterKernel. Tasks are spawned from the CPU onto Pagoda as they become available, and are scheduled by the MasterKernel at the warp granularity. Experimental results demonstrate that Pagoda achieves a geometric mean speedup of 5.70x over PThreads running on a 20-core CPU, 1.51x over CUDA-HyperQ, and 1.69x over GeMTC, the state-of- the-art runtime GPU task scheduling system.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {221–234},
numpages = {14},
keywords = {utilization, gpu runtime system, task parallelism},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018756,
author = {Ben-Nun, Tal and Sutton, Michael and Pai, Sreepathi and Pingali, Keshav},
title = {Groute: An Asynchronous Multi-GPU Programming Model for Irregular Computations},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018756},
doi = {10.1145/3155284.3018756},
abstract = {Nodes with multiple GPUs are becoming the platform of choice for high-performance computing. However, most applications are written using bulk-synchronous programming models, which may not be optimal for irregular algorithms that benefit from low-latency, asynchronous communication. This paper proposes constructs for asynchronous multi-GPU programming, and describes their implementation in a thin runtime environment called Groute. Groute also implements common collective operations and distributed work-lists, enabling the development of irregular applications without substantial programming effort. We demonstrate that this approach achieves state-of-the-art performance and exhibits strong scaling for a suite of irregular applications on 8-GPU and heterogeneous systems, yielding over 7x speedup for some algorithms.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {235–248},
numpages = {14},
keywords = {asynchronous programming, multi-gpu, irregular algorithms}
}

@inproceedings{10.1145/3018743.3018756,
author = {Ben-Nun, Tal and Sutton, Michael and Pai, Sreepathi and Pingali, Keshav},
title = {Groute: An Asynchronous Multi-GPU Programming Model for Irregular Computations},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018756},
doi = {10.1145/3018743.3018756},
abstract = {Nodes with multiple GPUs are becoming the platform of choice for high-performance computing. However, most applications are written using bulk-synchronous programming models, which may not be optimal for irregular algorithms that benefit from low-latency, asynchronous communication. This paper proposes constructs for asynchronous multi-GPU programming, and describes their implementation in a thin runtime environment called Groute. Groute also implements common collective operations and distributed work-lists, enabling the development of irregular applications without substantial programming effort. We demonstrate that this approach achieves state-of-the-art performance and exhibits strong scaling for a suite of irregular applications on 8-GPU and heterogeneous systems, yielding over 7x speedup for some algorithms.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {235–248},
numpages = {14},
keywords = {multi-gpu, irregular algorithms, asynchronous programming},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018758,
author = {Schardl, Tao B. and Moses, William S. and Leiserson, Charles E.},
title = {Tapir: Embedding Fork-Join Parallelism into LLVM's Intermediate Representation},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018758},
doi = {10.1145/3155284.3018758},
abstract = {This paper explores how fork-join parallelism, as supported by concurrency platforms such as Cilk and OpenMP, can be embedded into a compiler's intermediate representation (IR). Mainstream compilers typically treat parallel linguistic constructs as syntactic sugar for function calls into a parallel runtime. These calls prevent the compiler from performing optimizations across parallel control constructs. Remedying this situation is generally thought to require an extensive reworking of compiler analyses and code transformations to handle parallel semantics.Tapir is a compiler IR that represents logically parallel tasks asymmetrically in the program's control flow graph. Tapir allows the compiler to optimize across parallel control constructs with only minor changes to its existing analyses and code transformations. To prototype Tapir in the LLVM compiler, for example, we added or modified about 6000 lines of LLVM's 4-million-line codebase. Tapir enables LLVM's existing compiler optimizations for serial code -- including loop-invariant-code motion, common-subexpression elimination, and tail-recursion elimination -- to work with parallel control constructs such as spawning and parallel loops. Tapir also supports parallel optimizations such as loop scheduling.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {249–265},
numpages = {17},
keywords = {multicore, fork-join parallelism, tapir, llvm, control-flow graph, compiling, serial semantics, optimization, par- allel computing, cilk, openmp}
}

@inproceedings{10.1145/3018743.3018758,
author = {Schardl, Tao B. and Moses, William S. and Leiserson, Charles E.},
title = {Tapir: Embedding Fork-Join Parallelism into LLVM's Intermediate Representation},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018758},
doi = {10.1145/3018743.3018758},
abstract = {This paper explores how fork-join parallelism, as supported by concurrency platforms such as Cilk and OpenMP, can be embedded into a compiler's intermediate representation (IR). Mainstream compilers typically treat parallel linguistic constructs as syntactic sugar for function calls into a parallel runtime. These calls prevent the compiler from performing optimizations across parallel control constructs. Remedying this situation is generally thought to require an extensive reworking of compiler analyses and code transformations to handle parallel semantics.Tapir is a compiler IR that represents logically parallel tasks asymmetrically in the program's control flow graph. Tapir allows the compiler to optimize across parallel control constructs with only minor changes to its existing analyses and code transformations. To prototype Tapir in the LLVM compiler, for example, we added or modified about 6000 lines of LLVM's 4-million-line codebase. Tapir enables LLVM's existing compiler optimizations for serial code -- including loop-invariant-code motion, common-subexpression elimination, and tail-recursion elimination -- to work with parallel control constructs such as spawning and parallel loops. Tapir also supports parallel optimizations such as loop scheduling.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {249–265},
numpages = {17},
keywords = {openmp, llvm, compiling, tapir, control-flow graph, multicore, par- allel computing, cilk, serial semantics, fork-join parallelism, optimization},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018766,
author = {Matveev, Alexander and Meirovitch, Yaron and Saribekyan, Hayk and Jakubiuk, Wiktor and Kaler, Tim and Odor, Gergely and Budden, David and Zlateski, Aleksandar and Shavit, Nir},
title = {A Multicore Path to Connectomics-on-Demand},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018766},
doi = {10.1145/3155284.3018766},
abstract = {The current design trend in large scale machine learning is to use distributed clusters of CPUs and GPUs with MapReduce-style programming. Some have been led to believe that this type of horizontal scaling can reduce or even eliminate the need for traditional algorithm development, careful parallelization, and performance engineering. This paper is a case study showing the contrary: that the benefits of algorithms, parallelization, and performance engineering, can sometimes be so vast that it is possible to solve "cluster-scale" problems on a single commodity multicore machine.Connectomics is an emerging area of neurobiology that uses cutting edge machine learning and image processing to extract brain connectivity graphs from electron microscopy images. It has long been assumed that the processing of connectomics data will require mass storage, farms of CPU/GPUs, and will take months (if not years) of processing time. We present a high-throughput connectomics-on-demand system that runs on a multicore machine with less than 100 cores and extracts connectomes at the terabyte per hour pace of modern electron microscopes.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {267–281},
numpages = {15},
keywords = {multicore programming, machine learning}
}

@inproceedings{10.1145/3018743.3018766,
author = {Matveev, Alexander and Meirovitch, Yaron and Saribekyan, Hayk and Jakubiuk, Wiktor and Kaler, Tim and Odor, Gergely and Budden, David and Zlateski, Aleksandar and Shavit, Nir},
title = {A Multicore Path to Connectomics-on-Demand},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018766},
doi = {10.1145/3018743.3018766},
abstract = {The current design trend in large scale machine learning is to use distributed clusters of CPUs and GPUs with MapReduce-style programming. Some have been led to believe that this type of horizontal scaling can reduce or even eliminate the need for traditional algorithm development, careful parallelization, and performance engineering. This paper is a case study showing the contrary: that the benefits of algorithms, parallelization, and performance engineering, can sometimes be so vast that it is possible to solve "cluster-scale" problems on a single commodity multicore machine.Connectomics is an emerging area of neurobiology that uses cutting edge machine learning and image processing to extract brain connectivity graphs from electron microscopy images. It has long been assumed that the processing of connectomics data will require mass storage, farms of CPU/GPUs, and will take months (if not years) of processing time. We present a high-throughput connectomics-on-demand system that runs on a multicore machine with less than 100 cores and extracts connectomes at the terabyte per hour pace of modern electron microscopes.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {267–281},
numpages = {15},
keywords = {multicore programming, machine learning},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018746,
author = {Vollmer, Michael and Scott, Ryan G. and Musuvathi, Madanlal and Newton, Ryan R.},
title = {SC-Haskell: Sequential Consistency in Languages That Minimize Mutable Shared Heap},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018746},
doi = {10.1145/3155284.3018746},
abstract = {A core, but often neglected, aspect of a programming language design is its memory (consistency) model. Sequential consistency~(SC) is the most intuitive memory model for programmers as it guarantees sequential composition of instructions and provides a simple abstraction of shared memory as a single global store with atomic read and writes. Unfortunately, SC is widely considered to be impractical due to its associated performance overheads. Perhaps contrary to popular opinion, this paper demonstrates that SC is achievable with acceptable performance overheads for mainstream languages that minimize mutable shared heap. In particular, we modify the Glasgow Haskell Compiler to insert fences on all writes to shared mutable memory accessed in nonfunctional parts of the program. For a benchmark suite containing 1,279 programs, SC adds a geomean overhead of less than 0.4% on an x86 machine. The efficiency of SC arises primarily due to the isolation provided by the Haskell type system between purely functional and thread-local imperative computations on the one hand, and imperative computations on the global heap on the other. We show how to use new programming idioms to further reduce the SC overhead; these create a virtuous cycle of less overhead and even stronger semantic guarantees (static data-race freedom).},
journal = {SIGPLAN Not.},
month = {jan},
pages = {283–298},
numpages = {16},
keywords = {sequential consistency, functional programming, memory models}
}

@inproceedings{10.1145/3018743.3018746,
author = {Vollmer, Michael and Scott, Ryan G. and Musuvathi, Madanlal and Newton, Ryan R.},
title = {SC-Haskell: Sequential Consistency in Languages That Minimize Mutable Shared Heap},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018746},
doi = {10.1145/3018743.3018746},
abstract = {A core, but often neglected, aspect of a programming language design is its memory (consistency) model. Sequential consistency~(SC) is the most intuitive memory model for programmers as it guarantees sequential composition of instructions and provides a simple abstraction of shared memory as a single global store with atomic read and writes. Unfortunately, SC is widely considered to be impractical due to its associated performance overheads. Perhaps contrary to popular opinion, this paper demonstrates that SC is achievable with acceptable performance overheads for mainstream languages that minimize mutable shared heap. In particular, we modify the Glasgow Haskell Compiler to insert fences on all writes to shared mutable memory accessed in nonfunctional parts of the program. For a benchmark suite containing 1,279 programs, SC adds a geomean overhead of less than 0.4% on an x86 machine. The efficiency of SC arises primarily due to the isolation provided by the Haskell type system between purely functional and thread-local imperative computations on the one hand, and imperative computations on the global heap on the other. We show how to use new programming idioms to further reduce the SC overhead; these create a virtuous cycle of less overhead and even stronger semantic guarantees (static data-race freedom).},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {283–298},
numpages = {16},
keywords = {memory models, functional programming, sequential consistency},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018747,
author = {B\"{a}ttig, Martin and Gross, Thomas R.},
title = {Synchronized-by-Default Concurrency for Shared-Memory Systems},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018747},
doi = {10.1145/3155284.3018747},
abstract = {We explore a programming approach for concurrency that synchronizes all accesses to shared memory by default. Synchronization takes place by ensuring that all program code runs inside atomic sections even if the program code has external side effects. Threads are mapped to atomic sections that a programmer must explicitly split to increase concurrency.A naive implementation of this approach incurs a large amount of overhead. We show how to reduce this overhead to make the approach suitable for realistic application programs on existing hardware. We present an implementation technique based on a special-purpose software transactional memory system. To reduce the overhead, the technique exploits properties of managed, object-oriented programming languages as well as intraprocedural static analyses and uses field-level granularity locking in combination with transactional I/O to provide good scaling properties.We implemented the synchronized-by-default (SBD) approach for the Java language and evaluate its performance for six programs from the DaCapo benchmark suite. The evaluation shows that, compared to explicit synchronization, the SBD approach has an overhead between 0.4% and 102% depending on the benchmark and the number of threads, with a mean (geom.) of 23.9%.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {299–312},
numpages = {14},
keywords = {parallel programming, concurrency, synchronization, transactional memory, atomic blocks}
}

@inproceedings{10.1145/3018743.3018747,
author = {B\"{a}ttig, Martin and Gross, Thomas R.},
title = {Synchronized-by-Default Concurrency for Shared-Memory Systems},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018747},
doi = {10.1145/3018743.3018747},
abstract = {We explore a programming approach for concurrency that synchronizes all accesses to shared memory by default. Synchronization takes place by ensuring that all program code runs inside atomic sections even if the program code has external side effects. Threads are mapped to atomic sections that a programmer must explicitly split to increase concurrency.A naive implementation of this approach incurs a large amount of overhead. We show how to reduce this overhead to make the approach suitable for realistic application programs on existing hardware. We present an implementation technique based on a special-purpose software transactional memory system. To reduce the overhead, the technique exploits properties of managed, object-oriented programming languages as well as intraprocedural static analyses and uses field-level granularity locking in combination with transactional I/O to provide good scaling properties.We implemented the synchronized-by-default (SBD) approach for the Java language and evaluate its performance for six programs from the DaCapo benchmark suite. The evaluation shows that, compared to explicit synchronization, the SBD approach has an overhead between 0.4% and 102% depending on the benchmark and the number of threads, with a mean (geom.) of 23.9%.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {299–312},
numpages = {14},
keywords = {synchronization, parallel programming, atomic blocks, transactional memory, concurrency},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018751,
author = {Moreira, Rubens E.A. and Collange, Sylvain and Quint\~{a}o Pereira, Fernando Magno},
title = {Function Call Re-Vectorization},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018751},
doi = {10.1145/3155284.3018751},
abstract = {Programming languages such as C for CUDA, OpenCL or ISPC have contributed to increase the programmability of SIMD accelerators and graphics processing units. However, these languages still lack the flexibility offered by low-level SIMD programming on explicit vectors. To close this expressiveness gap while preserving performance, this paper introduces the notion of ourinvention{} (CREV). CREV allows changing the dimension of vectorization during the execution of a kernel, exposing it as a nested parallel kernel call. CREV affords programmability close to dynamic parallelism, a feature that allows the invocation of kernels from inside kernels, but at much lower cost. In this paper, we present a formal semantics of CREV, and an implementation of it on the ISPC compiler. We have used CREV to implement some classic algorithms, including string matching, depth first search and Bellman-Ford, with minimum effort. These algorithms, once compiled by ISPC to Intel-based vector instructions, are as fast as state-of-the-art implementations, yet much simpler. Thus, CREV gives developers the elegance of dynamic programming, and the performance of explicit SIMD programming.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {313–326},
numpages = {14},
keywords = {function, programmability, simd, simt}
}

@inproceedings{10.1145/3018743.3018751,
author = {Moreira, Rubens E.A. and Collange, Sylvain and Quint\~{a}o Pereira, Fernando Magno},
title = {Function Call Re-Vectorization},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018751},
doi = {10.1145/3018743.3018751},
abstract = {Programming languages such as C for CUDA, OpenCL or ISPC have contributed to increase the programmability of SIMD accelerators and graphics processing units. However, these languages still lack the flexibility offered by low-level SIMD programming on explicit vectors. To close this expressiveness gap while preserving performance, this paper introduces the notion of ourinvention{} (CREV). CREV allows changing the dimension of vectorization during the execution of a kernel, exposing it as a nested parallel kernel call. CREV affords programmability close to dynamic parallelism, a feature that allows the invocation of kernels from inside kernels, but at much lower cost. In this paper, we present a formal semantics of CREV, and an implementation of it on the ISPC compiler. We have used CREV to implement some classic algorithms, including string matching, depth first search and Bellman-Ford, with minimum effort. These algorithms, once compiled by ISPC to Intel-based vector instructions, are as fast as state-of-the-art implementations, yet much simpler. Thus, CREV gives developers the elegance of dynamic programming, and the performance of explicit SIMD programming.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {313–326},
numpages = {14},
keywords = {programmability, simd, simt, function},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018771,
author = {Rajbhandari, Samyam and Rastello, Fabrice and Kowalski, Karol and Krishnamoorthy, Sriram and Sadayappan, P.},
title = {Optimizing the Four-Index Integral Transform Using Data Movement Lower Bounds Analysis},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018771},
doi = {10.1145/3155284.3018771},
abstract = {The four-index integral transform is a fundamental and computationally demanding calculation used in many computational chemistry suites such as NWChem. It transforms a four-dimensional tensor from one basis to another. This transformation is most efficiently implemented as a sequence of four tensor contractions that each contract a four- dimensional tensor with a two-dimensional transformation matrix. Differing degrees of permutation symmetry in the intermediate and final tensors in the sequence of contractions cause intermediate tensors to be much larger than the final tensor and limit the number of electronic states in the modeled systems.Loop fusion, in conjunction with tiling, can be very effective in reducing the total space requirement, as well as data movement. However, the large number of possible choices for loop fusion and tiling, and data/computation distribution across a parallel system, make it challenging to develop an optimized parallel implementation for the four-index integral transform. We develop a novel approach to address this problem, using lower bounds modeling of data movement complexity. We establish relationships between available aggregate physical memory in a parallel computer system and ineffective fusion configurations, enabling their pruning and consequent identification of effective choices and a characterization of optimality criteria. This work has resulted in the development of a significantly improved implementation of the four-index transform that enables higher performance and the ability to model larger electronic systems than the current implementation in the NWChem quantum chemistry software suite.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {327–340},
numpages = {14},
keywords = {tensor contraction, fusion, 4-index, lower bounds, communication optimization, tensors, optimal schedule, parallel algorithm, optimizing 4-index transform, processor mapping, scheduling, distributed algorithm, four-index}
}

@inproceedings{10.1145/3018743.3018771,
author = {Rajbhandari, Samyam and Rastello, Fabrice and Kowalski, Karol and Krishnamoorthy, Sriram and Sadayappan, P.},
title = {Optimizing the Four-Index Integral Transform Using Data Movement Lower Bounds Analysis},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018771},
doi = {10.1145/3018743.3018771},
abstract = {The four-index integral transform is a fundamental and computationally demanding calculation used in many computational chemistry suites such as NWChem. It transforms a four-dimensional tensor from one basis to another. This transformation is most efficiently implemented as a sequence of four tensor contractions that each contract a four- dimensional tensor with a two-dimensional transformation matrix. Differing degrees of permutation symmetry in the intermediate and final tensors in the sequence of contractions cause intermediate tensors to be much larger than the final tensor and limit the number of electronic states in the modeled systems.Loop fusion, in conjunction with tiling, can be very effective in reducing the total space requirement, as well as data movement. However, the large number of possible choices for loop fusion and tiling, and data/computation distribution across a parallel system, make it challenging to develop an optimized parallel implementation for the four-index integral transform. We develop a novel approach to address this problem, using lower bounds modeling of data movement complexity. We establish relationships between available aggregate physical memory in a parallel computer system and ineffective fusion configurations, enabling their pruning and consequent identification of effective choices and a characterization of optimality criteria. This work has resulted in the development of a significantly improved implementation of the four-index transform that enables higher performance and the ability to model larger electronic systems than the current implementation in the NWChem quantum chemistry software suite.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {327–340},
numpages = {14},
keywords = {lower bounds, parallel algorithm, fusion, processor mapping, distributed algorithm, scheduling, four-index, tensor contraction, tensors, 4-index, optimal schedule, optimizing 4-index transform, communication optimization},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018757,
author = {Steele, Guy L. and Tristan, Jean-Baptiste},
title = {Using Butterfly-Patterned Partial Sums to Draw from Discrete Distributions},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018757},
doi = {10.1145/3155284.3018757},
abstract = {We describe a SIMD technique for drawing values from multiple discrete distributions, such as sampling from the random variables of a mixture model, that avoids computing a complete table of partial sums of the relative probabilities. A table of alternate ("butterfly-patterned") form is faster to compute, making better use of coalesced memory accesses; from this table, complete partial sums are computed on the fly during a binary search. Measurements using CUDA 7.5 on an NVIDIA Titan Black GPU show that this technique makes an entire machine-learning application that uses a Latent Dirichlet Allocation topic model with 1024 topics about about 13% faster (when using single-precision floating-point data) or about 35% faster (when using double-precision floating-point data) than doing a straightforward matrix transposition after using coalesced accesses.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {341–355},
numpages = {15},
keywords = {transposed memory access, lda, random sampling, gpu, coalesced memory access, multithreading, machine learning, discrete distribution, simd, latent dirichlet allocation, butterfly, parallel computing, memory bottleneck}
}

@inproceedings{10.1145/3018743.3018757,
author = {Steele, Guy L. and Tristan, Jean-Baptiste},
title = {Using Butterfly-Patterned Partial Sums to Draw from Discrete Distributions},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018757},
doi = {10.1145/3018743.3018757},
abstract = {We describe a SIMD technique for drawing values from multiple discrete distributions, such as sampling from the random variables of a mixture model, that avoids computing a complete table of partial sums of the relative probabilities. A table of alternate ("butterfly-patterned") form is faster to compute, making better use of coalesced memory accesses; from this table, complete partial sums are computed on the fly during a binary search. Measurements using CUDA 7.5 on an NVIDIA Titan Black GPU show that this technique makes an entire machine-learning application that uses a Latent Dirichlet Allocation topic model with 1024 topics about about 13% faster (when using single-precision floating-point data) or about 35% faster (when using double-precision floating-point data) than doing a straightforward matrix transposition after using coalesced accesses.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {341–355},
numpages = {15},
keywords = {memory bottleneck, transposed memory access, gpu, parallel computing, machine learning, butterfly, discrete distribution, simd, multithreading, lda, latent dirichlet allocation, random sampling, coalesced memory access},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018761,
author = {Basin, Dmitry and Bortnikov, Edward and Braginsky, Anastasia and Golan-Gueta, Guy and Hillel, Eshcar and Keidar, Idit and Sulamy, Moshe},
title = {KiWi: A Key-Value Map for Scalable Real-Time Analytics},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018761},
doi = {10.1145/3155284.3018761},
abstract = {Modern big data processing platforms employ huge in-memory key-value (KV) maps. Their applications simultaneously drive high-rate data ingestion and large-scale analytics. These two scenarios expect KV-map implementations that scale well with both real-time updates and large atomic scans triggered by range queries.We present KiWi, the first atomic KV-map to efficiently support simultaneous large scans and real-time access. The key to achieving this is treating scans as first class citizens,and organizing the data structure around them. KiWi provides wait-free scans, whereas its put operations are lightweight and lock-free. It optimizes memory management jointly with data structure access.We implement KiWi and compare it to state-of-the-art solutions. Compared to other KV-maps providing atomic scans, KiWi performs either long scans or concurrent puts an order of magnitude faster. Its scans are twice as fast as non-atomic ones implemented via iterators in the Java skiplist.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {357–369},
numpages = {13},
keywords = {lock-free key-value map, wait-free atomic scans}
}

@inproceedings{10.1145/3018743.3018761,
author = {Basin, Dmitry and Bortnikov, Edward and Braginsky, Anastasia and Golan-Gueta, Guy and Hillel, Eshcar and Keidar, Idit and Sulamy, Moshe},
title = {KiWi: A Key-Value Map for Scalable Real-Time Analytics},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018761},
doi = {10.1145/3018743.3018761},
abstract = {Modern big data processing platforms employ huge in-memory key-value (KV) maps. Their applications simultaneously drive high-rate data ingestion and large-scale analytics. These two scenarios expect KV-map implementations that scale well with both real-time updates and large atomic scans triggered by range queries.We present KiWi, the first atomic KV-map to efficiently support simultaneous large scans and real-time access. The key to achieving this is treating scans as first class citizens,and organizing the data structure around them. KiWi provides wait-free scans, whereas its put operations are lightweight and lock-free. It optimizes memory management jointly with data structure access.We implement KiWi and compare it to state-of-the-art solutions. Compared to other KV-maps providing atomic scans, KiWi performs either long scans or concurrent puts an order of magnitude faster. Its scans are twice as fast as non-atomic ones implemented via iterators in the Java skiplist.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {357–369},
numpages = {13},
keywords = {lock-free key-value map, wait-free atomic scans},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018772,
author = {Jiang, Lin and Zhao, Zhijia},
title = {Grammar-Aware Parallelization for Scalable XPath Querying},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018772},
doi = {10.1145/3155284.3018772},
abstract = {Semi-structured data emerge in many domains, especially in web analytics and business intelligence. However, querying such data is inherently sequential due to the nested structure of input data. Existing solutions pessimistically enumerate all execution paths to circumvent dependencies, yielding sub-optimal performance and limited scalability.This paper presents GAP, a parallelization scheme that, for the first time, leverages the grammar of the input data to boost the parallelization efficiency. GAP leverages static analysis to infer feasible execution paths for specific con- texts based on the grammar of the semi-structured data. It can eliminate unnecessary paths without compromising the correctness. In the absence of a pre-defined grammar, GAP switches into a speculative execution mode and takes potentially incomplete grammar extracted either from prior inputs. Together, the dual-mode GAP reduces the execution paths from all paths to a minimum, therefore maximizing the parallelization efficiency and scalability. The benefits of path elimination go beyond reducing extra computation -- it also enables the use of more efficient data structures, which further improves the efficiency. An evaluation on a large set of standard benchmarks with diverse queries shows that GAP yields significant efficiency increase and boosts the speedup of the state-of-the-art from 2.9X to 17.6X on a 20-core ma- chine for a set of 200 queries.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {371–383},
numpages = {13},
keywords = {xml, pushdown transducer, xpath, parallel pushdown transducer, grammar, speculation, parallelization}
}

@inproceedings{10.1145/3018743.3018772,
author = {Jiang, Lin and Zhao, Zhijia},
title = {Grammar-Aware Parallelization for Scalable XPath Querying},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018772},
doi = {10.1145/3018743.3018772},
abstract = {Semi-structured data emerge in many domains, especially in web analytics and business intelligence. However, querying such data is inherently sequential due to the nested structure of input data. Existing solutions pessimistically enumerate all execution paths to circumvent dependencies, yielding sub-optimal performance and limited scalability.This paper presents GAP, a parallelization scheme that, for the first time, leverages the grammar of the input data to boost the parallelization efficiency. GAP leverages static analysis to infer feasible execution paths for specific con- texts based on the grammar of the semi-structured data. It can eliminate unnecessary paths without compromising the correctness. In the absence of a pre-defined grammar, GAP switches into a speculative execution mode and takes potentially incomplete grammar extracted either from prior inputs. Together, the dual-mode GAP reduces the execution paths from all paths to a minimum, therefore maximizing the parallelization efficiency and scalability. The benefits of path elimination go beyond reducing extra computation -- it also enables the use of more efficient data structures, which further improves the efficiency. An evaluation on a large set of standard benchmarks with diverse queries shows that GAP yields significant efficiency increase and boosts the speedup of the state-of-the-art from 2.9X to 17.6X on a 20-core ma- chine for a set of 200 queries.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {371–383},
numpages = {13},
keywords = {xpath, xml, pushdown transducer, speculation, parallelization, parallel pushdown transducer, grammar},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018752,
author = {Wang, Xin and Zhang, Weihua and Wang, Zhaoguo and Wei, Ziyun and Chen, Haibo and Zhao, Wenyun},
title = {Eunomia: Scaling Concurrent Search Trees under Contention Using HTM},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018752},
doi = {10.1145/3155284.3018752},
abstract = {While hardware transactional memory (HTM) has recently been adopted to construct efficient concurrent search tree structures, such designs fail to deliver scalable performance under contention. In this paper, we first conduct a detailed analysis on an HTM-based concurrent B+Tree, which uncovers several reasons for excessive HTM aborts induced by both false and true conflicts under contention. Based on the analysis, we advocate Eunomia, a design pattern for search trees which contains several principles to reduce HTM aborts, including splitting HTM regions with version-based concurrency control to reduce HTM working sets, partitioned data layout to reduce false conflicts, proactively detecting and avoiding true conflicts, and adaptive concurrency control. To validate their effectiveness, we apply such designs to construct a scalable concurrent B+Tree using HTM. Evaluation using key-value store benchmarks on a 20-core HTM-capable multi-core machine shows that Eunomia leads to 5X-11X speedup under high contention, while incurring small overhead under low contention.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {385–399},
numpages = {15},
keywords = {concurrent search tree, opportunistic consistency, hardware transactional memory}
}

@inproceedings{10.1145/3018743.3018752,
author = {Wang, Xin and Zhang, Weihua and Wang, Zhaoguo and Wei, Ziyun and Chen, Haibo and Zhao, Wenyun},
title = {Eunomia: Scaling Concurrent Search Trees under Contention Using HTM},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018752},
doi = {10.1145/3018743.3018752},
abstract = {While hardware transactional memory (HTM) has recently been adopted to construct efficient concurrent search tree structures, such designs fail to deliver scalable performance under contention. In this paper, we first conduct a detailed analysis on an HTM-based concurrent B+Tree, which uncovers several reasons for excessive HTM aborts induced by both false and true conflicts under contention. Based on the analysis, we advocate Eunomia, a design pattern for search trees which contains several principles to reduce HTM aborts, including splitting HTM regions with version-based concurrency control to reduce HTM working sets, partitioned data layout to reduce false conflicts, proactively detecting and avoiding true conflicts, and adaptive concurrency control. To validate their effectiveness, we apply such designs to construct a scalable concurrent B+Tree using HTM. Evaluation using key-value store benchmarks on a 20-core HTM-capable multi-core machine shows that Eunomia leads to 5X-11X speedup under high contention, while incurring small overhead under low contention.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {385–399},
numpages = {15},
keywords = {concurrent search tree, hardware transactional memory, opportunistic consistency},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018745,
author = {Tang, Xiongchao and Zhai, Jidong and Yu, Bowen and Chen, Wenguang and Zheng, Weimin},
title = {Self-Checkpoint: An In-Memory Checkpoint Method Using Less Space and Its Practice on Fault-Tolerant HPL},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018745},
doi = {10.1145/3155284.3018745},
abstract = {Fault tolerance is increasingly important in high performance computing due to the substantial growth of system scale and decreasing system reliability. In-memory/diskless checkpoint has gained extensive attention as a solution to avoid the IO bottleneck of traditional disk-based checkpoint methods. However, applications using previous in-memory checkpoint suffer from little available memory space. To provide high reliability, previous in-memory checkpoint methods either need to keep two copies of checkpoints to tolerate failures while updating old checkpoints or trade performance for space by flushing in-memory checkpoints into disk.In this paper, we propose a novel in-memory checkpoint method, called self-checkpoint, which can not only achieve the same reliability of previous in-memory checkpoint methods, but also increase the available memory space for applications by almost 50%. To validate our method, we apply the self-checkpoint to an important problem, fault tolerant HPL. We implement a scalable and fault tolerant HPL based on this new method, called SKT-HPL, and validate it on two large-scale systems. Experimental results with 24,576 processes show that SKT-HPL achieves over 95% of the performance of the original HPL. Compared to the state-of-the-art in-memory checkpoint method, it improves the available memory size by 47% and the performance by 5%.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {401–413},
numpages = {13},
keywords = {in-memory checkpoint, memory consumption, fault tolerance, fault-tolerant hpl}
}

@inproceedings{10.1145/3018743.3018745,
author = {Tang, Xiongchao and Zhai, Jidong and Yu, Bowen and Chen, Wenguang and Zheng, Weimin},
title = {Self-Checkpoint: An In-Memory Checkpoint Method Using Less Space and Its Practice on Fault-Tolerant HPL},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018745},
doi = {10.1145/3018743.3018745},
abstract = {Fault tolerance is increasingly important in high performance computing due to the substantial growth of system scale and decreasing system reliability. In-memory/diskless checkpoint has gained extensive attention as a solution to avoid the IO bottleneck of traditional disk-based checkpoint methods. However, applications using previous in-memory checkpoint suffer from little available memory space. To provide high reliability, previous in-memory checkpoint methods either need to keep two copies of checkpoints to tolerate failures while updating old checkpoints or trade performance for space by flushing in-memory checkpoints into disk.In this paper, we propose a novel in-memory checkpoint method, called self-checkpoint, which can not only achieve the same reliability of previous in-memory checkpoint methods, but also increase the available memory space for applications by almost 50%. To validate our method, we apply the self-checkpoint to an important problem, fault tolerant HPL. We implement a scalable and fault tolerant HPL based on this new method, called SKT-HPL, and validate it on two large-scale systems. Experimental results with 24,576 processes show that SKT-HPL achieves over 95% of the performance of the original HPL. Compared to the state-of-the-art in-memory checkpoint method, it improves the available memory size by 47% and the performance by 5%.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {401–413},
numpages = {13},
keywords = {fault tolerance, fault-tolerant hpl, in-memory checkpoint, memory consumption},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018750,
author = {Wu, Panruo and DeBardeleben, Nathan and Guan, Qiang and Blanchard, Sean and Chen, Jieyang and Tao, Dingwen and Liang, Xin and Ouyang, Kaiming and Chen, Zizhong},
title = {Silent Data Corruption Resilient Two-Sided Matrix Factorizations},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018750},
doi = {10.1145/3155284.3018750},
abstract = {This paper presents an algorithm based fault tolerance method to harden three two-sided matrix factorizations against soft errors: reduction to Hessenberg form, tridiagonal form, and bidiagonal form. These two sided factorizations are usually the prerequisites to computing eigenvalues/eigenvectors and singular value decomposition. Algorithm based fault tolerance has been shown to work on three main one-sided matrix factorizations: LU, Cholesky, and QR, but extending it to cover two sided factorizations is non-trivial because there are no obvious textit{offline, problem} specific maintenance of checksums. We thus develop an textit{online, algorithm} specific checksum scheme and show how to systematically adapt the two sided factorization algorithms used in LAPACK and ScaLAPACK packages to introduce the algorithm based fault tolerance.The resulting ABFT scheme can detect and correct arithmetic errors textit{continuously} during the factorizations that allow timely error handling. Detailed analysis and experiments are conducted to show the cost and the gain in resilience. We demonstrate that our scheme covers a significant portion of the operations of the factorizations. Our checksum scheme achieves high error detection coverage and error correction coverage compared to the state of the art, with low overhead and high scalability.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {415–427},
numpages = {13},
keywords = {eigenvalue decomposition, singular value decomposition, abft, algorithm based fault tolerance, svd}
}

@inproceedings{10.1145/3018743.3018750,
author = {Wu, Panruo and DeBardeleben, Nathan and Guan, Qiang and Blanchard, Sean and Chen, Jieyang and Tao, Dingwen and Liang, Xin and Ouyang, Kaiming and Chen, Zizhong},
title = {Silent Data Corruption Resilient Two-Sided Matrix Factorizations},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018750},
doi = {10.1145/3018743.3018750},
abstract = {This paper presents an algorithm based fault tolerance method to harden three two-sided matrix factorizations against soft errors: reduction to Hessenberg form, tridiagonal form, and bidiagonal form. These two sided factorizations are usually the prerequisites to computing eigenvalues/eigenvectors and singular value decomposition. Algorithm based fault tolerance has been shown to work on three main one-sided matrix factorizations: LU, Cholesky, and QR, but extending it to cover two sided factorizations is non-trivial because there are no obvious textit{offline, problem} specific maintenance of checksums. We thus develop an textit{online, algorithm} specific checksum scheme and show how to systematically adapt the two sided factorization algorithms used in LAPACK and ScaLAPACK packages to introduce the algorithm based fault tolerance.The resulting ABFT scheme can detect and correct arithmetic errors textit{continuously} during the factorizations that allow timely error handling. Detailed analysis and experiments are conducted to show the cost and the gain in resilience. We demonstrate that our scheme covers a significant portion of the operations of the factorizations. Our checksum scheme achieves high error detection coverage and error correction coverage compared to the state of the art, with low overhead and high scalability.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {415–427},
numpages = {13},
keywords = {svd, singular value decomposition, algorithm based fault tolerance, eigenvalue decomposition, abft},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019035,
author = {Arbel-Raviv, Maya and Brown, Trevor},
title = {POSTER: Reuse, Don't Recycle: Transforming Algorithms That Throw Away Descriptors},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019035},
doi = {10.1145/3155284.3019035},
abstract = {Lock-free algorithms guarantee progress by having threads help one another. Complex lock-free operations facilitate helping by creating descriptor objects that describe how other threads should help them. In many lock-free algorithms, a new descriptor is allocated for each operation. After an operation completes, its descriptor must be reclaimed by a memory reclamation scheme. Allocating and reclaiming descriptors introduces significant space and time overhead.We present a transformation for a class of lock-free algorithms that allows each thread to efficiently reuse a single descriptor. Experiments on a variety of workloads show that our transformation yields significant improvements over implementations that reclaim descriptors.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {429–430},
numpages = {2},
keywords = {lock-free, concurrent data structures, synchronization}
}

@inproceedings{10.1145/3018743.3019035,
author = {Arbel-Raviv, Maya and Brown, Trevor},
title = {POSTER: Reuse, Don't Recycle: Transforming Algorithms That Throw Away Descriptors},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019035},
doi = {10.1145/3018743.3019035},
abstract = {Lock-free algorithms guarantee progress by having threads help one another. Complex lock-free operations facilitate helping by creating descriptor objects that describe how other threads should help them. In many lock-free algorithms, a new descriptor is allocated for each operation. After an operation completes, its descriptor must be reclaimed by a memory reclamation scheme. Allocating and reclaiming descriptors introduces significant space and time overhead.We present a transformation for a class of lock-free algorithms that allows each thread to efficiently reuse a single descriptor. Experiments on a variety of workloads show that our transformation yields significant improvements over implementations that reclaim descriptors.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {429–430},
numpages = {2},
keywords = {synchronization, concurrent data structures, lock-free},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019030,
author = {Balaji, Vignesh and Tirumala, Dhruva and Lucia, Brandon},
title = {POSTER: An Architecture and Programming Model for Accelerating Parallel Commutative Computations via Privatization},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019030},
doi = {10.1145/3155284.3019030},
abstract = {Synchronization and data movement are the key impediments to an efficient parallel execution. To ensure that data shared by multiple threads remain consistent, the programmer must use synchronization (e.g., mutex locks) to serialize threads' accesses to data. This limits parallelism because it forces threads to sequentially access shared resources. Additionally, systems use cache coherence to ensure that processors always operate on the most up-to-date version of a value even in the presence of private caches. Coherence protocol implementations cause processors to serialize their accesses to shared data, further limiting parallelism and performance.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {431–432},
numpages = {2},
keywords = {cache-coherence, commutativity, shared memory parallel programming}
}

@inproceedings{10.1145/3018743.3019030,
author = {Balaji, Vignesh and Tirumala, Dhruva and Lucia, Brandon},
title = {POSTER: An Architecture and Programming Model for Accelerating Parallel Commutative Computations via Privatization},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019030},
doi = {10.1145/3018743.3019030},
abstract = {Synchronization and data movement are the key impediments to an efficient parallel execution. To ensure that data shared by multiple threads remain consistent, the programmer must use synchronization (e.g., mutex locks) to serialize threads' accesses to data. This limits parallelism because it forces threads to sequentially access shared resources. Additionally, systems use cache coherence to ensure that processors always operate on the most up-to-date version of a value even in the presence of private caches. Coherence protocol implementations cause processors to serialize their accesses to shared data, further limiting parallelism and performance.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {431–432},
numpages = {2},
keywords = {shared memory parallel programming, commutativity, cache-coherence},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019027,
author = {Bhattacharyya, Arnamoy and Dai Wang, Mike and Burcea, Mihai and Ding, Yi and Deng, Allen and Varikooty, Sai and Hossain, Shafaaf and Amza, Cristiana},
title = {POSTER: HythTM: Extending the Applicability of Intel TSX Hardware Transactional Support},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019027},
doi = {10.1145/3155284.3019027},
abstract = {In this work, we introduce and experimentally evaluate a new hybrid software-hardware Transactional Memory prototype based on Intel's Haswell TSX architecture. Our prototype extends the applicability of the existing hardware support for TM by interposing a hybrid fall-back layer before the sequential, big-lock fall-back path, used by standard TSX-supported solutions in order to guarantee progress. In our experimental evaluation we use SynQuake, a realistic game benchmark modeled after Quake. Our results show that our hybrid transactional system,which we call HythTM, is able to reduce the number of transactions that go to the sequential software layer, hence avoiding hardware transaction aborts and loss of parallelism. HythTM optimizes application throughput and scalability up to 5.05x, when compared to the hardware TM with sequential fall-back path.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {433–434},
numpages = {2},
keywords = {shared memory parallel programming, commutativity, cache-coherence}
}

@inproceedings{10.1145/3018743.3019027,
author = {Bhattacharyya, Arnamoy and Dai Wang, Mike and Burcea, Mihai and Ding, Yi and Deng, Allen and Varikooty, Sai and Hossain, Shafaaf and Amza, Cristiana},
title = {POSTER: HythTM: Extending the Applicability of Intel TSX Hardware Transactional Support},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019027},
doi = {10.1145/3018743.3019027},
abstract = {In this work, we introduce and experimentally evaluate a new hybrid software-hardware Transactional Memory prototype based on Intel's Haswell TSX architecture. Our prototype extends the applicability of the existing hardware support for TM by interposing a hybrid fall-back layer before the sequential, big-lock fall-back path, used by standard TSX-supported solutions in order to guarantee progress. In our experimental evaluation we use SynQuake, a realistic game benchmark modeled after Quake. Our results show that our hybrid transactional system,which we call HythTM, is able to reduce the number of transactions that go to the sequential software layer, hence avoiding hardware transaction aborts and loss of parallelism. HythTM optimizes application throughput and scalability up to 5.05x, when compared to the hardware TM with sequential fall-back path.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {433–434},
numpages = {2},
keywords = {shared memory parallel programming, cache-coherence, commutativity},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019031,
author = {Chowdhury, Rezaul and Ganapathi, Pramod and Tang, Yuan and Tithi, Jesmin Jahan},
title = {POSTER: Provably Efficient Scheduling of Cache-Oblivious Wavefront Algorithms},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019031},
doi = {10.1145/3155284.3019031},
abstract = {Standard cache-oblivious recursive divide-and-conquer algorithms for evaluating dynamic programming recurrences have optimal serial cache complexity but often have lower parallelism compared with iterative wavefront algorithms due to artificial dependencies among subtasks. Very recently cache-oblivious recursive wavefront (COW) algorithms have been introduced which do not have any artificial dependencies. Though COW algorithms are based on fork-join primitives, they extensively use atomic operations, and as a result, performance guarantees provided by state-of-the-art schedulers for programs with fork-join primitives do not apply.In this work, we show how to systematically transform standard cache-oblivious recursive divide-and-conquer algorithms into recursive wavefront algorithms to achieve optimal parallel cache complexity and high parallelism under state-of-the-art schedulers for fork-join programs. Unlike COW algorithms these new algorithms do not use atomic operations. Instead, they use closed-form formulas to compute at what time each recursive function must be launched in order to achieve high parallelism without losing cache performance. The resulting implementations are arguably much simpler than implementations of known COW algorithms.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {435–436},
numpages = {2},
keywords = {dynamic programming, divide-and-conquer, recursive wavefront, cache-oblivious, parallel}
}

@inproceedings{10.1145/3018743.3019031,
author = {Chowdhury, Rezaul and Ganapathi, Pramod and Tang, Yuan and Tithi, Jesmin Jahan},
title = {POSTER: Provably Efficient Scheduling of Cache-Oblivious Wavefront Algorithms},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019031},
doi = {10.1145/3018743.3019031},
abstract = {Standard cache-oblivious recursive divide-and-conquer algorithms for evaluating dynamic programming recurrences have optimal serial cache complexity but often have lower parallelism compared with iterative wavefront algorithms due to artificial dependencies among subtasks. Very recently cache-oblivious recursive wavefront (COW) algorithms have been introduced which do not have any artificial dependencies. Though COW algorithms are based on fork-join primitives, they extensively use atomic operations, and as a result, performance guarantees provided by state-of-the-art schedulers for programs with fork-join primitives do not apply.In this work, we show how to systematically transform standard cache-oblivious recursive divide-and-conquer algorithms into recursive wavefront algorithms to achieve optimal parallel cache complexity and high parallelism under state-of-the-art schedulers for fork-join programs. Unlike COW algorithms these new algorithms do not use atomic operations. Instead, they use closed-form formulas to compute at what time each recursive function must be launched in order to achieve high parallelism without losing cache performance. The resulting implementations are arguably much simpler than implementations of known COW algorithms.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {435–436},
numpages = {2},
keywords = {cache-oblivious, divide-and-conquer, dynamic programming, parallel, recursive wavefront},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019026,
author = {Cohen, Nachshon and Herlihy, Maurice and Petrank, Erez and Wald, Elias},
title = {POSTER: State Teleportation via Hardware Transactional Memory},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019026},
doi = {10.1145/3155284.3019026},
abstract = {State teleportation is a new technique for exploiting hardware transactional memory (HTM) to improve existing synchronization and memory management schemes for highly-concurrent data structures. When applied to fine-grained locking, a thread holding the lock for a node launches a hardware transaction that traverses multiple successor nodes, acquires the lock for the last node reached, and releases the lock on the starting node, skipping lock acquisitions for intermediate nodes. When applied to lock-free data structures, a thread visiting a node protected by a hazard pointer launches a hardware transaction that traverses multiple successor nodes, and publishes the hazard pointer only for the last node reached, skipping the memory barriers needed to publish intermediate hazard pointers. Experimental results show that these applications of state teleportation can substantially increase the performance of both lock-based and lock-free data structures.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {437–438},
numpages = {2},
keywords = {hazard pointers, non-blocking, hand-over-hand, memory management, teleportation, concurrent data structures, lock-free}
}

@inproceedings{10.1145/3018743.3019026,
author = {Cohen, Nachshon and Herlihy, Maurice and Petrank, Erez and Wald, Elias},
title = {POSTER: State Teleportation via Hardware Transactional Memory},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019026},
doi = {10.1145/3018743.3019026},
abstract = {State teleportation is a new technique for exploiting hardware transactional memory (HTM) to improve existing synchronization and memory management schemes for highly-concurrent data structures. When applied to fine-grained locking, a thread holding the lock for a node launches a hardware transaction that traverses multiple successor nodes, acquires the lock for the last node reached, and releases the lock on the starting node, skipping lock acquisitions for intermediate nodes. When applied to lock-free data structures, a thread visiting a node protected by a hazard pointer launches a hardware transaction that traverses multiple successor nodes, and publishes the hazard pointer only for the last node reached, skipping the memory barriers needed to publish intermediate hazard pointers. Experimental results show that these applications of state teleportation can substantially increase the performance of both lock-based and lock-free data structures.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {437–438},
numpages = {2},
keywords = {hazard pointers, hand-over-hand, concurrent data structures, memory management, teleportation, lock-free, non-blocking},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019037,
author = {Dai, Dong and Zhang, Wei and Chen, Yong},
title = {POSTER: IOGP: An Incremental Online Graph Partitioning for Large-Scale Distributed Graph Databases},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019037},
doi = {10.1145/3155284.3019037},
abstract = {Large-scale graphs are becoming critical in various domains such as social network, scientific application, knowledge discovery, and even system software, etc. Many of those use cases require large-scale high-performance graph databases, which are designed for serving continuous updates from the clients, and at the same time, answering complex queries towards the current graph in an on-line manner. Those operations in graph databases, also referred as OLTP (online transaction processing) operations, need specific design and implementation in graph partitioning algorithms. In this study, we designed an incremental online graph partitioning (IOGP), optimized for OLTP workloads. It is designed to achieve better locality, generate balanced partitions, and increase the parallelism for accessing hotspots of the graph. Our evaluation results on both real world and synthetic graphs in both simulation and real system confirm a better performance on graph queries (as much as 2X) with small overheads during graph insertion (less than 10%).},
journal = {SIGPLAN Not.},
month = {jan},
pages = {439–440},
numpages = {2},
keywords = {graph partition, graph database, oltp}
}

@inproceedings{10.1145/3018743.3019037,
author = {Dai, Dong and Zhang, Wei and Chen, Yong},
title = {POSTER: IOGP: An Incremental Online Graph Partitioning for Large-Scale Distributed Graph Databases},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019037},
doi = {10.1145/3018743.3019037},
abstract = {Large-scale graphs are becoming critical in various domains such as social network, scientific application, knowledge discovery, and even system software, etc. Many of those use cases require large-scale high-performance graph databases, which are designed for serving continuous updates from the clients, and at the same time, answering complex queries towards the current graph in an on-line manner. Those operations in graph databases, also referred as OLTP (online transaction processing) operations, need specific design and implementation in graph partitioning algorithms. In this study, we designed an incremental online graph partitioning (IOGP), optimized for OLTP workloads. It is designed to achieve better locality, generate balanced partitions, and increase the parallelism for accessing hotspots of the graph. Our evaluation results on both real world and synthetic graphs in both simulation and real system confirm a better performance on graph queries (as much as 2X) with small overheads during graph insertion (less than 10%).},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {439–440},
numpages = {2},
keywords = {graph database, oltp, graph partition},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019036,
author = {Firoz, Jesun Shariar and Kanewala, Thejaka Amila and Zalewski, Marcin and Barnas, Martina and Lumsdaine, Andrew},
title = {POSTER: Distributed Control: The Benefits of Eliminating Global Synchronization via Effective Scheduling},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019036},
doi = {10.1145/3155284.3019036},
abstract = {In distributed computing, parallel overheads such as emph{synchronization overhead} may hinder performance. We introduce the idea of emph{Distributed Control} (DC) where global synchronization is reduced to emph{termination detection} and each worker proceeds ahead optimistically, based on the local knowledge of the global computation. To avoid "wasted'' work, DC relies on local work prioritization. However, the work order obtained by local prioritization is susceptible to interference from the runtime. We show that employing effective scheduling policies and optimizations in the runtime, in conjunction with eliminating global barriers, improves performance in two graph applications: single-source shortest paths and connected components.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {441–442},
numpages = {2},
keywords = {connected components, graph processing, single- source shortest paths, distributed runtimes}
}

@inproceedings{10.1145/3018743.3019036,
author = {Firoz, Jesun Shariar and Kanewala, Thejaka Amila and Zalewski, Marcin and Barnas, Martina and Lumsdaine, Andrew},
title = {POSTER: Distributed Control: The Benefits of Eliminating Global Synchronization via Effective Scheduling},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019036},
doi = {10.1145/3018743.3019036},
abstract = {In distributed computing, parallel overheads such as emph{synchronization overhead} may hinder performance. We introduce the idea of emph{Distributed Control} (DC) where global synchronization is reduced to emph{termination detection} and each worker proceeds ahead optimistically, based on the local knowledge of the global computation. To avoid "wasted'' work, DC relies on local work prioritization. However, the work order obtained by local prioritization is susceptible to interference from the runtime. We show that employing effective scheduling policies and optimizations in the runtime, in conjunction with eliminating global barriers, improves performance in two graph applications: single-source shortest paths and connected components.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {441–442},
numpages = {2},
keywords = {distributed runtimes, connected components, single- source shortest paths, graph processing},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019034,
author = {Jo, Gangwon and Jung, Jaehoon and Park, Jiyoung and Lee, Jaejin},
title = {POSTER: MAPA: An Automatic Memory Access Pattern Analyzer for GPU Applications},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019034},
doi = {10.1145/3155284.3019034},
abstract = {Various existing optimization and memory consistency management techniques for GPU applications rely on memory access patterns of kernels. However, they suffer from poor practicality because they require explicit user interventions to extract kernel memory access patterns. This paper proposes an automatic memory-access-pattern analysis framework called MAPA. MAPA is based on a source-level analysis technique derived from traditional symbolic analyses and a run-time pattern selection technique. The experimental results show that MAPA properly analyzes 116 real-world OpenCL kernels from Rodinia and Parboil.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {443–444},
numpages = {2},
keywords = {optimization, opencl, compiler, auto-tuning, memory access pattern, gpgpu, symbolic analysis}
}

@inproceedings{10.1145/3018743.3019034,
author = {Jo, Gangwon and Jung, Jaehoon and Park, Jiyoung and Lee, Jaejin},
title = {POSTER: MAPA: An Automatic Memory Access Pattern Analyzer for GPU Applications},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019034},
doi = {10.1145/3018743.3019034},
abstract = {Various existing optimization and memory consistency management techniques for GPU applications rely on memory access patterns of kernels. However, they suffer from poor practicality because they require explicit user interventions to extract kernel memory access patterns. This paper proposes an automatic memory-access-pattern analysis framework called MAPA. MAPA is based on a source-level analysis technique derived from traditional symbolic analyses and a run-time pattern selection technique. The experimental results show that MAPA properly analyzes 116 real-world OpenCL kernels from Rodinia and Parboil.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {443–444},
numpages = {2},
keywords = {opencl, gpgpu, auto-tuning, optimization, memory access pattern, symbolic analysis, compiler},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019025,
author = {Li, Shigang and Zhang, Yunquan and Hoefler, Torsten},
title = {POSTER: Cache-Oblivious MPI All-to-All Communications on Many-Core Architectures},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019025},
doi = {10.1145/3155284.3019025},
abstract = {In the many-core era, the performance of MPI collectives is more dependent on the intra-node communication component. However, the communication algorithms generally inherit from the inter-node version and ignore the cache complexity. We propose cache-oblivious algorithms for MPI all-to-all operations, in which data blocks are copied into the receive buffers in Morton order to exploit data locality. Experimental results on different many-core architectures show that our cache-oblivious implementations significantly outperform the naive implementations based on shared heap and the highly optimized MPI libraries.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {445–446},
numpages = {2},
keywords = {mpi_alltoall, many-core, cache-oblivious algorithms}
}

@inproceedings{10.1145/3018743.3019025,
author = {Li, Shigang and Zhang, Yunquan and Hoefler, Torsten},
title = {POSTER: Cache-Oblivious MPI All-to-All Communications on Many-Core Architectures},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019025},
doi = {10.1145/3018743.3019025},
abstract = {In the many-core era, the performance of MPI collectives is more dependent on the intra-node communication component. However, the communication algorithms generally inherit from the inter-node version and ignore the cache complexity. We propose cache-oblivious algorithms for MPI all-to-all operations, in which data blocks are copied into the receive buffers in Morton order to exploit data locality. Experimental results on different many-core architectures show that our cache-oblivious implementations significantly outperform the naive implementations based on shared heap and the highly optimized MPI libraries.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {445–446},
numpages = {2},
keywords = {mpi_alltoall, many-core, cache-oblivious algorithms},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019033,
author = {Menon, Harshitha and Chandrasekar, Kavitha and Kale, Laxmikant V.},
title = {POSTER: Automated Load Balancer Selection Based on Application Characteristics},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019033},
doi = {10.1145/3155284.3019033},
abstract = {Many HPC applications require dynamic load balancing to achieve high performance and system utilization. Different applications have different characteristics and hence require different load balancing strategies. Invocation of a suboptimal load balancing strategy can lead to inefficient execution. We propose Meta-Balancer, a framework to automatically decide the best load balancing strategy. It employs randomized decision forests, a machine learning method, to learn a model for choosing the best load balancing strategy for an application represented by a set of features that capture the application characteristics.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {447–448},
numpages = {2},
keywords = {runtime system, load balancing, machine learning, hpc}
}

@inproceedings{10.1145/3018743.3019033,
author = {Menon, Harshitha and Chandrasekar, Kavitha and Kale, Laxmikant V.},
title = {POSTER: Automated Load Balancer Selection Based on Application Characteristics},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019033},
doi = {10.1145/3018743.3019033},
abstract = {Many HPC applications require dynamic load balancing to achieve high performance and system utilization. Different applications have different characteristics and hence require different load balancing strategies. Invocation of a suboptimal load balancing strategy can lead to inefficient execution. We propose Meta-Balancer, a framework to automatically decide the best load balancing strategy. It employs randomized decision forests, a machine learning method, to learn a model for choosing the best load balancing strategy for an application represented by a set of features that capture the application characteristics.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {447–448},
numpages = {2},
keywords = {machine learning, load balancing, hpc, runtime system},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019032,
author = {Moscovici, Nurit and Cohen, Nachshon and Petrank, Erez},
title = {POSTER: A GPU-Friendly Skiplist Algorithm},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019032},
doi = {10.1145/3155284.3019032},
abstract = {We propose a design for a fine-grained lock-based skiplist optimized for Graphics Processing Units (GPUs). While GPUs are often used to accelerate streaming parallel computations, it remains a significant challenge to efficiently offload concurrent computations with more complicated data-irregular access and fine-grained synchronization. Natural building blocks for such computations would be concurrent data structures, such as skiplists, which are widely used in general purpose computations. Our design utilizes array-based nodes which are accessed and updated by warp-cooperative functions, thus taking advantage of the fact that GPUs are most efficient when memory accesses are coalesced and execution divergence is minimized. The proposed design has been implemented, and measurements demonstrate improved performance of up to 2.6x over skiplist designs for the GPU existing today.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {449–450},
numpages = {2},
keywords = {data structures, gpu, simd, skip list}
}

@inproceedings{10.1145/3018743.3019032,
author = {Moscovici, Nurit and Cohen, Nachshon and Petrank, Erez},
title = {POSTER: A GPU-Friendly Skiplist Algorithm},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019032},
doi = {10.1145/3018743.3019032},
abstract = {We propose a design for a fine-grained lock-based skiplist optimized for Graphics Processing Units (GPUs). While GPUs are often used to accelerate streaming parallel computations, it remains a significant challenge to efficiently offload concurrent computations with more complicated data-irregular access and fine-grained synchronization. Natural building blocks for such computations would be concurrent data structures, such as skiplists, which are widely used in general purpose computations. Our design utilizes array-based nodes which are accessed and updated by warp-cooperative functions, thus taking advantage of the fact that GPUs are most efficient when memory accesses are coalesced and execution divergence is minimized. The proposed design has been implemented, and measurements demonstrate improved performance of up to 2.6x over skiplist designs for the GPU existing today.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {449–450},
numpages = {2},
keywords = {gpu, simd, skip list, data structures},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019021,
author = {Ramalhete, Pedro and Correia, Andreia},
title = {POSTER: Poor Man's URCU},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019021},
doi = {10.1145/3155284.3019021},
abstract = {RCU is, among other things, a well known mechanism for memory reclamation that is meant to be used in languages without an automatic Garbage Collector, unfortunately, it requires operating system support, which is currently provided only in Linux. An alternative is to use Userspace RCU (URCU) which has two variants that can be deployed on other operating systems, named emph{Memory Barrier} and emph{Bullet Proof}.We present a novel algorithm that implements the three core APIs of RCU: texttt{rcu_read_lock()}, texttt{rcu_read_unlock()}, and texttt{synchronize_rcu()}. Our algorithm uses one mutual exclusion lock and two reader-writer locks with texttt{trylock()} capabilities, which means it does not need a language with a memory model or atomics API, and as such, it can be easily implemented in almost any language, regardless of the underlying CPU architecture, or operating system.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {451–452},
numpages = {2},
keywords = {locks, rcu}
}

@inproceedings{10.1145/3018743.3019021,
author = {Ramalhete, Pedro and Correia, Andreia},
title = {POSTER: Poor Man's URCU},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019021},
doi = {10.1145/3018743.3019021},
abstract = {RCU is, among other things, a well known mechanism for memory reclamation that is meant to be used in languages without an automatic Garbage Collector, unfortunately, it requires operating system support, which is currently provided only in Linux. An alternative is to use Userspace RCU (URCU) which has two variants that can be deployed on other operating systems, named emph{Memory Barrier} and emph{Bullet Proof}.We present a novel algorithm that implements the three core APIs of RCU: texttt{rcu_read_lock()}, texttt{rcu_read_unlock()}, and texttt{synchronize_rcu()}. Our algorithm uses one mutual exclusion lock and two reader-writer locks with texttt{trylock()} capabilities, which means it does not need a language with a memory model or atomics API, and as such, it can be easily implemented in almost any language, regardless of the underlying CPU architecture, or operating system.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {451–452},
numpages = {2},
keywords = {locks, rcu},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019022,
author = {Ramalhete, Pedro and Correia, Andreia},
title = {POSTER: A Wait-Free Queue with Wait-Free Memory Reclamation},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019022},
doi = {10.1145/3155284.3019022},
abstract = {Queues are a widely deployed data structure. They are used extensively in many multi threaded applications, or as a communication mechanism between threads or processes. We propose a new linearizable multi-producer-multi-consumer queue we named Turn queue, with wait-free progress bounded by the number of threads, and with wait-free bounded memory reclamation. Its main characteristics are: a simple algorithm that does no memory allocation apart from creating the node that is placed in the queue, a new wait-free consensus algorithm using only the atomic instruction compare-and-swap (CAS), and is easy to plugin with other algorithms for either enqueue or dequeue methods.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {453–454},
numpages = {2},
keywords = {low latency, wait-free, non-blocking queue}
}

@inproceedings{10.1145/3018743.3019022,
author = {Ramalhete, Pedro and Correia, Andreia},
title = {POSTER: A Wait-Free Queue with Wait-Free Memory Reclamation},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019022},
doi = {10.1145/3018743.3019022},
abstract = {Queues are a widely deployed data structure. They are used extensively in many multi threaded applications, or as a communication mechanism between threads or processes. We propose a new linearizable multi-producer-multi-consumer queue we named Turn queue, with wait-free progress bounded by the number of threads, and with wait-free bounded memory reclamation. Its main characteristics are: a simple algorithm that does no memory allocation apart from creating the node that is placed in the queue, a new wait-free consensus algorithm using only the atomic instruction compare-and-swap (CAS), and is easy to plugin with other algorithms for either enqueue or dequeue methods.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {453–454},
numpages = {2},
keywords = {non-blocking queue, wait-free, low latency},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019029,
author = {Tang, Yuan and You, Ronghui},
title = {POSTER: STAR (Space-Time Adaptive and Reductive) Algorithms for Real-World Space-Time Optimality},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019029},
doi = {10.1145/3155284.3019029},
abstract = {It's important to hit a space-time balance for a real-world algorithm to achieve high performance on modern shared-memory multi-core or many-core systems. However, a large class of dynamic programs with more than $O(1)$ dependency achieve optimality either in space or time, but not both. In the literature, the problem is known as the fundamental space-time tradeoff. By exploiting properly on the runtime system, we show that our STAR (Space-Time Adaptive and Reductive) technique can help these dynamic programs to achieve sublinear parallel time bounds while still maintaining work-, space-, and cache-optimality in a processor- and cache-oblivious fashion.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {455–456},
numpages = {2},
keywords = {shared-memory multicore system, dynamic program, cache-oblivious algorithm, space-time balance}
}

@inproceedings{10.1145/3018743.3019029,
author = {Tang, Yuan and You, Ronghui},
title = {POSTER: STAR (Space-Time Adaptive and Reductive) Algorithms for Real-World Space-Time Optimality},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019029},
doi = {10.1145/3018743.3019029},
abstract = {It's important to hit a space-time balance for a real-world algorithm to achieve high performance on modern shared-memory multi-core or many-core systems. However, a large class of dynamic programs with more than $O(1)$ dependency achieve optimality either in space or time, but not both. In the literature, the problem is known as the fundamental space-time tradeoff. By exploiting properly on the runtime system, we show that our STAR (Space-Time Adaptive and Reductive) technique can help these dynamic programs to achieve sublinear parallel time bounds while still maintaining work-, space-, and cache-optimality in a processor- and cache-oblivious fashion.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {455–456},
numpages = {2},
keywords = {shared-memory multicore system, cache-oblivious algorithm, dynamic program, space-time balance},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019039,
author = {Wu, Mingyu and Guan, Haibing and Zang, Binyu and Chen, Haibo},
title = {POSTER: Recovering Performance for Vector-Based Machine Learning on Managed Runtime},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019039},
doi = {10.1145/3155284.3019039},
journal = {SIGPLAN Not.},
month = {jan},
pages = {457–458},
numpages = {2},
keywords = {machine learning, managed runtime, vector}
}

@inproceedings{10.1145/3018743.3019039,
author = {Wu, Mingyu and Guan, Haibing and Zang, Binyu and Chen, Haibo},
title = {POSTER: Recovering Performance for Vector-Based Machine Learning on Managed Runtime},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019039},
doi = {10.1145/3018743.3019039},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {457–458},
numpages = {2},
keywords = {managed runtime, machine learning, vector},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019024,
author = {Zhang, Minjia and Biswas, Swarnendu and Bond, Michael D.},
title = {POSTER: On the Problem of Consistency Exceptions in the Context of Strong Memory Models},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019024},
doi = {10.1145/3155284.3019024},
abstract = {This work considers the problem of availability for memory models that throw consistency exceptions. We define a new memory model called RIx based on isolation of synchronization-free regions and a new approach called Avalon that provides RIx. Our evaluation shows that Avalon and RIx substantially reduce consistency exceptions, by 1-3 orders of magnitude and sometimes eliminate them completely. Furthermore, our exploration provides new, compelling points in the performance-availability tradeoff space.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {459–460},
numpages = {2},
keywords = {memory model, region isolation, availability, runtime system, concurrency}
}

@inproceedings{10.1145/3018743.3019024,
author = {Zhang, Minjia and Biswas, Swarnendu and Bond, Michael D.},
title = {POSTER: On the Problem of Consistency Exceptions in the Context of Strong Memory Models},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019024},
doi = {10.1145/3018743.3019024},
abstract = {This work considers the problem of availability for memory models that throw consistency exceptions. We define a new memory model called RIx based on isolation of synchronization-free regions and a new approach called Avalon that provides RIx. Our evaluation shows that Avalon and RIx substantially reduce consistency exceptions, by 1-3 orders of magnitude and sometimes eliminate them completely. Furthermore, our exploration provides new, compelling points in the performance-availability tradeoff space.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {459–460},
numpages = {2},
keywords = {region isolation, concurrency, memory model, availability, runtime system},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3019023,
author = {Zhao, Yue and Liao, Chunhua and Shen, Xipeng},
title = {POSTER: An Infrastructure for HPC Knowledge Sharing and Reuse},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3019023},
doi = {10.1145/3155284.3019023},
abstract = {This paper presents a prototype infrastructure for addressing the barriers for effective accumulation, sharing, and reuse of the various types of knowledge for high performance parallel computing.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {461–462},
numpages = {2},
keywords = {knowledge graph, ontology, hpc}
}

@inproceedings{10.1145/3018743.3019023,
author = {Zhao, Yue and Liao, Chunhua and Shen, Xipeng},
title = {POSTER: An Infrastructure for HPC Knowledge Sharing and Reuse},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3019023},
doi = {10.1145/3018743.3019023},
abstract = {This paper presents a prototype infrastructure for addressing the barriers for effective accumulation, sharing, and reuse of the various types of knowledge for high performance parallel computing.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {461–462},
numpages = {2},
keywords = {ontology, knowledge graph, hpc},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

