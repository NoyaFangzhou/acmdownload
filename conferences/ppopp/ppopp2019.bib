@inproceedings{10.1145/3293883.3295710,
author = {Hestness, Joel and Ardalani, Newsha and Diamos, Gregory},
title = {Beyond Human-Level Accuracy: Computational Challenges in Deep Learning},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295710},
doi = {10.1145/3293883.3295710},
abstract = {Deep learning (DL) research yields accuracy and product improvements from both model architecture changes and scale: larger data sets and models, and more computation. For hardware design, it is difficult to predict DL model changes. However, recent prior work shows that as dataset sizes grow, DL model accuracy and model size grow predictably. This paper leverages the prior work to project the dataset and model size growth required to advance DL accuracy beyond human-level, to frontier targets defined by machine learning experts. Datasets will need to grow 33--971\texttimes{}, while models will need to grow 6.6--456\texttimes{} to achieve target accuracies.We further characterize and project the computational requirements to train these applications at scale. Our characterization reveals an important segmentation of DL training challenges for recurrent neural networks (RNNs) that contrasts with prior studies of deep convolutional networks. RNNs will have comparatively moderate operational intensities and very large memory footprint requirements. In contrast to emerging accelerator designs, large-scale RNN training characteristics suggest designs with significantly larger memory capacity and on-chip caches.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {1–14},
numpages = {14},
keywords = {deep learning, data parallelism, neural networks, compute requirements, model parallelism, compute graph},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295722,
author = {Xiao, Junmin and Wang, Shijie and Wan, Weiqiang and Hong, Xuehai and Tan, Guangming},
title = {S-EnKF: Co-Designing for Scalable Ensemble Kalman Filter},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295722},
doi = {10.1145/3293883.3295722},
abstract = {Ensemble Kalman filter (EnKF) is one of the most important methods for data assimilation, which is widely applied to the reconstruction of observed historical data for providing initial conditions of numerical atmospheric and oceanic models. With the improvement of data resolution and the increase in the amount of model data, the scalability of recent parallel implementations suffers from high overhead on data transfer. In this paper, we propose, S-EnKF: a scalable and distributed EnKF adaptation for modern clusters. With an in-depth analysis of new requirements brought forward by recent frameworks and limitations of current designs, we present a co-design of S-EnKF. For fully exploiting the resources available in modern parallel file systems, we design a concurrent access approach to accelerate the process of reading large amounts of background data. Through a deeper investigation of the data dependence relations, we modify EnKF's workflow to maximize the overlap of file reading and local analysis with a new multi-stage computation approach. Furthermore, we push the envelope of performance further with aggressive co-design of auto-tuning through tradeoff between the benefit on runtime and the cost on processors based on classic cost models. The experimental evaluation of S-EnKF demonstrates nearly ideal strong scalability on up to 12,000 processors. The largest run sustains a performance of 3x-speedup compared with P-EnKF, which represents the state-of-art parallel implementation of EnKF.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {15–26},
numpages = {12},
keywords = {data assimilation, domain localization, scalability, ensemble Kalman filter, parallel implementation},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295727,
author = {Gelado, Isaac and Garland, Michael},
title = {Throughput-Oriented GPU Memory Allocation},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295727},
doi = {10.1145/3293883.3295727},
abstract = {Throughput-oriented architectures, such as GPUs, can sustain three orders of magnitude more concurrent threads than multicore architectures. This level of concurrency pushes typical synchronization primitives (e.g., mutexes) over their scalability limits, creating significant performance bottlenecks in modules, such as memory allocators, that use them. In this paper, we develop concurrent programming techniques and synchronization primitives, in support of a dynamic memory allocator, that are efficient for use with very high levels of concurrency.We formulate resource allocation as a two-stage process, that decouples accounting for the number of available resources from the tracking of the available resources themselves. To facilitate the accounting stage, we introduce a novel bulk semaphore abstraction that extends traditional semaphore semantics by optimizing for the case where threads operate on the semaphore simultaneously. We also similarly design new collective synchronization primitives that enable groups of cooperating threads to enter critical sections together. Finally, we show that delegation of deferred reclamation to threads already blocked greatly improves efficiency.Using all these techniques, our throughput-oriented memory allocator delivers both high allocation rates and low memory fragmentation on modern GPUs. Our experiments demonstrate that it achieves allocation rates that are on average 16.56 times higher than the counterpart implementation in the CUDA 9 toolkit.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {27–37},
numpages = {11},
keywords = {GPU programming, memory allocation, concurrency},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295733,
author = {Wang, Hao and Geng, Liang and Lee, Rubao and Hou, Kaixi and Zhang, Yanfeng and Zhang, Xiaodong},
title = {SEP-Graph: Finding Shortest Execution Paths for Graph Processing under a Hybrid Framework on GPU},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295733},
doi = {10.1145/3293883.3295733},
abstract = {In general, the performance of parallel graph processing is determined by three pairs of critical parameters, namely synchronous or asynchronous execution mode (Sync or Async), Push or Pull communication mechanism (Push or Pull), and Data-driven or Topology-driven traversing scheme (DD or TD), which increases the complexity and sophistication of programming and system implementation of GPU. Existing graph-processing frameworks mainly use a single combination in the entire execution for a given application, but we have observed their variable and suboptimal performance. In this paper, we present SEP-Graph, a highly efficient software framework for graph-processing on GPU. The hybrid execution mode is automatically switched among three pairs of parameters, with an objective to achieve the shortest execution time in each iteration. We also apply a set of optimizations to SEP-Graph, considering the characteristics of graph algorithms and underlying GPU architectures. We show the effectiveness of SEP-Graph based on our intensive and comparative performance evaluation on NVIDIA 1080, P100, and V100 GPUs. Compared with existing and representative GPU graph-processing framework Groute and Gunrock, SEP-Graph can reduce execution time up to 45.8 times and 39.4 times.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {38–52},
numpages = {15},
keywords = {hybrid, graph algorithms, GPU},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295707,
author = {Henriksen, Troels and Thor\o{}e, Frederik and Elsman, Martin and Oancea, Cosmin},
title = {Incremental Flattening for Nested Data Parallelism},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295707},
doi = {10.1145/3293883.3295707},
abstract = {Compilation techniques for nested-parallel applications that can adapt to hardware and dataset characteristics are vital for unlocking the power of modern hardware. This paper proposes such a technique, which builds on flattening and is applied in the context of a functional data-parallel language. Our solution uses the degree of utilized parallelism as the driver for generating a multitude of code versions, which together cover all possible mappings of the application's regular nested parallelism to the levels of parallelism supported by the hardware. These code versions are then combined into one program by guarding them with predicates, whose threshold values are automatically tuned to hardware and dataset characteristics. Our unsupervised method---of statically clustering datasets to code versions---is different from autotuning work that typically searches for the combination of code transformations producing a single version, best suited for a specific dataset or on average for all datasets.We demonstrate---by fully integrating our technique in the repertoire of a compiler for the Futhark programming language---significant performance gains on two GPUs for three real-world applications, from the financial domain, and for six Rodinia benchmarks.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {53–67},
numpages = {15},
keywords = {GPGPU, parallel, compilers, functional language},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295701,
author = {Winter, Martin and Mlakar, Daniel and Zayer, Rhaleb and Seidel, Hans-Peter and Steinberger, Markus},
title = {Adaptive Sparse Matrix-Matrix Multiplication on the GPU},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295701},
doi = {10.1145/3293883.3295701},
abstract = {In the ongoing efforts targeting the vectorization of linear algebra primitives, sparse matrix-matrix multiplication (SpGEMM) has received considerably less attention than sparse Matrix-Vector multiplication (SpMV). While both are equally important, this disparity can be attributed mainly to the additional formidable challenges raised by SpGEMM.In this paper, we present a dynamic approach for addressing SpGEMM on the GPU. Our approach works directly on the standard compressed sparse rows (CSR) data format. In comparison to previous SpGEMM implementations, our approach guarantees a homogeneous, load-balanced access pattern to the first input matrix and improves memory access to the second input matrix. It adaptively re-purposes GPU threads during execution and maximizes the time efficient on-chip scratchpad memory can be used. Adhering to a completely deterministic scheduling pattern guarantees bit-stable results during repetitive execution, a property missing from other approaches. Evaluation on an extensive sparse matrix benchmark suggests our approach being the fastest SpGEMM implementation for highly sparse matrices (80% of the set). When bit-stable results are sought, our approach is the fastest across the entire test set.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {68–81},
numpages = {14},
keywords = {GPU, bit-stable, SpGEMM, sparse matrix, adaptive, ESC},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295708,
author = {Dongol, Brijesh and Jagadeesan, Radha and Riely, James},
title = {Modular Transactions: Bounding Mixed Races in Space and Time},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295708},
doi = {10.1145/3293883.3295708},
abstract = {We define local transactional race freedom (LTRF), which provides a programmer model for software transactional memory. LTRF programs satisfy the SC-LTRF property, thus allowing the programmer to focus on sequential executions in which transactions execute atomically. Unlike previous results, SC-LTRF does not require global race freedom. We also provide a lower-level implementation model to reason about quiescence fences and validate numerous compiler optimizations.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {82–93},
numpages = {12},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295711,
author = {Yates, Ryan and Scott, Michael L.},
title = {Leveraging Hardware TM in Haskell},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295711},
doi = {10.1145/3293883.3295711},
abstract = {Transactional memory (TM) is heavily used for synchronization in the Haskell programming language, but its performance has historically been poor. We set out to improve this performance using hardware TM (HTM) on Intel processors. This task is complicated by Haskell's retry mechanism, which requires information to escape aborted transactions, and by the heavy use of indirection in the Haskell runtime, which means that even small transactions are likely to over-flow hardware buffers. It is eased by functional semantics, which preclude irreversible operations; by the static separation of transactional state, which precludes privatization; and by the error containment of strong typing, which enables so-called lazy subscription to the lock that protects the "fallback" code path.We describe a three-level hybrid TM system for the Glasgow Haskell Compiler (GHC). Our system first attempts to perform an entire transaction in hardware. Failing that, it falls back to software tracking of read and write sets combined with a commit-time hardware transaction. If necessary, it employs a global lock to serialize commits (but still not the bodies of transactions). To get good performance from hardware TM while preserving Haskell semantics, we use Bloom filters for read and write set tracking. We also implemented and extended the newly proposed mutable constructor fields language feature to significantly reduce indirection. Experimental results with complex data structures show significant improvements in throughput and scalability.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {94–106},
numpages = {13},
keywords = {transactional memory, Haskell, synchronization, scalability},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295714,
author = {Filipe, Ricardo and Issa, Shady and Romano, Paolo and Barreto, Jo\~{a}o},
title = {Stretching the Capacity of Hardware Transactional Memory in IBM POWER Architectures},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295714},
doi = {10.1145/3293883.3295714},
abstract = {The hardware transactional memory (HTM) implementations in commercially available processors are significantly hindered by their tight capacity constraints. In practice, this renders current HTMs unsuitable to many real-world workloads of in-memory databases.This paper proposes SI-HTM, which stretches the capacity bounds of the underlying HTM, thus opening HTM to a much broader class of applications. SI-HTM leverages the HTM implementation of the IBM POWER architecture with a software layer to offer a single-version implementation of Snapshot Isolation. When compared to HTM- and software-based concurrency control alternatives, SI-HTM exhibits improved scalability, achieving speedups of up to 300% relatively to HTM on in-memory database benchmarks.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {107–119},
numpages = {13},
keywords = {IMDBS, IBM POWER, snapshot isolation, transactional memory, HTM},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295730,
author = {Saad, Mohamed M. and Kishi, Masoomeh Javidi and Jing, Shihao and Hans, Sandeep and Palmieri, Roberto},
title = {Processing Transactions in a Predefined Order},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295730},
doi = {10.1145/3293883.3295730},
abstract = {In this paper we provide a high performance solution to the problem of committing transactions while enforcing a pre-defined order. We provide the design and implementation of three algorithms, which deploy a specialized cooperative transaction execution model. This model permits the propagation of written values along the chain of ordered transactions. We show that, even in the presence of data conflicts, the proposed algorithms outperform single threaded execution, and other baseline and specialized state-of-the-art competitors (e.g., STMLite). The maximum speedup achieved in micro benchmarks, STAMP, PARSEC and SPEC200 applications is in the range of 4.3x -- 16.5x.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {120–132},
numpages = {13},
keywords = {transactions, ordering, parallelization},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295704,
author = {Yan, Zhaofeng and Lin, Yuzhe and Peng, Lu and Zhang, Weihua},
title = {Harmonia: A High Throughput B+tree for GPUs},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295704},
doi = {10.1145/3293883.3295704},
abstract = {B+tree is one of the most important data structures and has been widely used in different fields. With the increase of concurrent queries and data-scale in storage, designing an efficient B+tree structure has become critical. Due to abundant computation resources, GPUs provide potential opportunities to achieve high query throughput for B+tree. However, prior methods cannot achieve satisfactory performance results due to low resource utilization and poor memory performance.In this paper, we first identify the gaps between B+tree and GPUs. Concurrent B+tree queries involve many global memory accesses and different divergences, which mismatch with GPU features. Based on this observation, we propose Harmonia, a novel B+tree structure to bridge the gap. In Harmonia, a B+tree structure is divided into a key region and a child region. The key region stores the nodes with its keys in a breadth-first order. The child region is organized as a prefix-sum array, which only stores each node's first child index in the key region. Since the prefix-sum child region is small and the children's index can be retrieved through index computations, most of it can be stored in on-chip caches, which can achieve good cache locality. To make it more efficient, Harmonia also includes two optimizations: partially-sorted aggregation and narrowed thread-group traversal, which can mitigate memory and warp divergence and improve resource utilization. Evaluations on a TITAN V GPU show that Harmonia can achieve up to 3.6 billion queries per second, which is about 3.4X faster than that of HB+Tree [39], a recent state-of-the-art GPU solution.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {133–144},
numpages = {12},
keywords = {high-throughput, GPU, B+tree},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295706,
author = {Awad, Muhammad A. and Ashkiani, Saman and Johnson, Rob and Farach-Colton, Mart\'{\i}n and Owens, John D.},
title = {Engineering a High-Performance GPU B-Tree},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295706},
doi = {10.1145/3293883.3295706},
abstract = {We engineer a GPU implementation of a B-Tree that supports concurrent queries (point, range, and successor) and updates (insertions and deletions). Our B-tree outperforms the state of the art, a GPU log-structured merge tree (LSM) and a GPU sorted array. In particular, point and range queries are significantly faster than in a GPU LSM (the GPU LSM does not implement successor queries). Furthermore, B-Tree insertions are also faster than LSM and sorted array insertions unless insertions come in batches of more than roughly 100k. Because we cache the upper levels of the tree, we achieve lookup throughput that exceeds the DRAM bandwidth of the GPU. We demonstrate that the key limiter of performance on a GPU is contention and describe the design choices that allow us to achieve this high performance.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {145–157},
numpages = {13},
keywords = {B-tree, data structures, GPU, mutable, dynamic},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295705,
author = {Hu, Xiaokang and Wei, Changzheng and Li, Jian and Will, Brian and Yu, Ping and Gong, Lu and Guan, Haibing},
title = {QTLS: High-Performance TLS Asynchronous Offload Framework with Intel® QuickAssist Technology},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295705},
doi = {10.1145/3293883.3295705},
abstract = {Hardware accelerators are a promising solution to optimize the Total Cost of Ownership (TCO) of cloud datacenters. This paper targets the costly Transport Layer Security (TLS) and investigates the TLS acceleration for the widely-deployed event-driven TLS servers or terminators. Our study reveals an important fact: the straight offloading of TLS-involved crypto operations suffers from the frequent long-lasting blockings in the offload I/O, leading to the underutilization of both CPU and accelerator resources.To achieve efficient TLS acceleration for the event-driven web architecture, we propose QTLS, a high-performance TLS asynchronous offload framework based on Intel® QuickAssist Technology (QAT). QTLS re-engineers the TLS software stack and divides the TLS offloading into four phases to eliminate blockings. Then, multiple crypto operations from different TLS connections can be offloaded concurrently in one process/thread, bringing a performance boost. Moreover, QTLS is built with a heuristic polling scheme to retrieve accelerator responses efficiently and timely, and a kernel-bypass notification scheme to avoid expensive switches between user mode and kernel mode while delivering async events. The comprehensive evaluation shows that QTLS can provide up to 9x connections per second (CPS) with TLS-RSA (2048bit), 2x secure data transfer throughput and 85% reduction of average response time compared to the software baseline.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {158–172},
numpages = {15},
keywords = {asynchronous offload, crypto operations, crypto accelerator, event-driven web architecture, SSL/TLS},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295737,
author = {Gruber, Fabian and Selva, Manuel and Sampaio, Diogo and Guillon, Christophe and Moynault, Antoine and Pouchet, Louis-No\"{e}l and Rastello, Fabrice},
title = {Data-Flow/Dependence Profiling for Structured Transformations},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295737},
doi = {10.1145/3293883.3295737},
abstract = {Profiling feedback is an important technique used by developers for performance debugging, where it is usually used to pinpoint performance bottlenecks and also to find optimization opportunities. Assessing the validity and potential benefit of a program transformation requires accurate knowledge of the data flow and dependencies, which can be uncovered by profiling a particular execution of the program.In this work we develop poly-prof, an end-to-end infrastructure for dynamic binary analysis, which produces feedback about the potential to apply complex program rescheduling. Our tool can handle both inter- and intraprocedural aspects of the program in a unified way, thus providing interprocedural transformation feedback.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {173–185},
numpages = {13},
keywords = {compiler optimization, polyhedral model, performance feedback, binary, dynamic dependence graph, loop transformations, instrumentation},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295728,
author = {Wang, Qingsen and Su, Pengfei and Chabbi, Milind and Liu, Xu},
title = {Lightweight Hardware Transactional Memory Profiling},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295728},
doi = {10.1145/3293883.3295728},
abstract = {Programs that use hardware transactional memory (HTM) demand sophisticated performance analysis tools when they suffer from performance losses. We have developed TxSampler---a lightweight profiler for programs that use HTM. TxSampler measures performance via sampling and provides a structured performance analysis to guide intuitive optimization with a novel decision-tree model. TxSampler computes metrics that drive the investigation process in a systematic way. It not only pinpoints hot transactions with time quantification of transactional and fallback paths, but also identifies causes of transaction aborts such as data contention, capacity overflow, false sharing, and problematic instructions. TxSampler associates metrics with full call paths that are even deeply embedded inside transactions and maps them to the program's source code. Our evaluation of more than 30 HTM benchmarks and applications shows that TxSampler incurs ~4% runtime overhead and negligible memory overhead for its insightful analyses. Guided by TxSampler, we are able to optimize several HTM programs and obtain nontrivial speedups.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {186–200},
numpages = {15},
keywords = {HTM benchmark suite, hardware transactional memory, optimization, profiling},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295716,
author = {Meng, Ke and Li, Jiajia and Tan, Guangming and Sun, Ninghui},
title = {A Pattern Based Algorithmic Autotuner for Graph Processing on GPUs},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295716},
doi = {10.1145/3293883.3295716},
abstract = {This paper proposes Gswitch, a pattern-based algorithmic auto-tuning system that dynamically switches between optimization variants with negligible overhead. Its novelty lies in a small set of algorithmic patterns that allow for the configurable assembly of variants of the algorithm. The fast transition of Gswitch is based on a machine learning model trained using 644 real graphs. Moreover, Gswitch provides a simple programming interface that conceals low-level tuning details from the user. We evaluate Gswitch on typical graph algorithms (BFS, CC, PR, SSSP, and BC) using Nvidia Kepler and Pascal GPUs. The results show that Gswitch runs up to 10\texttimes{} faster than the best configuration of the state-of-the-art programmable GPU-based graph processing libraries on 10 representative graphs. Gswitch outperforms Gunrock on 92.4% cases of 644 graphs which is the largest dataset evaluation reported to date.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {201–213},
numpages = {13},
keywords = {graph processing, GPU, auto-tuning},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295725,
author = {Acar, Umut A. and Aksenov, Vitaly and Chargu\'{e}raud, Arthur and Rainey, Mike},
title = {Provably and Practically Efficient Granularity Control},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295725},
doi = {10.1145/3293883.3295725},
abstract = {Over the past decade, many programming languages and systems for parallel-computing have been developed, e.g., Fork/Join and Habanero Java, Parallel Haskell, Parallel ML, and X10. Although these systems raise the level of abstraction for writing parallel codes, performance continues to require labor-intensive optimizations for coarsening the granularity of parallel executions. In this paper, we present provably and practically efficient techniques for controlling granularity within the run-time system of the language. Our starting point is "oracle-guided scheduling", a result from the functional-programming community that shows that granularity can be controlled by an "oracle" that can predict the execution time of parallel codes. We give an algorithm for implementing such an oracle and prove that it has the desired theoretical properties under the nested-parallel programming model. We implement the oracle in C++ by extending Cilk and evaluate its practical performance. The results show that our techniques can essentially eliminate hand tuning while closely matching the performance of hand tuned codes.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {214–228},
numpages = {15},
keywords = {parallel programming languages, granularity control},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295734,
author = {Li, Xiuhong and Liang, Yun and Yan, Shengen and Jia, Liancheng and Li, Yinghan},
title = {A Coordinated Tiling and Batching Framework for Efficient GEMM on GPUs},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295734},
doi = {10.1145/3293883.3295734},
abstract = {General matrix multiplication (GEMM) plays a paramount role in a broad range of domains such as deep learning, scientific computing, and image processing. The primary optimization method is to partition the matrix into many tiles and exploit the parallelism within and between tiles. The tiling hierarchy closely mirrors the thread hierarchy on GPUs. In practice, GPUs can fully unleash its computing power only when the matrix size is large and there are sufficient number of tiles and workload for each tile. However, in many real-world applications especially deep learning domain, the matrix size is small. To this end, prior work proposes batched GEMM to process a group of small independent GEMMs together by designing a single CUDA kernel for all of these GEMMs.However, the current support for batched GEMM is still rudimentary. Tiling and batching are tightly correlated. A large tile size can increase the data reuse, but it will decrease the thread-level parallelism, which further decrease the optimization space for the batching. A small tile size can increase the thread-level parallelism and then provide larger optimization space for the batching, but at the cost of sacrificing data reuse. In this paper, we propose a coordinated tiling and batching framework for accelerating GEMMs on GPUs. It is a two-phase framework, which consists of a tiling engine and a batching engine to perform efficient batched GEMM on GPUs. Tiling engine partitions the GEMMs into independent tiles and batching engine assigns the tiles to thread blocks. Moreover, we propose a general programming interface for the coordinated tiling and batching solution. Finally, experiment evaluation results on synthetic batched GEMM cases show that our framework can achieve about 1.40X performance speedup on average over the state-of-the-art technique. We also use GoogleNet as a real-world case study and our framework can achieve 1.23X speedup.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {229–241},
numpages = {13},
keywords = {batching, GEMM, GPGPU, tiling},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295731,
author = {Zhao, Qi and Qiu, Zhengyi and Jin, Guoliang},
title = {Semantics-Aware Scheduling Policies for Synchronization Determinism},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295731},
doi = {10.1145/3293883.3295731},
abstract = {A common task for all deterministic multithreading (DMT) systems is to enforce synchronization determinism. However, synchronization determinism has not been the focus of existing DMT research. Instead, most DMT systems focused on how to order data races remained after synchronization determinism is enforced. Consequently, existing scheduling policies for synchronization determinism all have limitations. They may either require performance annotations to achieve good performance or fail to provide schedule stability.In this paper, we argue that synchronization determinism is more fundamental to DMT systems than existing research suggests and propose efficient and effective scheduling policies. Our key insight is that synchronization operations actually encode programmers' intention on how inter-thread communication should be done and can be used as hints while scheduling synchronization operations. Based on this insight, we have built QiThread, a synchronization-determinism system with semantics-aware scheduling policies. Results of a diverse set of 108 programs show that QiThread is able to achieve comparable low overhead as state-of-the-art synchronization-determinism systems without the limitations associated with them.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {242–256},
numpages = {15},
keywords = {deterministic multithreading, semantics-aware policies, stable multithreading, synchronization determinism, synchronization scheduling},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295735,
author = {Singer, Kyle and Xu, Yifan and Lee, I-Ting Angelina},
title = {Proactive Work Stealing for Futures},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295735},
doi = {10.1145/3293883.3295735},
abstract = {The use of futures provides a flexible way to express parallelism and can generate arbitrary dependences among parallel subcomputations. The additional flexibility that futures provide comes with a cost, however. When scheduled using classic work stealing, a program with futures, compared to a program that uses only fork-join parallelism, can incur a much higher number of "deviations," a metric for evaluating the performance of parallel executions. All prior works assume a parsimonious work-stealing scheduler, however, where a worker thread (surrogate of a processor) steals work only when its local deque becomes empty.In this work, we investigate an alternative scheduling approach, called ProWS, where the workers perform proactive work stealing when handling future operations. We show that ProWS, for programs that use futures, can provide provably efficient execution time and equal or better bounds on the number of deviations compared to classic parsimonious work stealing. Given a computation with T1 work and T∞ span, ProWS executes the computation on P processors in expected time O(T1/P + T∞ lg P), with an additional lg P overhead on the span term compared to the parsimonious variant. For structured use of futures, where each future is single touch with no race on the future handle, the algorithm incurs deviations, matching that of the parsimonious variant. For general use of futures, the algorithm incurs O(mkT∞ + PT∞ lg P) deviations, where mk is the maximum number of future touches that are logically parallel. Compared to the bound for the parsimonious variant, O(kT∞ + PT∞), with k being the total number of touches in the entire computation, this bound is better assuming mk = Ω(P lg P) and is smaller than k, which holds true for all the benchmarks we examined.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {257–271},
numpages = {15},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295729,
author = {Hoang, Loc and Pontecorvi, Matteo and Dathathri, Roshan and Gill, Gurbinder and You, Bozhi and Pingali, Keshav and Ramachandran, Vijaya},
title = {A Round-Efficient Distributed Betweenness Centrality Algorithm},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295729},
doi = {10.1145/3293883.3295729},
abstract = {We present Min-Rounds BC (MRBC), a distributed-memory algorithm in the CONGEST model that computes the betweenness centrality (BC) of every vertex in a directed unweighted n-node graph in O(n) rounds. Min-Rounds BC also computes all-pairs-shortest-paths (APSP) in such graphs. It improves the number of rounds by at least a constant factor over previous results for unweighted directed APSP and for unweighted BC, both directed and undirected.We implemented MRBC in D-Galois, a state-of-the-art distributed graph analytics system, incorporated additional optimizations enabled by the D-Galois model, and evaluated its performance on a production cluster with up to 256 hosts using power-law and road networks. Compared to the BC algorithm of Brandes, on average, MRBC reduces the number of rounds by 14.0\texttimes{} and the communication time by 2.8\texttimes{} for the graphs in our test suite. As a result, MRBC is 2.1\texttimes{} faster on average than Brandes BC for real-world web-crawls on 256 hosts.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {272–286},
numpages = {15},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295721,
author = {K\"{u}ttler, Martin and Planeta, Maksym and Bierbaum, Jan and Weinhold, Carsten and H\"{a}rtig, Hermann and Barak, Amnon and Hoefler, Torsten},
title = {Corrected Trees for Reliable Group Communication},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295721},
doi = {10.1145/3293883.3295721},
abstract = {Driven by ever increasing performance demands of compute-intensive applications, supercomputing systems comprise more and more nodes. This growth is a significant burden for fast group communication primitives and also makes those systems more susceptible to failures of individual nodes. In this paper we present a two-phase fault-tolerant scheme for group communication. Using broadcast as an example, we provide a full-spectrum discussion of our approach --- from a formal analysis to LogP-based simulations to a message-passing-based implementation running on a large cluster. Ultimately, we are able to reduce the complex problem of reliable and fault-tolerant collective group communication to a graph theoretical renumbering problem. Both, simulations and measurements, show our solution to achieve a latency reduction of 50% with up to six times fewer messages sent in comparison to existing schemes.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {287–299},
numpages = {13},
keywords = {low-latency broadcast, MPI, HPC, gossip, LogP model},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295712,
author = {Hong, Changwan and Sukumaran-Rajam, Aravind and Nisa, Israt and Singh, Kunal and Sadayappan, P.},
title = {Adaptive Sparse Tiling for Sparse Matrix Multiplication},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295712},
doi = {10.1145/3293883.3295712},
abstract = {Tiling is a key technique for data locality optimization and is widely used in high-performance implementations of dense matrix-matrix multiplication for multicore/manycore CPUs and GPUs. However, the irregular and matrix-dependent data access pattern of sparse matrix multiplication makes it challenging to use tiling to enhance data reuse. In this paper, we devise an adaptive tiling strategy and apply it to enhance the performance of two primitives: SpMM (product of sparse matrix and dense matrix) and SDDMM (sampled dense-dense matrix multiplication). In contrast to studies that have resorted to non-standard sparse-matrix representations to enhance performance, we use the standard Compressed Sparse Row (CSR) representation, within which intra-row reordering is performed to enable adaptive tiling. Experimental evaluation using an extensive set of matrices from the Sparse Suite collection demonstrates significant performance improvement over currently available state-of-the-art alternatives.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {300–314},
numpages = {15},
keywords = {sampled dense-dense matrix multiplication, multicore/manycore, SpMM, SDDMM, GPU, tiling, sparse matrix-matrix multiplication},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295723,
author = {B\"{a}ttig, Martin and Gross, Thomas R.},
title = {Encapsulated Open Nesting for STM: Fine-Grained Higher-Level Conflict Detection},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295723},
doi = {10.1145/3293883.3295723},
abstract = {Open nesting allows replacing the automatic detection of conflicting memory accesses used in transactional memory (TM) with programmer-specified higher-level conflict detection. Higher-level conflict detection allows removing conflicts of commuting operations where the automatic conflict detection is too conservative. Different conflict detection schemes are incompatible with each other and thus must operate on separate memory locations to prevent inconsistencies. Using open nesting, a programmer implements this separation manually using source code modifications possibly assisted by static checks.Encapsulated open nesting extends open nesting by automatically preventing concurrent accesses to the same memory location using different conflict detection schemes unless all accesses are reads. This approach uses an additional synchronization layer with the same granularity as the existing synchronization of TM. This automatism reduces programming effort by allowing fine-grained use of open nesting and extends the scope of open nesting to abstractions with unknown implementation.We extend the TLRW algorithm to support encapsulated open nesting. An evaluation using an implementation of this algorithm within the Deuce framework shows an average overhead of 21.7% for encapsulated open nesting (compared to regular open nesting) for nine benchmarks while simplifying programming.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {315–326},
numpages = {12},
keywords = {open nesting, higher-level conflict detection},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295719,
author = {Jordan, Herbert and Suboti\'{c}, Pavle and Zhao, David and Scholz, Bernhard},
title = {A Specialized B-Tree for Concurrent Datalog Evaluation},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295719},
doi = {10.1145/3293883.3295719},
abstract = {Modern Datalog engines are employed in industrial applications such as graph-databases, networks, and static program analysis. To cope with vast amount of data, Datalog engines must employ parallel execution strategies, for which specialized concurrent data structures are of paramount importance.In this paper, we introduce a specialized B-tree data structure for an open-source Datalog compiler written in C++. Our data structure has been specialized for Datalog workloads running on shared-memory multi-core computers. It features (1) an optimistic locking protocol for scalability, (2) is highly tuned, and (3) uses the notion of "hints" to re-use the results of previously performed tree traversals to exploit data ordering properties exhibited by Datalog evaluation. In parallel micro-benchmarks, the new data structure achieves up to 59\texttimes{} higher performance than state-of-the-art industrial standards, while integrated into a Datalog engine it accounts for 3\texttimes{} higher, overall system performance.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {327–339},
numpages = {13},
keywords = {datalog evaluation, optimistic locking, B-tree},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295732,
author = {Utterback, Robert and Agrawal, Kunal and Fineman, Jeremy and Lee, I-Ting Angelina},
title = {Efficient Race Detection with Futures},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295732},
doi = {10.1145/3293883.3295732},
abstract = {This paper addresses the problem of provably efficient and practically good on-the-fly determinacy race detection in task parallel programs that use futures. Prior works on determinacy race detection have mostly focused on either task parallel programs that follow a series-parallel dependence structure or ones with unrestricted use of futures that generate arbitrary dependences. In this work, we consider a restricted use of futures and show that we can detect races more efficiently than with general use of futures.Specifically, we present two algorithms: MultiBags and MultiBags+. MultiBags targets programs that use futures in a restricted fashion and runs in time O(T1α(m, n)), where T1 is the sequential running time of the program, α is the inverse Ackermann's function, m is the total number of memory accesses, n is the dynamic count of places at which parallelism is created. Since α is a very slowly growing function (upper bounded by 4 for all practical purposes), it can be treated as a close-to-constant overhead. MultiBags+ is an extension of MultiBags that target programs with general use of futures. It runs in time O((T1 + k2)α(m, n)) where T1, α, m and n are defined as before, and k is the number of future operations in the computation. We implemented both algorithms and empirically demonstrate their efficiency.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {340–354},
numpages = {15},
keywords = {dynamic program analysis, determinacy race, race detection, series-parallel maintenance},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295702,
author = {Doherty, Simon and Dongol, Brijesh and Wehrheim, Heike and Derrick, John},
title = {Verifying C11 Programs Operationally},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295702},
doi = {10.1145/3293883.3295702},
abstract = {This paper develops an operational semantics for a release-acquire fragment of the C11 memory model with relaxed accesses. We show that the semantics is both sound and complete with respect to the axiomatic model of Batty et al. The semantics relies on a per-thread notion of observability, which allows one to reason about a weak memory C11 program in program order. On top of this, we develop a proof calculus for invariant-based reasoning, which we use to verify the release-acquire version of Peterson's mutual exclusion algorithm.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {355–365},
numpages = {11},
keywords = {soundness and completeness, operational semantics, C11, verification},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295726,
author = {Ozkan, Burcu Kulahcioglu and Majumdar, Rupak and Niksic, Filip},
title = {Checking Linearizability Using Hitting Families},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295726},
doi = {10.1145/3293883.3295726},
abstract = {Linearizability is a key correctness property for concurrent data types. Linearizability requires that the behavior of concurrently invoked operations of the data type be equivalent to the behavior in an execution where each operation takes effect at an instantaneous point of time between its invocation and return. Given an execution trace of operations, the problem of verifying its linearizability is NP-complete, and current exhaustive search tools scale poorly.In this work, we empirically show that linearizability of an execution trace is often witnessed by a schedule that orders only a small number of operations (the "linearizability depth") in a specific way, independently of other operations. Accordingly, one can structure the search for linearizability witnesses by exploring schedules of low linearizability depth first. We provide such an algorithm. Key to our algorithm is a procedure to generate a strong d-hitting family of schedules, which is guaranteed to cover all linearizability witnesses of depth d. A strong d-hitting family of schedules of an execution trace consists of a set of schedules, such that for each tuple of d operations in the trace, there is a schedule in the family that (i) executes these operations in the order they appear in the tuple, and (ii) as late as possible in the execution.We show that most linearizable execution traces from existing benchmarks can be witnessed by strongly d-hitting schedules for d ≤ 5. Our result suggests a practical and automated method for showing linearizability of a trace based on a prioritization of schedules parameterized by the lineariz-ability depth.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {366–377},
numpages = {12},
keywords = {hitting families, scheduling, concurrent data structures, linearizability},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295724,
author = {Voss, Caleb and Cogumbreiro, Tiago and Sarkar, Vivek},
title = {Transitive Joins: A Sound and Efficient Online Deadlock-Avoidance Policy},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295724},
doi = {10.1145/3293883.3295724},
abstract = {We introduce a new online deadlock-avoidance policy, Transitive Joins (TJ), that targets programs with dynamic task parallelism and arbitrary join operations. In this model, a computation task can asynchronously spawn new tasks and selectively join (block) on any task for which it has a handle. We prove that TJ soundly guarantees the absence of deadlock cycles among the blocking join operations. We present an algorithm for dynamically verifying TJ and show that TJ results in fewer false positives than the state-of-the-art policy, Known Joins (KJ). We evaluate an implementation of our verifier in comparison to prior work. The evaluation results show that instrumenting a program with a TJ verifier incurs geometric mean overheads of only 1.06\texttimes{} in execution time and 1.09\texttimes{} in memory usage, which is better overall than existing KJ verifiers. TJ is a practical online deadlock-avoidance policy that is applicable to a wide range of parallel programming models.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {378–390},
numpages = {13},
keywords = {deadlock avoidance, task parallelism, futures},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

@inproceedings{10.1145/3293883.3295703,
author = {Sun, Jiawen and Vandierendonck, Hans and Nikolopoulos, Dimitrios S.},
title = {VEBO: A Vertex- and Edge-Balanced Ordering Heuristic to Load Balance Parallel Graph Processing},
year = {2019},
isbn = {9781450362252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293883.3295703},
doi = {10.1145/3293883.3295703},
abstract = {This work proposes Vertex- and Edge-Balanced Ordering (VEBO): balance the number of edges and the number of unique destinations of those edges. VEBO balances edges and vertices for graphs with a power-law degree distribution, and ensures an equal degree distribution between partitions. Experimental evaluation on three shared-memory graph processing systems (Ligra, Polymer and GraphGrind) shows that VEBO achieves excellent load balance and improves performance by 1.09\texttimes{} over Ligra, 1.41\texttimes{} over Polymer and 1.65\texttimes{} over GraphGrind, compared to their respective partitioning algorithms, averaged across 8 algorithms and 7 graphs. VEBO improves GraphGrind performance with a speedup of 2.9\texttimes{} over Ligra on average.},
booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming},
pages = {391–392},
numpages = {2},
location = {Washington, District of Columbia},
series = {PPoPP '19}
}

