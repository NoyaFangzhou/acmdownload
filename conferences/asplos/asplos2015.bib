@inproceedings{10.1145/3251025,
author = {Ozturk, Ozcan},
title = {Session Details: Keynote I: Keynote Address I},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251025},
doi = {10.1145/3251025},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694375,
author = {Lee, Edward A.},
title = {Architectural Support for Cyber-Physical Systems},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694375},
doi = {10.1145/2775054.2694375},
abstract = {Cyber-physical systems are integrations of computation, communication networks, and physical dynamics. Although time plays a central role in the physical world, all widely used software abstractions lack temporal semantics. The notion of correct execution of a program written in every widely-used programming language today does not depend on the temporal behavior of the program. But temporal behavior matters in almost all systems, and most particularly in cyber-physical systems. In this talk, I will argue that time can and must become part of the semantics of programs for a large class of applications. To illustrate that this is both practical and useful, we will describe a recent effort at Berkeley in the design and implementation of timing-centric software systems. Specifically, I will describe PRET machines, which redefine the instruction-set architecture (ISA) of a microprocessor to embrace temporal semantics. Such machines can be used in high-confidence and safety-critical systems, in energy-constrained systems, in mixed-criticality systems, and as a Real-Time Unit (RTU) that cooperates with a general-purpose processor to provide real-time services, in a manner similar to how a GPU provides graphics services.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {1},
numpages = {1},
keywords = {computer architecture, embedded systems, cyber-physical systems, real-time systems}
}

@article{10.1145/2786763.2694375,
author = {Lee, Edward A.},
title = {Architectural Support for Cyber-Physical Systems},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694375},
doi = {10.1145/2786763.2694375},
abstract = {Cyber-physical systems are integrations of computation, communication networks, and physical dynamics. Although time plays a central role in the physical world, all widely used software abstractions lack temporal semantics. The notion of correct execution of a program written in every widely-used programming language today does not depend on the temporal behavior of the program. But temporal behavior matters in almost all systems, and most particularly in cyber-physical systems. In this talk, I will argue that time can and must become part of the semantics of programs for a large class of applications. To illustrate that this is both practical and useful, we will describe a recent effort at Berkeley in the design and implementation of timing-centric software systems. Specifically, I will describe PRET machines, which redefine the instruction-set architecture (ISA) of a microprocessor to embrace temporal semantics. Such machines can be used in high-confidence and safety-critical systems, in energy-constrained systems, in mixed-criticality systems, and as a Real-Time Unit (RTU) that cooperates with a general-purpose processor to provide real-time services, in a manner similar to how a GPU provides graphics services.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {1},
numpages = {1},
keywords = {embedded systems, real-time systems, computer architecture, cyber-physical systems}
}

@inproceedings{10.1145/2694344.2694375,
author = {Lee, Edward A.},
title = {Architectural Support for Cyber-Physical Systems},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694375},
doi = {10.1145/2694344.2694375},
abstract = {Cyber-physical systems are integrations of computation, communication networks, and physical dynamics. Although time plays a central role in the physical world, all widely used software abstractions lack temporal semantics. The notion of correct execution of a program written in every widely-used programming language today does not depend on the temporal behavior of the program. But temporal behavior matters in almost all systems, and most particularly in cyber-physical systems. In this talk, I will argue that time can and must become part of the semantics of programs for a large class of applications. To illustrate that this is both practical and useful, we will describe a recent effort at Berkeley in the design and implementation of timing-centric software systems. Specifically, I will describe PRET machines, which redefine the instruction-set architecture (ISA) of a microprocessor to embrace temporal semantics. Such machines can be used in high-confidence and safety-critical systems, in energy-constrained systems, in mixed-criticality systems, and as a Real-Time Unit (RTU) that cooperates with a general-purpose processor to provide real-time services, in a manner similar to how a GPU provides graphics services.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {1},
numpages = {1},
keywords = {real-time systems, cyber-physical systems, computer architecture, embedded systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251026,
author = {Demke Brown, Angela},
title = {Session Details: Session 1A: Persistent Memory},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251026},
doi = {10.1145/3251026},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694370,
author = {Zhang, Yiying and Yang, Jian and Memaripour, Amirsaman and Swanson, Steven},
title = {Mojim: A Reliable and Highly-Available Non-Volatile Memory System},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694370},
doi = {10.1145/2775054.2694370},
abstract = {Next-generation non-volatile memories (NVMs) promise DRAM-like performance, persistence, and high density. They can attach directly to processors to form non-volatile main memory (NVMM) and offer the opportunity to build very low-latency storage systems. These high-performance storage systems would be especially useful in large-scale data center environments where reliability and availability are critical. However, providing reliability and availability to NVMM is challenging, since the latency of data replication can overwhelm the low latency that NVMM should provide. We propose Mojim, a system that provides the reliability and availability that large-scale storage systems require, while preserving the performance of NVMM. Mojim achieves these goals by using a two-tier architecture in which the primary tier contains a mirrored pair of nodes and the secondary tier contains one or more secondary backup nodes with weakly consistent copies of data. Mojim uses highly-optimized replication protocols, software, and networking stacks to minimize replication costs and expose as much of NVMM?s performance as possible. We evaluate Mojim using raw DRAM as a proxy for NVMM and using an industrial NVMM emulation system. We find that Mojim provides replicated NVMM with similar or even better performance than un-replicated NVMM (reducing latency by 27% to 63% and delivering between 0.4 to 2.7X the throughput). We demonstrate that replacing MongoDB's built-in replication system with Mojim improves MongoDB's performance by 3.4 to 4X.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {3–18},
numpages = {16},
keywords = {availability, data center, distributed storage systems, keywords non-volatile memory, reliability, storage-class memory}
}

@article{10.1145/2786763.2694370,
author = {Zhang, Yiying and Yang, Jian and Memaripour, Amirsaman and Swanson, Steven},
title = {Mojim: A Reliable and Highly-Available Non-Volatile Memory System},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694370},
doi = {10.1145/2786763.2694370},
abstract = {Next-generation non-volatile memories (NVMs) promise DRAM-like performance, persistence, and high density. They can attach directly to processors to form non-volatile main memory (NVMM) and offer the opportunity to build very low-latency storage systems. These high-performance storage systems would be especially useful in large-scale data center environments where reliability and availability are critical. However, providing reliability and availability to NVMM is challenging, since the latency of data replication can overwhelm the low latency that NVMM should provide. We propose Mojim, a system that provides the reliability and availability that large-scale storage systems require, while preserving the performance of NVMM. Mojim achieves these goals by using a two-tier architecture in which the primary tier contains a mirrored pair of nodes and the secondary tier contains one or more secondary backup nodes with weakly consistent copies of data. Mojim uses highly-optimized replication protocols, software, and networking stacks to minimize replication costs and expose as much of NVMM?s performance as possible. We evaluate Mojim using raw DRAM as a proxy for NVMM and using an industrial NVMM emulation system. We find that Mojim provides replicated NVMM with similar or even better performance than un-replicated NVMM (reducing latency by 27% to 63% and delivering between 0.4 to 2.7X the throughput). We demonstrate that replacing MongoDB's built-in replication system with Mojim improves MongoDB's performance by 3.4 to 4X.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {3–18},
numpages = {16},
keywords = {availability, data center, distributed storage systems, keywords non-volatile memory, storage-class memory, reliability}
}

@inproceedings{10.1145/2694344.2694370,
author = {Zhang, Yiying and Yang, Jian and Memaripour, Amirsaman and Swanson, Steven},
title = {Mojim: A Reliable and Highly-Available Non-Volatile Memory System},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694370},
doi = {10.1145/2694344.2694370},
abstract = {Next-generation non-volatile memories (NVMs) promise DRAM-like performance, persistence, and high density. They can attach directly to processors to form non-volatile main memory (NVMM) and offer the opportunity to build very low-latency storage systems. These high-performance storage systems would be especially useful in large-scale data center environments where reliability and availability are critical. However, providing reliability and availability to NVMM is challenging, since the latency of data replication can overwhelm the low latency that NVMM should provide. We propose Mojim, a system that provides the reliability and availability that large-scale storage systems require, while preserving the performance of NVMM. Mojim achieves these goals by using a two-tier architecture in which the primary tier contains a mirrored pair of nodes and the secondary tier contains one or more secondary backup nodes with weakly consistent copies of data. Mojim uses highly-optimized replication protocols, software, and networking stacks to minimize replication costs and expose as much of NVMM?s performance as possible. We evaluate Mojim using raw DRAM as a proxy for NVMM and using an industrial NVMM emulation system. We find that Mojim provides replicated NVMM with similar or even better performance than un-replicated NVMM (reducing latency by 27% to 63% and delivering between 0.4 to 2.7X the throughput). We demonstrate that replacing MongoDB's built-in replication system with Mojim improves MongoDB's performance by 3.4 to 4X.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {3–18},
numpages = {16},
keywords = {distributed storage systems, data center, reliability, keywords non-volatile memory, availability, storage-class memory},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694352,
author = {Wang, Rujia and Jiang, Lei and Zhang, Youtao and Yang, Jun},
title = {SD-PCM: Constructing Reliable Super Dense Phase Change Memory under Write Disturbance},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694352},
doi = {10.1145/2775054.2694352},
abstract = {Phase Change Memory (PCM) has better scalability and smaller cell size comparing to DRAM. However, further scaling PCM cell in deep sub-micron regime results in significant thermal based write disturbance (WD). Naively allocating large inter-cell space increases cell size from 4F2 ideal to 12F2. While a recent work mitigates WD along word-lines through disturbance resilient data encoding, it is ineffective for WD along bit-lines, which is more severe due to widely adopted $mu$Trench structure in constructing PCM cell arrays. Without mitigating WD along bit-lines, a PCM cell still has 8F2, which is 100% larger than the ideal. In this paper, we propose SD-PCM for achieving reliable write operations in super dense PCM. In particular, we focus on mitigating WD along bit-lines such that we can construct super dense PCM chips with 4F2 cell size, i.e., the minimal for diode-switch based PCM. Based on simple verification-n-correction (VnC), we propose LazyCorrection and PreRead to effectively reduce VnC overhead and minimize cascading verification during write. We further propose (n:m)-Alloc for achieving good tradeoff between VnC overhead minimization and memory capacity loss. Our experimental results show that, comparing to a WD-free low density PCM, SD-PCM achieves 80% capacity improvement in cell arrays while incurring around 0-10% performance degradation when using different (n:m) allocators.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {19–31},
numpages = {13},
keywords = {write disturbance, phase change memory}
}

@article{10.1145/2786763.2694352,
author = {Wang, Rujia and Jiang, Lei and Zhang, Youtao and Yang, Jun},
title = {SD-PCM: Constructing Reliable Super Dense Phase Change Memory under Write Disturbance},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694352},
doi = {10.1145/2786763.2694352},
abstract = {Phase Change Memory (PCM) has better scalability and smaller cell size comparing to DRAM. However, further scaling PCM cell in deep sub-micron regime results in significant thermal based write disturbance (WD). Naively allocating large inter-cell space increases cell size from 4F2 ideal to 12F2. While a recent work mitigates WD along word-lines through disturbance resilient data encoding, it is ineffective for WD along bit-lines, which is more severe due to widely adopted $mu$Trench structure in constructing PCM cell arrays. Without mitigating WD along bit-lines, a PCM cell still has 8F2, which is 100% larger than the ideal. In this paper, we propose SD-PCM for achieving reliable write operations in super dense PCM. In particular, we focus on mitigating WD along bit-lines such that we can construct super dense PCM chips with 4F2 cell size, i.e., the minimal for diode-switch based PCM. Based on simple verification-n-correction (VnC), we propose LazyCorrection and PreRead to effectively reduce VnC overhead and minimize cascading verification during write. We further propose (n:m)-Alloc for achieving good tradeoff between VnC overhead minimization and memory capacity loss. Our experimental results show that, comparing to a WD-free low density PCM, SD-PCM achieves 80% capacity improvement in cell arrays while incurring around 0-10% performance degradation when using different (n:m) allocators.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {19–31},
numpages = {13},
keywords = {phase change memory, write disturbance}
}

@inproceedings{10.1145/2694344.2694352,
author = {Wang, Rujia and Jiang, Lei and Zhang, Youtao and Yang, Jun},
title = {SD-PCM: Constructing Reliable Super Dense Phase Change Memory under Write Disturbance},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694352},
doi = {10.1145/2694344.2694352},
abstract = {Phase Change Memory (PCM) has better scalability and smaller cell size comparing to DRAM. However, further scaling PCM cell in deep sub-micron regime results in significant thermal based write disturbance (WD). Naively allocating large inter-cell space increases cell size from 4F2 ideal to 12F2. While a recent work mitigates WD along word-lines through disturbance resilient data encoding, it is ineffective for WD along bit-lines, which is more severe due to widely adopted $mu$Trench structure in constructing PCM cell arrays. Without mitigating WD along bit-lines, a PCM cell still has 8F2, which is 100% larger than the ideal. In this paper, we propose SD-PCM for achieving reliable write operations in super dense PCM. In particular, we focus on mitigating WD along bit-lines such that we can construct super dense PCM chips with 4F2 cell size, i.e., the minimal for diode-switch based PCM. Based on simple verification-n-correction (VnC), we propose LazyCorrection and PreRead to effectively reduce VnC overhead and minimize cascading verification during write. We further propose (n:m)-Alloc for achieving good tradeoff between VnC overhead minimization and memory capacity loss. Our experimental results show that, comparing to a WD-free low density PCM, SD-PCM achieves 80% capacity improvement in cell arrays while incurring around 0-10% performance degradation when using different (n:m) allocators.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {19–31},
numpages = {13},
keywords = {write disturbance, phase change memory},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694387,
author = {Young, Vinson and Nair, Prashant J. and Qureshi, Moinuddin K.},
title = {DEUCE: Write-Efficient Encryption for Non-Volatile Memories},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694387},
doi = {10.1145/2775054.2694387},
abstract = {Phase Change Memory (PCM) is an emerging Non Volatile Memory (NVM) technology that has the potential to provide scalable high-density memory systems. While the non-volatility of PCM is a desirable property in order to save leakage power, it also has the undesirable effect of making PCM main memories susceptible to newer modes of security vulnerabilities, for example, accessibility to sensitive data if a PCM DIMM gets stolen. PCM memories can be made secure by encrypting the data. Unfortunately, such encryption comes with a significant overhead in terms of bits written to PCM memory, causing half of the bits in the line to change on every write, even if the actual number of bits being written to memory is small. Our studies show that a typical writeback modifies, on average, only 12% of the bits in the cacheline. Thus, encryption causes almost a 4x increase in the number of bits written to PCM memories. Such extraneous bit writes cause significant increase in write power, reduction in write endurance, and reduction in write bandwidth. To provide the benefit of secure memory in a write efficient manner this paper proposes Dual Counter Encryption (DEUCE). DEUCE is based on the observation that a typical writeback only changes a few words, so DEUCE reencrypts only the words that have changed. We show that DEUCE reduces the number of modified bits per writeback for a secure memory from 50% to 24%, which improves performance by 27% and increases lifetime by 2x.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {33–44},
numpages = {12},
keywords = {NVM, phase change memory, bitflip, security, encryption}
}

@article{10.1145/2786763.2694387,
author = {Young, Vinson and Nair, Prashant J. and Qureshi, Moinuddin K.},
title = {DEUCE: Write-Efficient Encryption for Non-Volatile Memories},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694387},
doi = {10.1145/2786763.2694387},
abstract = {Phase Change Memory (PCM) is an emerging Non Volatile Memory (NVM) technology that has the potential to provide scalable high-density memory systems. While the non-volatility of PCM is a desirable property in order to save leakage power, it also has the undesirable effect of making PCM main memories susceptible to newer modes of security vulnerabilities, for example, accessibility to sensitive data if a PCM DIMM gets stolen. PCM memories can be made secure by encrypting the data. Unfortunately, such encryption comes with a significant overhead in terms of bits written to PCM memory, causing half of the bits in the line to change on every write, even if the actual number of bits being written to memory is small. Our studies show that a typical writeback modifies, on average, only 12% of the bits in the cacheline. Thus, encryption causes almost a 4x increase in the number of bits written to PCM memories. Such extraneous bit writes cause significant increase in write power, reduction in write endurance, and reduction in write bandwidth. To provide the benefit of secure memory in a write efficient manner this paper proposes Dual Counter Encryption (DEUCE). DEUCE is based on the observation that a typical writeback only changes a few words, so DEUCE reencrypts only the words that have changed. We show that DEUCE reduces the number of modified bits per writeback for a secure memory from 50% to 24%, which improves performance by 27% and increases lifetime by 2x.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {33–44},
numpages = {12},
keywords = {NVM, bitflip, encryption, phase change memory, security}
}

@inproceedings{10.1145/2694344.2694387,
author = {Young, Vinson and Nair, Prashant J. and Qureshi, Moinuddin K.},
title = {DEUCE: Write-Efficient Encryption for Non-Volatile Memories},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694387},
doi = {10.1145/2694344.2694387},
abstract = {Phase Change Memory (PCM) is an emerging Non Volatile Memory (NVM) technology that has the potential to provide scalable high-density memory systems. While the non-volatility of PCM is a desirable property in order to save leakage power, it also has the undesirable effect of making PCM main memories susceptible to newer modes of security vulnerabilities, for example, accessibility to sensitive data if a PCM DIMM gets stolen. PCM memories can be made secure by encrypting the data. Unfortunately, such encryption comes with a significant overhead in terms of bits written to PCM memory, causing half of the bits in the line to change on every write, even if the actual number of bits being written to memory is small. Our studies show that a typical writeback modifies, on average, only 12% of the bits in the cacheline. Thus, encryption causes almost a 4x increase in the number of bits written to PCM memories. Such extraneous bit writes cause significant increase in write power, reduction in write endurance, and reduction in write bandwidth. To provide the benefit of secure memory in a write efficient manner this paper proposes Dual Counter Encryption (DEUCE). DEUCE is based on the observation that a typical writeback only changes a few words, so DEUCE reencrypts only the words that have changed. We show that DEUCE reduces the number of modified bits per writeback for a secure memory from 50% to 24%, which improves performance by 27% and increases lifetime by 2x.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {33–44},
numpages = {12},
keywords = {phase change memory, bitflip, security, encryption, NVM},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251027,
author = {Lebeck, Alvin},
title = {Session Details: Session 1B: Memory Models I},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251027},
doi = {10.1145/3251027},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694374,
author = {Morrison, Adam and Afek, Yehuda},
title = {Temporally Bounding TSO for Fence-Free Asymmetric Synchronization},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694374},
doi = {10.1145/2775054.2694374},
abstract = {This paper introduces a temporally bounded total store ordering (TBTSO) memory model, and shows that it enables nonblocking fence-free solutions to asymmetric synchronization problems, such as those arising in memory reclamation and biased locking.TBTSO strengthens the TSO memory model by bounding the time it takes a store to drain from the store buffer into memory. This bound enables devising fence-free algorithms for asymmetric problems, which require a performance-critical fast path to synchronize with an infrequently executed slow path. We demonstrate this by constructing (1) a fence-free version of the hazard pointers memory reclamation scheme, and (2) a fence-free biased lock algorithm which is compatible with unmanaged environments as it does not rely on safe points or similar mechanisms.We further argue that TBTSO can be implemented in hardware with modest modifications to existing TSO architectures. However, our design makes assumptions about proprietary implementation details of commercial hardware; it thus best serves as a starting point for a discussion on the feasibility of hardware TBTSO implementation. We also show how minimal OS support enables the adaptation of TBTSO algorithms to x86 systems.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {45–58},
numpages = {14},
keywords = {biased locks, hazard pointers, TSO, memory fences, bounded TSO}
}

@article{10.1145/2786763.2694374,
author = {Morrison, Adam and Afek, Yehuda},
title = {Temporally Bounding TSO for Fence-Free Asymmetric Synchronization},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694374},
doi = {10.1145/2786763.2694374},
abstract = {This paper introduces a temporally bounded total store ordering (TBTSO) memory model, and shows that it enables nonblocking fence-free solutions to asymmetric synchronization problems, such as those arising in memory reclamation and biased locking.TBTSO strengthens the TSO memory model by bounding the time it takes a store to drain from the store buffer into memory. This bound enables devising fence-free algorithms for asymmetric problems, which require a performance-critical fast path to synchronize with an infrequently executed slow path. We demonstrate this by constructing (1) a fence-free version of the hazard pointers memory reclamation scheme, and (2) a fence-free biased lock algorithm which is compatible with unmanaged environments as it does not rely on safe points or similar mechanisms.We further argue that TBTSO can be implemented in hardware with modest modifications to existing TSO architectures. However, our design makes assumptions about proprietary implementation details of commercial hardware; it thus best serves as a starting point for a discussion on the feasibility of hardware TBTSO implementation. We also show how minimal OS support enables the adaptation of TBTSO algorithms to x86 systems.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {45–58},
numpages = {14},
keywords = {biased locks, TSO, bounded TSO, hazard pointers, memory fences}
}

@inproceedings{10.1145/2694344.2694374,
author = {Morrison, Adam and Afek, Yehuda},
title = {Temporally Bounding TSO for Fence-Free Asymmetric Synchronization},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694374},
doi = {10.1145/2694344.2694374},
abstract = {This paper introduces a temporally bounded total store ordering (TBTSO) memory model, and shows that it enables nonblocking fence-free solutions to asymmetric synchronization problems, such as those arising in memory reclamation and biased locking.TBTSO strengthens the TSO memory model by bounding the time it takes a store to drain from the store buffer into memory. This bound enables devising fence-free algorithms for asymmetric problems, which require a performance-critical fast path to synchronize with an infrequently executed slow path. We demonstrate this by constructing (1) a fence-free version of the hazard pointers memory reclamation scheme, and (2) a fence-free biased lock algorithm which is compatible with unmanaged environments as it does not rely on safe points or similar mechanisms.We further argue that TBTSO can be implemented in hardware with modest modifications to existing TSO architectures. However, our design makes assumptions about proprietary implementation details of commercial hardware; it thus best serves as a starting point for a discussion on the feasibility of hardware TBTSO implementation. We also show how minimal OS support enables the adaptation of TBTSO algorithms to x86 systems.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {45–58},
numpages = {14},
keywords = {TSO, biased locks, hazard pointers, bounded TSO, memory fences},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694393,
author = {Matveev, Alexander and Shavit, Nir},
title = {Reduced Hardware NOrec: A Safe and Scalable Hybrid Transactional Memory},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694393},
doi = {10.1145/2775054.2694393},
abstract = {Because of hardware TM limitations, software fallbacks are the only way to make TM algorithms guarantee progress. Nevertheless, all known software fallbacks to date, from simple locks to sophisticated versions of the NOrec Hybrid TM algorithm, have either limited scalability or weakened semantics. We propose a novel reduced-hardware (RH) version of the NOrec HyTM algorithm. Instead of an all-software slow path, in our RH NOrec the slow-path is a "mix" of hardware and software: one short hardware transaction executes a maximal amount of initial reads in the hardware, and the second executes all of the writes. This novel combination of the RH approach and the NOrec algorithm delivers the first Hybrid TM that scales while fully preserving the hardware's original semantics of opacity and privatization.Our GCC implementation of RH NOrec is promising in that it shows improved performance relative to all prior methods, at the concurrency levels we could test today.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {59–71},
numpages = {13},
keywords = {design, transactional memory, algorithms}
}

@article{10.1145/2786763.2694393,
author = {Matveev, Alexander and Shavit, Nir},
title = {Reduced Hardware NOrec: A Safe and Scalable Hybrid Transactional Memory},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694393},
doi = {10.1145/2786763.2694393},
abstract = {Because of hardware TM limitations, software fallbacks are the only way to make TM algorithms guarantee progress. Nevertheless, all known software fallbacks to date, from simple locks to sophisticated versions of the NOrec Hybrid TM algorithm, have either limited scalability or weakened semantics. We propose a novel reduced-hardware (RH) version of the NOrec HyTM algorithm. Instead of an all-software slow path, in our RH NOrec the slow-path is a "mix" of hardware and software: one short hardware transaction executes a maximal amount of initial reads in the hardware, and the second executes all of the writes. This novel combination of the RH approach and the NOrec algorithm delivers the first Hybrid TM that scales while fully preserving the hardware's original semantics of opacity and privatization.Our GCC implementation of RH NOrec is promising in that it shows improved performance relative to all prior methods, at the concurrency levels we could test today.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {59–71},
numpages = {13},
keywords = {design, transactional memory, algorithms}
}

@inproceedings{10.1145/2694344.2694393,
author = {Matveev, Alexander and Shavit, Nir},
title = {Reduced Hardware NOrec: A Safe and Scalable Hybrid Transactional Memory},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694393},
doi = {10.1145/2694344.2694393},
abstract = {Because of hardware TM limitations, software fallbacks are the only way to make TM algorithms guarantee progress. Nevertheless, all known software fallbacks to date, from simple locks to sophisticated versions of the NOrec Hybrid TM algorithm, have either limited scalability or weakened semantics. We propose a novel reduced-hardware (RH) version of the NOrec HyTM algorithm. Instead of an all-software slow path, in our RH NOrec the slow-path is a "mix" of hardware and software: one short hardware transaction executes a maximal amount of initial reads in the hardware, and the second executes all of the writes. This novel combination of the RH approach and the NOrec algorithm delivers the first Hybrid TM that scales while fully preserving the hardware's original semantics of opacity and privatization.Our GCC implementation of RH NOrec is promising in that it shows improved performance relative to all prior methods, at the concurrency levels we could test today.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {59–71},
numpages = {13},
keywords = {design, transactional memory, algorithms},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694350,
author = {Orr, Marc S. and Che, Shuai and Yilmazer, Ayse and Beckmann, Bradford M. and Hill, Mark D. and Wood, David A.},
title = {Synchronization Using Remote-Scope Promotion},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694350},
doi = {10.1145/2775054.2694350},
abstract = {Heterogeneous system architecture (HSA) and OpenCL define scoped synchronization to facilitate low overhead communication across a subset of threads. Scoped synchronization works well for static sharing patterns, where consumer threads are known a priori. It works poorly for dynamic sharing patterns (e.g., work stealing) where programmers cannot use a faster small scope due to the rare possibility that the work is stolen by a thread in a distant slower scope. This puts programmers in a conundrum: optimize the common case by synchronizing at a faster small scope or use work stealing at a slower large scope. In this paper, we propose to extend scoped synchronization with remote-scope promotion. This allows the most frequent sharers to synchronize through a small scope. Infrequent sharers synchronize by promoting that remote small scope to a larger shared scope. Synchronization using remote-scope promotion provides performance robustness for dynamic workloads, where the benefits provided by scoped synchronization and work stealing are hard to anticipate. Compared to a na\"{\i}ve baseline, static scoped synchronization alone achieves a 1.07x speedup on average and dynamic work stealing alone achieves a 1.18x speedup on average. In contrast, synchronization using remote-scope promotion achieves a robust 1.25x speedup on average, across a diverse set of graph benchmarks and inputs.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {73–86},
numpages = {14},
keywords = {memory model, work stealing, scoped synchronization, graphics processing unit (GPU), scope promotion}
}

@article{10.1145/2786763.2694350,
author = {Orr, Marc S. and Che, Shuai and Yilmazer, Ayse and Beckmann, Bradford M. and Hill, Mark D. and Wood, David A.},
title = {Synchronization Using Remote-Scope Promotion},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694350},
doi = {10.1145/2786763.2694350},
abstract = {Heterogeneous system architecture (HSA) and OpenCL define scoped synchronization to facilitate low overhead communication across a subset of threads. Scoped synchronization works well for static sharing patterns, where consumer threads are known a priori. It works poorly for dynamic sharing patterns (e.g., work stealing) where programmers cannot use a faster small scope due to the rare possibility that the work is stolen by a thread in a distant slower scope. This puts programmers in a conundrum: optimize the common case by synchronizing at a faster small scope or use work stealing at a slower large scope. In this paper, we propose to extend scoped synchronization with remote-scope promotion. This allows the most frequent sharers to synchronize through a small scope. Infrequent sharers synchronize by promoting that remote small scope to a larger shared scope. Synchronization using remote-scope promotion provides performance robustness for dynamic workloads, where the benefits provided by scoped synchronization and work stealing are hard to anticipate. Compared to a na\"{\i}ve baseline, static scoped synchronization alone achieves a 1.07x speedup on average and dynamic work stealing alone achieves a 1.18x speedup on average. In contrast, synchronization using remote-scope promotion achieves a robust 1.25x speedup on average, across a diverse set of graph benchmarks and inputs.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {73–86},
numpages = {14},
keywords = {work stealing, memory model, scope promotion, graphics processing unit (GPU), scoped synchronization}
}

@inproceedings{10.1145/2694344.2694350,
author = {Orr, Marc S. and Che, Shuai and Yilmazer, Ayse and Beckmann, Bradford M. and Hill, Mark D. and Wood, David A.},
title = {Synchronization Using Remote-Scope Promotion},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694350},
doi = {10.1145/2694344.2694350},
abstract = {Heterogeneous system architecture (HSA) and OpenCL define scoped synchronization to facilitate low overhead communication across a subset of threads. Scoped synchronization works well for static sharing patterns, where consumer threads are known a priori. It works poorly for dynamic sharing patterns (e.g., work stealing) where programmers cannot use a faster small scope due to the rare possibility that the work is stolen by a thread in a distant slower scope. This puts programmers in a conundrum: optimize the common case by synchronizing at a faster small scope or use work stealing at a slower large scope. In this paper, we propose to extend scoped synchronization with remote-scope promotion. This allows the most frequent sharers to synchronize through a small scope. Infrequent sharers synchronize by promoting that remote small scope to a larger shared scope. Synchronization using remote-scope promotion provides performance robustness for dynamic workloads, where the benefits provided by scoped synchronization and work stealing are hard to anticipate. Compared to a na\"{\i}ve baseline, static scoped synchronization alone achieves a 1.07x speedup on average and dynamic work stealing alone achieves a 1.18x speedup on average. In contrast, synchronization using remote-scope promotion achieves a robust 1.25x speedup on average, across a diverse set of graph benchmarks and inputs.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {73–86},
numpages = {14},
keywords = {memory model, scope promotion, work stealing, scoped synchronization, graphics processing unit (GPU)},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251028,
author = {Criswell, John},
title = {Session Details: Session 2A: Memory and Security I},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251028},
doi = {10.1145/3251028},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694385,
author = {Liu, Chang and Harris, Austin and Maas, Martin and Hicks, Michael and Tiwari, Mohit and Shi, Elaine},
title = {GhostRider: A Hardware-Software System for Memory Trace Oblivious Computation},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694385},
doi = {10.1145/2775054.2694385},
abstract = {This paper presents a new, co-designed compiler and architecture called GhostRider for supporting privacy preserving computation in the cloud. GhostRider ensures all programs satisfy a property called memory-trace obliviousness (MTO): Even an adversary that observes memory, bus traffic, and access times while the program executes can learn nothing about the program's sensitive inputs and outputs. One way to achieve MTO is to employ Oblivious RAM (ORAM), allocating all code and data in a single ORAM bank, and to also disable caches or fix the rate of memory traffic. This baseline approach can be inefficient, and so GhostRider's compiler uses a program analysis to do better, allocating data to non-oblivious, encrypted RAM (ERAM) and employing a scratchpad when doing so will not compromise MTO. The compiler can also allocate to multiple ORAM banks, which sometimes significantly reduces access times.We have formalized our approach and proved it enjoys MTO. Our FPGA-based hardware prototype and simulation results show that GhostRider significantly outperforms the baseline strategy.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {87–101},
numpages = {15},
keywords = {oblivious ram, secure type system, memory trace obliviousness}
}

@article{10.1145/2786763.2694385,
author = {Liu, Chang and Harris, Austin and Maas, Martin and Hicks, Michael and Tiwari, Mohit and Shi, Elaine},
title = {GhostRider: A Hardware-Software System for Memory Trace Oblivious Computation},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694385},
doi = {10.1145/2786763.2694385},
abstract = {This paper presents a new, co-designed compiler and architecture called GhostRider for supporting privacy preserving computation in the cloud. GhostRider ensures all programs satisfy a property called memory-trace obliviousness (MTO): Even an adversary that observes memory, bus traffic, and access times while the program executes can learn nothing about the program's sensitive inputs and outputs. One way to achieve MTO is to employ Oblivious RAM (ORAM), allocating all code and data in a single ORAM bank, and to also disable caches or fix the rate of memory traffic. This baseline approach can be inefficient, and so GhostRider's compiler uses a program analysis to do better, allocating data to non-oblivious, encrypted RAM (ERAM) and employing a scratchpad when doing so will not compromise MTO. The compiler can also allocate to multiple ORAM banks, which sometimes significantly reduces access times.We have formalized our approach and proved it enjoys MTO. Our FPGA-based hardware prototype and simulation results show that GhostRider significantly outperforms the baseline strategy.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {87–101},
numpages = {15},
keywords = {oblivious ram, memory trace obliviousness, secure type system}
}

@inproceedings{10.1145/2694344.2694385,
author = {Liu, Chang and Harris, Austin and Maas, Martin and Hicks, Michael and Tiwari, Mohit and Shi, Elaine},
title = {GhostRider: A Hardware-Software System for Memory Trace Oblivious Computation},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694385},
doi = {10.1145/2694344.2694385},
abstract = {This paper presents a new, co-designed compiler and architecture called GhostRider for supporting privacy preserving computation in the cloud. GhostRider ensures all programs satisfy a property called memory-trace obliviousness (MTO): Even an adversary that observes memory, bus traffic, and access times while the program executes can learn nothing about the program's sensitive inputs and outputs. One way to achieve MTO is to employ Oblivious RAM (ORAM), allocating all code and data in a single ORAM bank, and to also disable caches or fix the rate of memory traffic. This baseline approach can be inefficient, and so GhostRider's compiler uses a program analysis to do better, allocating data to non-oblivious, encrypted RAM (ERAM) and employing a scratchpad when doing so will not compromise MTO. The compiler can also allocate to multiple ORAM banks, which sometimes significantly reduces access times.We have formalized our approach and proved it enjoys MTO. Our FPGA-based hardware prototype and simulation results show that GhostRider significantly outperforms the baseline strategy.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {87–101},
numpages = {15},
keywords = {secure type system, memory trace obliviousness, oblivious ram},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694353,
author = {Fletcher, Christopher W. and Ren, Ling and Kwon, Albert and van Dijk, Marten and Devadas, Srinivas},
title = {Freecursive ORAM: [Nearly] Free Recursion and Integrity Verification for Position-Based Oblivious RAM},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694353},
doi = {10.1145/2775054.2694353},
abstract = {Oblivious RAM (ORAM) is a cryptographic primitive that hides memory access patterns as seen by untrusted storage. Recently, ORAM has been architected into secure processors. A big challenge for hardware ORAM schemes is how to efficiently manage the Position Map (PosMap), a central component in modern ORAM algorithms. Implemented naively, the PosMap causes ORAM to be fundamentally unscalable in terms of on-chip area. On the other hand, a technique called Recursive ORAM fixes the area problem yet significantly increases ORAM's performance overhead.To address this challenge, we propose three new mechanisms. We propose a new ORAM structure called the PosMap Lookaside Buffer (PLB) and PosMap compression techniques to reduce the performance overhead from Recursive ORAM empirically (the latter also improves the construction asymptotically). Through simulation, we show that these techniques reduce the memory bandwidth overhead needed to support recursion by 95%, reduce overall ORAM bandwidth by 37% and improve overall SPEC benchmark performance by 1.27x. We then show how our PosMap compression techniques further facilitate an extremely efficient integrity verification scheme for ORAM which we call PosMap MAC (PMMAC). For a practical parameterization, PMMAC reduces the amount of hashing needed for integrity checking by &gt;= 68x relative to prior schemes and introduces only 7% performance overhead.We prototype our mechanisms in hardware and report area and clock frequency for a complete ORAM design post-synthesis and post-layout using an ASIC flow in a 32~nm commercial process. With 2 DRAM channels, the design post-layout runs at 1~GHz and has a total area of .47~mm2. Depending on PLB-specific parameters, the PLB accounts for 10% to 26% area. PMMAC costs 12% of total design area. Our work is the first to prototype Recursive ORAM or ORAM with any integrity scheme in hardware.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {103–116},
numpages = {14},
keywords = {security, secure processor, oblivious ram}
}

@article{10.1145/2786763.2694353,
author = {Fletcher, Christopher W. and Ren, Ling and Kwon, Albert and van Dijk, Marten and Devadas, Srinivas},
title = {Freecursive ORAM: [Nearly] Free Recursion and Integrity Verification for Position-Based Oblivious RAM},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694353},
doi = {10.1145/2786763.2694353},
abstract = {Oblivious RAM (ORAM) is a cryptographic primitive that hides memory access patterns as seen by untrusted storage. Recently, ORAM has been architected into secure processors. A big challenge for hardware ORAM schemes is how to efficiently manage the Position Map (PosMap), a central component in modern ORAM algorithms. Implemented naively, the PosMap causes ORAM to be fundamentally unscalable in terms of on-chip area. On the other hand, a technique called Recursive ORAM fixes the area problem yet significantly increases ORAM's performance overhead.To address this challenge, we propose three new mechanisms. We propose a new ORAM structure called the PosMap Lookaside Buffer (PLB) and PosMap compression techniques to reduce the performance overhead from Recursive ORAM empirically (the latter also improves the construction asymptotically). Through simulation, we show that these techniques reduce the memory bandwidth overhead needed to support recursion by 95%, reduce overall ORAM bandwidth by 37% and improve overall SPEC benchmark performance by 1.27x. We then show how our PosMap compression techniques further facilitate an extremely efficient integrity verification scheme for ORAM which we call PosMap MAC (PMMAC). For a practical parameterization, PMMAC reduces the amount of hashing needed for integrity checking by &gt;= 68x relative to prior schemes and introduces only 7% performance overhead.We prototype our mechanisms in hardware and report area and clock frequency for a complete ORAM design post-synthesis and post-layout using an ASIC flow in a 32~nm commercial process. With 2 DRAM channels, the design post-layout runs at 1~GHz and has a total area of .47~mm2. Depending on PLB-specific parameters, the PLB accounts for 10% to 26% area. PMMAC costs 12% of total design area. Our work is the first to prototype Recursive ORAM or ORAM with any integrity scheme in hardware.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {103–116},
numpages = {14},
keywords = {oblivious ram, secure processor, security}
}

@inproceedings{10.1145/2694344.2694353,
author = {Fletcher, Christopher W. and Ren, Ling and Kwon, Albert and van Dijk, Marten and Devadas, Srinivas},
title = {Freecursive ORAM: [Nearly] Free Recursion and Integrity Verification for Position-Based Oblivious RAM},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694353},
doi = {10.1145/2694344.2694353},
abstract = {Oblivious RAM (ORAM) is a cryptographic primitive that hides memory access patterns as seen by untrusted storage. Recently, ORAM has been architected into secure processors. A big challenge for hardware ORAM schemes is how to efficiently manage the Position Map (PosMap), a central component in modern ORAM algorithms. Implemented naively, the PosMap causes ORAM to be fundamentally unscalable in terms of on-chip area. On the other hand, a technique called Recursive ORAM fixes the area problem yet significantly increases ORAM's performance overhead.To address this challenge, we propose three new mechanisms. We propose a new ORAM structure called the PosMap Lookaside Buffer (PLB) and PosMap compression techniques to reduce the performance overhead from Recursive ORAM empirically (the latter also improves the construction asymptotically). Through simulation, we show that these techniques reduce the memory bandwidth overhead needed to support recursion by 95%, reduce overall ORAM bandwidth by 37% and improve overall SPEC benchmark performance by 1.27x. We then show how our PosMap compression techniques further facilitate an extremely efficient integrity verification scheme for ORAM which we call PosMap MAC (PMMAC). For a practical parameterization, PMMAC reduces the amount of hashing needed for integrity checking by &gt;= 68x relative to prior schemes and introduces only 7% performance overhead.We prototype our mechanisms in hardware and report area and clock frequency for a complete ORAM design post-synthesis and post-layout using an ASIC flow in a 32~nm commercial process. With 2 DRAM channels, the design post-layout runs at 1~GHz and has a total area of .47~mm2. Depending on PLB-specific parameters, the PLB accounts for 10% to 26% area. PMMAC costs 12% of total design area. Our work is the first to prototype Recursive ORAM or ORAM with any integrity scheme in hardware.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {103–116},
numpages = {14},
keywords = {secure processor, oblivious ram, security},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694367,
author = {Chisnall, David and Rothwell, Colin and Watson, Robert N.M. and Woodruff, Jonathan and Vadera, Munraj and Moore, Simon W. and Roe, Michael and Davis, Brooks and Neumann, Peter G.},
title = {Beyond the PDP-11: Architectural Support for a Memory-Safe C Abstract Machine},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694367},
doi = {10.1145/2775054.2694367},
abstract = {We propose a new memory-safe interpretation of the C abstract machine that provides stronger protection to benefit security and debugging. Despite ambiguities in the specification intended to provide implementation flexibility, contemporary implementations of C have converged on a memory model similar to the PDP-11, the original target for C. This model lacks support for memory safety despite well-documented impacts on security and reliability.Attempts to change this model are often hampered by assumptions embedded in a large body of existing C code, dating back to the memory model exposed by the original C compiler for the PDP-11. Our experience with attempting to implement a memory-safe variant of C on the CHERI experimental microprocessor led us to identify a number of problematic idioms. We describe these as well as their interaction with existing memory safety schemes and the assumptions that they make beyond the requirements of the C specification. Finally, we refine the CHERI ISA and abstract model for C, by combining elements of the CHERI capability model and fat pointers, and present a softcore CPU that implements a C abstract machine that can run legacy C code with strong memory protection guarantees.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {117–130},
numpages = {14},
keywords = {bounds checking, processor design, security, C language, memory safety, memory protection, capabilities, compilers}
}

@article{10.1145/2786763.2694367,
author = {Chisnall, David and Rothwell, Colin and Watson, Robert N.M. and Woodruff, Jonathan and Vadera, Munraj and Moore, Simon W. and Roe, Michael and Davis, Brooks and Neumann, Peter G.},
title = {Beyond the PDP-11: Architectural Support for a Memory-Safe C Abstract Machine},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694367},
doi = {10.1145/2786763.2694367},
abstract = {We propose a new memory-safe interpretation of the C abstract machine that provides stronger protection to benefit security and debugging. Despite ambiguities in the specification intended to provide implementation flexibility, contemporary implementations of C have converged on a memory model similar to the PDP-11, the original target for C. This model lacks support for memory safety despite well-documented impacts on security and reliability.Attempts to change this model are often hampered by assumptions embedded in a large body of existing C code, dating back to the memory model exposed by the original C compiler for the PDP-11. Our experience with attempting to implement a memory-safe variant of C on the CHERI experimental microprocessor led us to identify a number of problematic idioms. We describe these as well as their interaction with existing memory safety schemes and the assumptions that they make beyond the requirements of the C specification. Finally, we refine the CHERI ISA and abstract model for C, by combining elements of the CHERI capability model and fat pointers, and present a softcore CPU that implements a C abstract machine that can run legacy C code with strong memory protection guarantees.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {117–130},
numpages = {14},
keywords = {C language, processor design, memory protection, memory safety, bounds checking, compilers, security, capabilities}
}

@inproceedings{10.1145/2694344.2694367,
author = {Chisnall, David and Rothwell, Colin and Watson, Robert N.M. and Woodruff, Jonathan and Vadera, Munraj and Moore, Simon W. and Roe, Michael and Davis, Brooks and Neumann, Peter G.},
title = {Beyond the PDP-11: Architectural Support for a Memory-Safe C Abstract Machine},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694367},
doi = {10.1145/2694344.2694367},
abstract = {We propose a new memory-safe interpretation of the C abstract machine that provides stronger protection to benefit security and debugging. Despite ambiguities in the specification intended to provide implementation flexibility, contemporary implementations of C have converged on a memory model similar to the PDP-11, the original target for C. This model lacks support for memory safety despite well-documented impacts on security and reliability.Attempts to change this model are often hampered by assumptions embedded in a large body of existing C code, dating back to the memory model exposed by the original C compiler for the PDP-11. Our experience with attempting to implement a memory-safe variant of C on the CHERI experimental microprocessor led us to identify a number of problematic idioms. We describe these as well as their interaction with existing memory safety schemes and the assumptions that they make beyond the requirements of the C specification. Finally, we refine the CHERI ISA and abstract model for C, by combining elements of the CHERI capability model and fat pointers, and present a softcore CPU that implements a C abstract machine that can run legacy C code with strong memory protection guarantees.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {117–130},
numpages = {14},
keywords = {memory protection, processor design, memory safety, security, C language, bounds checking, capabilities, compilers},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251029,
author = {Tang, Lingjia},
title = {Session Details: Session 2B: Warehouse Scale Computing I},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251029},
doi = {10.1145/3251029},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694382,
author = {Ma, Jiuyue and Sui, Xiufeng and Sun, Ninghui and Li, Yupeng and Yu, Zihao and Huang, Bowen and Xu, Tianni and Yao, Zhicheng and Chen, Yun and Wang, Haibin and Zhang, Lixin and Bao, Yungang},
title = {Supporting Differentiated Services in Computers via Programmable Architecture for Resourcing-on-Demand (PARD)},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694382},
doi = {10.1145/2775054.2694382},
abstract = {This paper presents PARD, a programmable architecture for resourcing-on-demand that provides a new programming interface to convey an application's high-level information like quality-of-service requirements to the hardware. PARD enables new functionalities like fully hardware-supported virtualization and differentiated services in computers. PARD is inspired by the observation that a computer is inherently a network in which hardware components communicate via packets (e.g., over the NoC or PCIe). We apply principles of software-defined networking to this intra-computer network and address three major challenges. First, to deal with the semantic gap between high-level applications and underlying hardware packets, PARD attaches a high-level semantic tag (e.g., a virtual machine or thread ID) to each memory-access, I/O, or interrupt packet. Second, to make hardware components more manageable, PARD implements programmable control planes that can be integrated into various shared resources (e.g., cache, DRAM, and I/O devices) and can differentially process packets according to tag-based rules. Third, to facilitate programming, PARD abstracts all control planes as a device file tree to provide a uniform programming interface via which users create and apply tag-based rules.Full-system simulation results show that by co-locating latencycritical memcached applications with other workloads PARD can improve a four-core computer's CPU utilization by up to a factor of four without significantly increasing tail latency. FPGA emulation based on a preliminary RTL implementation demonstrates that the cache control plane introduces no extra latency and that the memory control plane can reduce queueing delay for high-priority memory-access requests by up to a factor of 5.6.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {131–143},
numpages = {13},
keywords = {hardware/software interface, QoS, data center}
}

@article{10.1145/2786763.2694382,
author = {Ma, Jiuyue and Sui, Xiufeng and Sun, Ninghui and Li, Yupeng and Yu, Zihao and Huang, Bowen and Xu, Tianni and Yao, Zhicheng and Chen, Yun and Wang, Haibin and Zhang, Lixin and Bao, Yungang},
title = {Supporting Differentiated Services in Computers via Programmable Architecture for Resourcing-on-Demand (PARD)},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694382},
doi = {10.1145/2786763.2694382},
abstract = {This paper presents PARD, a programmable architecture for resourcing-on-demand that provides a new programming interface to convey an application's high-level information like quality-of-service requirements to the hardware. PARD enables new functionalities like fully hardware-supported virtualization and differentiated services in computers. PARD is inspired by the observation that a computer is inherently a network in which hardware components communicate via packets (e.g., over the NoC or PCIe). We apply principles of software-defined networking to this intra-computer network and address three major challenges. First, to deal with the semantic gap between high-level applications and underlying hardware packets, PARD attaches a high-level semantic tag (e.g., a virtual machine or thread ID) to each memory-access, I/O, or interrupt packet. Second, to make hardware components more manageable, PARD implements programmable control planes that can be integrated into various shared resources (e.g., cache, DRAM, and I/O devices) and can differentially process packets according to tag-based rules. Third, to facilitate programming, PARD abstracts all control planes as a device file tree to provide a uniform programming interface via which users create and apply tag-based rules.Full-system simulation results show that by co-locating latencycritical memcached applications with other workloads PARD can improve a four-core computer's CPU utilization by up to a factor of four without significantly increasing tail latency. FPGA emulation based on a preliminary RTL implementation demonstrates that the cache control plane introduces no extra latency and that the memory control plane can reduce queueing delay for high-priority memory-access requests by up to a factor of 5.6.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {131–143},
numpages = {13},
keywords = {data center, QoS, hardware/software interface}
}

@inproceedings{10.1145/2694344.2694382,
author = {Ma, Jiuyue and Sui, Xiufeng and Sun, Ninghui and Li, Yupeng and Yu, Zihao and Huang, Bowen and Xu, Tianni and Yao, Zhicheng and Chen, Yun and Wang, Haibin and Zhang, Lixin and Bao, Yungang},
title = {Supporting Differentiated Services in Computers via Programmable Architecture for Resourcing-on-Demand (PARD)},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694382},
doi = {10.1145/2694344.2694382},
abstract = {This paper presents PARD, a programmable architecture for resourcing-on-demand that provides a new programming interface to convey an application's high-level information like quality-of-service requirements to the hardware. PARD enables new functionalities like fully hardware-supported virtualization and differentiated services in computers. PARD is inspired by the observation that a computer is inherently a network in which hardware components communicate via packets (e.g., over the NoC or PCIe). We apply principles of software-defined networking to this intra-computer network and address three major challenges. First, to deal with the semantic gap between high-level applications and underlying hardware packets, PARD attaches a high-level semantic tag (e.g., a virtual machine or thread ID) to each memory-access, I/O, or interrupt packet. Second, to make hardware components more manageable, PARD implements programmable control planes that can be integrated into various shared resources (e.g., cache, DRAM, and I/O devices) and can differentially process packets according to tag-based rules. Third, to facilitate programming, PARD abstracts all control planes as a device file tree to provide a uniform programming interface via which users create and apply tag-based rules.Full-system simulation results show that by co-locating latencycritical memcached applications with other workloads PARD can improve a four-core computer's CPU utilization by up to a factor of four without significantly increasing tail latency. FPGA emulation based on a preliminary RTL implementation demonstrates that the cache control plane introduces no extra latency and that the memory control plane can reduce queueing delay for high-priority memory-access requests by up to a factor of 5.6.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {131–143},
numpages = {13},
keywords = {data center, QoS, hardware/software interface},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694349,
author = {Omote, Yushi and Shinagawa, Takahiro and Kato, Kazuhiko},
title = {Improving Agility and Elasticity in Bare-Metal Clouds},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694349},
doi = {10.1145/2775054.2694349},
abstract = {Bare-metal clouds are an emerging infrastructure-as-a-service (IaaS) that leases physical machines (bare-metal instances) rather than virtual machines, allowing resource-intensive applications to have exclusive access to physical hardware. Unfortunately, bare-metal instances require time-consuming or OS-specific tasks for deployment due to the lack of virtualization layers, thereby sacrificing several beneficial features of traditional IaaS clouds such as agility, elasticity, and OS transparency. We present BMcast, an OS deployment system with a special-purpose de-virtualizable virtual machine monitor (VMM) that supports quick and OS-transparent startup of bare-metal instances. BMcast performs streaming OS deployment while allowing direct access to physical hardware from the guest OS, and then disappears after completing the deployment. Quick startup of instances improves agility and elasticity significantly, and OS transparency greatly simplifies management tasks for cloud customers. Experimental results have confirmed that BMcast initiated a bare-metal instance 8.6 times faster than image copying, and database performance on BMcast during streaming OS deployment was comparable to that on a state-of-the-art VMM without performing deployment. BMcast incurred zero overhead after de-virtualization.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {145–159},
numpages = {15},
keywords = {device mediators, virtualization, bare-metal clouds, operating systems}
}

@article{10.1145/2786763.2694349,
author = {Omote, Yushi and Shinagawa, Takahiro and Kato, Kazuhiko},
title = {Improving Agility and Elasticity in Bare-Metal Clouds},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694349},
doi = {10.1145/2786763.2694349},
abstract = {Bare-metal clouds are an emerging infrastructure-as-a-service (IaaS) that leases physical machines (bare-metal instances) rather than virtual machines, allowing resource-intensive applications to have exclusive access to physical hardware. Unfortunately, bare-metal instances require time-consuming or OS-specific tasks for deployment due to the lack of virtualization layers, thereby sacrificing several beneficial features of traditional IaaS clouds such as agility, elasticity, and OS transparency. We present BMcast, an OS deployment system with a special-purpose de-virtualizable virtual machine monitor (VMM) that supports quick and OS-transparent startup of bare-metal instances. BMcast performs streaming OS deployment while allowing direct access to physical hardware from the guest OS, and then disappears after completing the deployment. Quick startup of instances improves agility and elasticity significantly, and OS transparency greatly simplifies management tasks for cloud customers. Experimental results have confirmed that BMcast initiated a bare-metal instance 8.6 times faster than image copying, and database performance on BMcast during streaming OS deployment was comparable to that on a state-of-the-art VMM without performing deployment. BMcast incurred zero overhead after de-virtualization.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {145–159},
numpages = {15},
keywords = {bare-metal clouds, device mediators, operating systems, virtualization}
}

@inproceedings{10.1145/2694344.2694349,
author = {Omote, Yushi and Shinagawa, Takahiro and Kato, Kazuhiko},
title = {Improving Agility and Elasticity in Bare-Metal Clouds},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694349},
doi = {10.1145/2694344.2694349},
abstract = {Bare-metal clouds are an emerging infrastructure-as-a-service (IaaS) that leases physical machines (bare-metal instances) rather than virtual machines, allowing resource-intensive applications to have exclusive access to physical hardware. Unfortunately, bare-metal instances require time-consuming or OS-specific tasks for deployment due to the lack of virtualization layers, thereby sacrificing several beneficial features of traditional IaaS clouds such as agility, elasticity, and OS transparency. We present BMcast, an OS deployment system with a special-purpose de-virtualizable virtual machine monitor (VMM) that supports quick and OS-transparent startup of bare-metal instances. BMcast performs streaming OS deployment while allowing direct access to physical hardware from the guest OS, and then disappears after completing the deployment. Quick startup of instances improves agility and elasticity significantly, and OS transparency greatly simplifies management tasks for cloud customers. Experimental results have confirmed that BMcast initiated a bare-metal instance 8.6 times faster than image copying, and database performance on BMcast during streaming OS deployment was comparable to that on a state-of-the-art VMM without performing deployment. BMcast incurred zero overhead after de-virtualization.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {145–159},
numpages = {15},
keywords = {device mediators, operating systems, virtualization, bare-metal clouds},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694384,
author = {Haque, Md E. and Eom, Yong hun and He, Yuxiong and Elnikety, Sameh and Bianchini, Ricardo and McKinley, Kathryn S.},
title = {Few-to-Many: Incremental Parallelism for Reducing Tail Latency in Interactive Services},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694384},
doi = {10.1145/2775054.2694384},
abstract = {Interactive services, such as Web search, recommendations, games, and finance, must respond quickly to satisfy customers. Achieving this goal requires optimizing tail (e.g., 99th+ percentile) latency. Although every server is multicore, parallelizing individual requests to reduce tail latency is challenging because (1) service demand is unknown when requests arrive; (2) blindly parallelizing all requests quickly oversubscribes hardware resources; and (3) parallelizing the numerous short requests will not improve tail latency. This paper introduces Few-to-Many (FM) incremental parallelization, which dynamically increases parallelism to reduce tail latency. FM uses request service demand profiles and hardware parallelism in an offline phase to compute a policy, represented as an interval table, which specifies when and how much software parallelism to add. At runtime, FM adds parallelism as specified by the interval table indexed by dynamic system load and request execution time progress. The longer a request executes, the more parallelism FM adds. We evaluate FM in Lucene, an open-source enterprise search engine, and in Bing, a commercial Web search engine. FM improves the 99th percentile response time up to 32% in Lucene and up to 26% in Bing, compared to prior state-of-the-art parallelization. Compared to running requests sequentially in Bing, FM improves tail latency by a factor of two. These results illustrate that incremental parallelism is a powerful tool for reducing tail latency.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {161–175},
numpages = {15},
keywords = {web search, thread scheduling, tail latency, multithreading, interactive services, dynamic parallelism}
}

@article{10.1145/2786763.2694384,
author = {Haque, Md E. and Eom, Yong hun and He, Yuxiong and Elnikety, Sameh and Bianchini, Ricardo and McKinley, Kathryn S.},
title = {Few-to-Many: Incremental Parallelism for Reducing Tail Latency in Interactive Services},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694384},
doi = {10.1145/2786763.2694384},
abstract = {Interactive services, such as Web search, recommendations, games, and finance, must respond quickly to satisfy customers. Achieving this goal requires optimizing tail (e.g., 99th+ percentile) latency. Although every server is multicore, parallelizing individual requests to reduce tail latency is challenging because (1) service demand is unknown when requests arrive; (2) blindly parallelizing all requests quickly oversubscribes hardware resources; and (3) parallelizing the numerous short requests will not improve tail latency. This paper introduces Few-to-Many (FM) incremental parallelization, which dynamically increases parallelism to reduce tail latency. FM uses request service demand profiles and hardware parallelism in an offline phase to compute a policy, represented as an interval table, which specifies when and how much software parallelism to add. At runtime, FM adds parallelism as specified by the interval table indexed by dynamic system load and request execution time progress. The longer a request executes, the more parallelism FM adds. We evaluate FM in Lucene, an open-source enterprise search engine, and in Bing, a commercial Web search engine. FM improves the 99th percentile response time up to 32% in Lucene and up to 26% in Bing, compared to prior state-of-the-art parallelization. Compared to running requests sequentially in Bing, FM improves tail latency by a factor of two. These results illustrate that incremental parallelism is a powerful tool for reducing tail latency.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {161–175},
numpages = {15},
keywords = {interactive services, multithreading, tail latency, thread scheduling, web search, dynamic parallelism}
}

@inproceedings{10.1145/2694344.2694384,
author = {Haque, Md E. and Eom, Yong hun and He, Yuxiong and Elnikety, Sameh and Bianchini, Ricardo and McKinley, Kathryn S.},
title = {Few-to-Many: Incremental Parallelism for Reducing Tail Latency in Interactive Services},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694384},
doi = {10.1145/2694344.2694384},
abstract = {Interactive services, such as Web search, recommendations, games, and finance, must respond quickly to satisfy customers. Achieving this goal requires optimizing tail (e.g., 99th+ percentile) latency. Although every server is multicore, parallelizing individual requests to reduce tail latency is challenging because (1) service demand is unknown when requests arrive; (2) blindly parallelizing all requests quickly oversubscribes hardware resources; and (3) parallelizing the numerous short requests will not improve tail latency. This paper introduces Few-to-Many (FM) incremental parallelization, which dynamically increases parallelism to reduce tail latency. FM uses request service demand profiles and hardware parallelism in an offline phase to compute a policy, represented as an interval table, which specifies when and how much software parallelism to add. At runtime, FM adds parallelism as specified by the interval table indexed by dynamic system load and request execution time progress. The longer a request executes, the more parallelism FM adds. We evaluate FM in Lucene, an open-source enterprise search engine, and in Bing, a commercial Web search engine. FM improves the 99th percentile response time up to 32% in Lucene and up to 26% in Bing, compared to prior state-of-the-art parallelization. Compared to running requests sequentially in Bing, FM improves tail latency by a factor of two. These results illustrate that incremental parallelism is a powerful tool for reducing tail latency.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {161–175},
numpages = {15},
keywords = {web search, interactive services, dynamic parallelism, tail latency, multithreading, thread scheduling},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251030,
author = {Tsafrir, Dan},
title = {Session Details: Session 3A: Memory and Security II},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251030},
doi = {10.1145/3251030},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694380,
author = {Colp, Patrick and Zhang, Jiawen and Gleeson, James and Suneja, Sahil and de Lara, Eyal and Raj, Himanshu and Saroiu, Stefan and Wolman, Alec},
title = {Protecting Data on Smartphones and Tablets from Memory Attacks},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694380},
doi = {10.1145/2775054.2694380},
abstract = {Smartphones and tablets are easily lost or stolen. This makes them susceptible to an inexpensive class of memory attacks, such as cold-boot attacks, using a bus monitor to observe the memory bus, and DMA attacks. This paper describes Sentry, a system that allows applications and OS components to store their code and data on the System-on-Chip (SoC) rather than in DRAM. We use ARM-specific mechanisms originally designed for embedded systems, but still present in today's mobile devices, to protect applications and OS subsystems from memory attacks.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {177–189},
numpages = {13},
keywords = {DMA attack, nexus, cache, arm, cold boot, bus monitoring, tegra, encrypted RAM, encrypted memory, AES, android, iRAM}
}

@article{10.1145/2786763.2694380,
author = {Colp, Patrick and Zhang, Jiawen and Gleeson, James and Suneja, Sahil and de Lara, Eyal and Raj, Himanshu and Saroiu, Stefan and Wolman, Alec},
title = {Protecting Data on Smartphones and Tablets from Memory Attacks},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694380},
doi = {10.1145/2786763.2694380},
abstract = {Smartphones and tablets are easily lost or stolen. This makes them susceptible to an inexpensive class of memory attacks, such as cold-boot attacks, using a bus monitor to observe the memory bus, and DMA attacks. This paper describes Sentry, a system that allows applications and OS components to store their code and data on the System-on-Chip (SoC) rather than in DRAM. We use ARM-specific mechanisms originally designed for embedded systems, but still present in today's mobile devices, to protect applications and OS subsystems from memory attacks.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {177–189},
numpages = {13},
keywords = {tegra, cold boot, encrypted memory, arm, AES, nexus, encrypted RAM, cache, android, iRAM, bus monitoring, DMA attack}
}

@inproceedings{10.1145/2694344.2694380,
author = {Colp, Patrick and Zhang, Jiawen and Gleeson, James and Suneja, Sahil and de Lara, Eyal and Raj, Himanshu and Saroiu, Stefan and Wolman, Alec},
title = {Protecting Data on Smartphones and Tablets from Memory Attacks},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694380},
doi = {10.1145/2694344.2694380},
abstract = {Smartphones and tablets are easily lost or stolen. This makes them susceptible to an inexpensive class of memory attacks, such as cold-boot attacks, using a bus monitor to observe the memory bus, and DMA attacks. This paper describes Sentry, a system that allows applications and OS components to store their code and data on the System-on-Chip (SoC) rather than in DRAM. We use ARM-specific mechanisms originally designed for embedded systems, but still present in today's mobile devices, to protect applications and OS subsystems from memory attacks.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {177–189},
numpages = {13},
keywords = {android, iRAM, encrypted RAM, DMA attack, encrypted memory, cold boot, AES, arm, cache, bus monitoring, tegra, nexus},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694386,
author = {Dautenhahn, Nathan and Kasampalis, Theodoros and Dietz, Will and Criswell, John and Adve, Vikram},
title = {Nested Kernel: An Operating System Architecture for Intra-Kernel Privilege Separation},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694386},
doi = {10.1145/2775054.2694386},
abstract = {Monolithic operating system designs undermine the security of computing systems by allowing single exploits anywhere in the kernel to enjoy full supervisor privilege. The nested kernel operating system architecture addresses this problem by "nesting" a small isolated kernel within a traditional monolithic kernel. The "nested kernel" interposes on all updates to virtual memory translations to assert protections on physical memory, thus significantly reducing the trusted computing base for memory access control enforcement. We incorporated the nested kernel architecture into FreeBSD on x86-64 hardware while allowing the entire operating system, including untrusted components, to operate at the highest hardware privilege level by write-protecting MMU translations and de-privileging the untrusted part of the kernel. Our implementation inherently enforces kernel code integrity while still allowing dynamically loaded kernel modules, thus defending against code injection attacks. We also demonstrate that the nested kernel architecture allows kernel developers to isolate memory in ways not possible in monolithic kernels by introducing write-mediation and write-logging services to protect critical system data structures. Performance of the nested kernel prototype shows modest overheads: &lt;1% average for Apache and 2.7% for kernel compile. Overall, our results and experience show that the nested kernel design can be retrofitted to existing monolithic kernels, providing important security benefits.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {191–206},
numpages = {16},
keywords = {intra-kernel isolation, virtual memory, operating system architecture, malicious operating systems}
}

@article{10.1145/2786763.2694386,
author = {Dautenhahn, Nathan and Kasampalis, Theodoros and Dietz, Will and Criswell, John and Adve, Vikram},
title = {Nested Kernel: An Operating System Architecture for Intra-Kernel Privilege Separation},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694386},
doi = {10.1145/2786763.2694386},
abstract = {Monolithic operating system designs undermine the security of computing systems by allowing single exploits anywhere in the kernel to enjoy full supervisor privilege. The nested kernel operating system architecture addresses this problem by "nesting" a small isolated kernel within a traditional monolithic kernel. The "nested kernel" interposes on all updates to virtual memory translations to assert protections on physical memory, thus significantly reducing the trusted computing base for memory access control enforcement. We incorporated the nested kernel architecture into FreeBSD on x86-64 hardware while allowing the entire operating system, including untrusted components, to operate at the highest hardware privilege level by write-protecting MMU translations and de-privileging the untrusted part of the kernel. Our implementation inherently enforces kernel code integrity while still allowing dynamically loaded kernel modules, thus defending against code injection attacks. We also demonstrate that the nested kernel architecture allows kernel developers to isolate memory in ways not possible in monolithic kernels by introducing write-mediation and write-logging services to protect critical system data structures. Performance of the nested kernel prototype shows modest overheads: &lt;1% average for Apache and 2.7% for kernel compile. Overall, our results and experience show that the nested kernel design can be retrofitted to existing monolithic kernels, providing important security benefits.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {191–206},
numpages = {16},
keywords = {malicious operating systems, intra-kernel isolation, operating system architecture, virtual memory}
}

@inproceedings{10.1145/2694344.2694386,
author = {Dautenhahn, Nathan and Kasampalis, Theodoros and Dietz, Will and Criswell, John and Adve, Vikram},
title = {Nested Kernel: An Operating System Architecture for Intra-Kernel Privilege Separation},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694386},
doi = {10.1145/2694344.2694386},
abstract = {Monolithic operating system designs undermine the security of computing systems by allowing single exploits anywhere in the kernel to enjoy full supervisor privilege. The nested kernel operating system architecture addresses this problem by "nesting" a small isolated kernel within a traditional monolithic kernel. The "nested kernel" interposes on all updates to virtual memory translations to assert protections on physical memory, thus significantly reducing the trusted computing base for memory access control enforcement. We incorporated the nested kernel architecture into FreeBSD on x86-64 hardware while allowing the entire operating system, including untrusted components, to operate at the highest hardware privilege level by write-protecting MMU translations and de-privileging the untrusted part of the kernel. Our implementation inherently enforces kernel code integrity while still allowing dynamically loaded kernel modules, thus defending against code injection attacks. We also demonstrate that the nested kernel architecture allows kernel developers to isolate memory in ways not possible in monolithic kernels by introducing write-mediation and write-logging services to protect critical system data structures. Performance of the nested kernel prototype shows modest overheads: &lt;1% average for Apache and 2.7% for kernel compile. Overall, our results and experience show that the nested kernel design can be retrofitted to existing monolithic kernels, providing important security benefits.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {191–206},
numpages = {16},
keywords = {operating system architecture, intra-kernel isolation, virtual memory, malicious operating systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251031,
author = {Chen, Yunji},
title = {Session Details: Session 3B: Warehouse Scale Computing II},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251031},
doi = {10.1145/3251031},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694362,
author = {Tan, Zhangxi and Qian, Zhenghao and Chen, Xi and Asanovic, Krste and Patterson, David},
title = {DIABLO: A Warehouse-Scale Computer Network Simulator Using FPGAs},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694362},
doi = {10.1145/2775054.2694362},
abstract = {Motivated by rapid software and hardware innovation in warehouse-scale computing (WSC), we visit the problem of warehouse-scale network design evaluation. A WSC is composed of about 30 arrays or clusters, each of which contains about 3000 servers, leading to a total of about 100,000 servers per WSC. We found many prior experiments have been conducted on relatively small physical testbeds, and they often assume the workload is static and that computations are only loosely coupled with the adaptive networking stack. We present a novel and cost-efficient FPGAbased evaluation methodology, called Datacenter-In-A-Box at LOw cost (DIABLO), which treats arrays as whole computers with tightly integrated hardware and software. We have built a 3,000-node prototype running the full WSC software stack. Using our prototype, we have successfully reproduced a few WSC phenomena, such as TCP Incast and memcached request latency long tail, and found that results do indeed change with both scale and with version of the full software stack.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {207–221},
numpages = {15},
keywords = {FPGA, warehouse-scale computing, performance, evaluation}
}

@article{10.1145/2786763.2694362,
author = {Tan, Zhangxi and Qian, Zhenghao and Chen, Xi and Asanovic, Krste and Patterson, David},
title = {DIABLO: A Warehouse-Scale Computer Network Simulator Using FPGAs},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694362},
doi = {10.1145/2786763.2694362},
abstract = {Motivated by rapid software and hardware innovation in warehouse-scale computing (WSC), we visit the problem of warehouse-scale network design evaluation. A WSC is composed of about 30 arrays or clusters, each of which contains about 3000 servers, leading to a total of about 100,000 servers per WSC. We found many prior experiments have been conducted on relatively small physical testbeds, and they often assume the workload is static and that computations are only loosely coupled with the adaptive networking stack. We present a novel and cost-efficient FPGAbased evaluation methodology, called Datacenter-In-A-Box at LOw cost (DIABLO), which treats arrays as whole computers with tightly integrated hardware and software. We have built a 3,000-node prototype running the full WSC software stack. Using our prototype, we have successfully reproduced a few WSC phenomena, such as TCP Incast and memcached request latency long tail, and found that results do indeed change with both scale and with version of the full software stack.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {207–221},
numpages = {15},
keywords = {FPGA, performance, warehouse-scale computing, evaluation}
}

@inproceedings{10.1145/2694344.2694362,
author = {Tan, Zhangxi and Qian, Zhenghao and Chen, Xi and Asanovic, Krste and Patterson, David},
title = {DIABLO: A Warehouse-Scale Computer Network Simulator Using FPGAs},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694362},
doi = {10.1145/2694344.2694362},
abstract = {Motivated by rapid software and hardware innovation in warehouse-scale computing (WSC), we visit the problem of warehouse-scale network design evaluation. A WSC is composed of about 30 arrays or clusters, each of which contains about 3000 servers, leading to a total of about 100,000 servers per WSC. We found many prior experiments have been conducted on relatively small physical testbeds, and they often assume the workload is static and that computations are only loosely coupled with the adaptive networking stack. We present a novel and cost-efficient FPGAbased evaluation methodology, called Datacenter-In-A-Box at LOw cost (DIABLO), which treats arrays as whole computers with tightly integrated hardware and software. We have built a 3,000-node prototype running the full WSC software stack. Using our prototype, we have successfully reproduced a few WSC phenomena, such as TCP Incast and memcached request latency long tail, and found that results do indeed change with both scale and with version of the full software stack.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {207–221},
numpages = {15},
keywords = {evaluation, FPGA, performance, warehouse-scale computing},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694347,
author = {Hauswald, Johann and Laurenzano, Michael A. and Zhang, Yunqi and Li, Cheng and Rovinski, Austin and Khurana, Arjun and Dreslinski, Ronald G. and Mudge, Trevor and Petrucci, Vinicius and Tang, Lingjia and Mars, Jason},
title = {Sirius: An Open End-to-End Voice and Vision Personal Assistant and Its Implications for Future Warehouse Scale Computers},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694347},
doi = {10.1145/2775054.2694347},
abstract = {As user demand scales for intelligent personal assistants (IPAs) such as Apple's Siri, Google's Google Now, and Microsoft's Cortana, we are approaching the computational limits of current datacenter architectures. It is an open question how future server architectures should evolve to enable this emerging class of applications, and the lack of an open-source IPA workload is an obstacle in addressing this question. In this paper, we present the design of Sirius, an open end-to-end IPA web-service application that accepts queries in the form of voice and images, and responds with natural language. We then use this workload to investigate the implications of four points in the design space of future accelerator-based server architectures spanning traditional CPUs, GPUs, manycore throughput co-processors, and FPGAs.To investigate future server designs for Sirius, we decompose Sirius into a suite of 7 benchmarks (Sirius Suite) comprising the computationally intensive bottlenecks of Sirius. We port Sirius Suite to a spectrum of accelerator platforms and use the performance and power trade-offs across these platforms to perform a total cost of ownership (TCO) analysis of various server design points. In our study, we find that accelerators are critical for the future scalability of IPA services. Our results show that GPU- and FPGA-accelerated servers improve the query latency on average by 10x and 16x. For a given throughput, GPU- and FPGA-accelerated servers can reduce the TCO of datacenters by 2.6x and 1.4x, respectively.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {223–238},
numpages = {16},
keywords = {datacenters, warehouse scale computers, intelligent personal assistants, emerging workloads}
}

@article{10.1145/2786763.2694347,
author = {Hauswald, Johann and Laurenzano, Michael A. and Zhang, Yunqi and Li, Cheng and Rovinski, Austin and Khurana, Arjun and Dreslinski, Ronald G. and Mudge, Trevor and Petrucci, Vinicius and Tang, Lingjia and Mars, Jason},
title = {Sirius: An Open End-to-End Voice and Vision Personal Assistant and Its Implications for Future Warehouse Scale Computers},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694347},
doi = {10.1145/2786763.2694347},
abstract = {As user demand scales for intelligent personal assistants (IPAs) such as Apple's Siri, Google's Google Now, and Microsoft's Cortana, we are approaching the computational limits of current datacenter architectures. It is an open question how future server architectures should evolve to enable this emerging class of applications, and the lack of an open-source IPA workload is an obstacle in addressing this question. In this paper, we present the design of Sirius, an open end-to-end IPA web-service application that accepts queries in the form of voice and images, and responds with natural language. We then use this workload to investigate the implications of four points in the design space of future accelerator-based server architectures spanning traditional CPUs, GPUs, manycore throughput co-processors, and FPGAs.To investigate future server designs for Sirius, we decompose Sirius into a suite of 7 benchmarks (Sirius Suite) comprising the computationally intensive bottlenecks of Sirius. We port Sirius Suite to a spectrum of accelerator platforms and use the performance and power trade-offs across these platforms to perform a total cost of ownership (TCO) analysis of various server design points. In our study, we find that accelerators are critical for the future scalability of IPA services. Our results show that GPU- and FPGA-accelerated servers improve the query latency on average by 10x and 16x. For a given throughput, GPU- and FPGA-accelerated servers can reduce the TCO of datacenters by 2.6x and 1.4x, respectively.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {223–238},
numpages = {16},
keywords = {datacenters, warehouse scale computers, emerging workloads, intelligent personal assistants}
}

@inproceedings{10.1145/2694344.2694347,
author = {Hauswald, Johann and Laurenzano, Michael A. and Zhang, Yunqi and Li, Cheng and Rovinski, Austin and Khurana, Arjun and Dreslinski, Ronald G. and Mudge, Trevor and Petrucci, Vinicius and Tang, Lingjia and Mars, Jason},
title = {Sirius: An Open End-to-End Voice and Vision Personal Assistant and Its Implications for Future Warehouse Scale Computers},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694347},
doi = {10.1145/2694344.2694347},
abstract = {As user demand scales for intelligent personal assistants (IPAs) such as Apple's Siri, Google's Google Now, and Microsoft's Cortana, we are approaching the computational limits of current datacenter architectures. It is an open question how future server architectures should evolve to enable this emerging class of applications, and the lack of an open-source IPA workload is an obstacle in addressing this question. In this paper, we present the design of Sirius, an open end-to-end IPA web-service application that accepts queries in the form of voice and images, and responds with natural language. We then use this workload to investigate the implications of four points in the design space of future accelerator-based server architectures spanning traditional CPUs, GPUs, manycore throughput co-processors, and FPGAs.To investigate future server designs for Sirius, we decompose Sirius into a suite of 7 benchmarks (Sirius Suite) comprising the computationally intensive bottlenecks of Sirius. We port Sirius Suite to a spectrum of accelerator platforms and use the performance and power trade-offs across these platforms to perform a total cost of ownership (TCO) analysis of various server design points. In our study, we find that accelerators are critical for the future scalability of IPA services. Our results show that GPU- and FPGA-accelerated servers improve the query latency on average by 10x and 16x. For a given throughput, GPU- and FPGA-accelerated servers can reduce the TCO of datacenters by 2.6x and 1.4x, respectively.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {223–238},
numpages = {16},
keywords = {emerging workloads, intelligent personal assistants, datacenters, warehouse scale computers},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251032,
author = {Shriraman, Arrvindh},
title = {Session Details: Session 4A: Energy},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251032},
doi = {10.1145/3251032},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694360,
author = {Xu, Chao and Lin, Felix Xiaozhu and Wang, Yuyang and Zhong, Lin},
title = {Automated OS-Level Device Runtime Power Management},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694360},
doi = {10.1145/2775054.2694360},
abstract = {Non-CPU devices on a modern system-on-a-chip (SoC), ranging from accelerators to I/O controllers, account for a significant portion of the chip area. It is therefore vital for system energy efficiency that idle devices can enter a low-power state while still meeting the performance expectation. This is called device runtime Power Management (PM) for which individual device drivers in commodity OSes are held responsible today. Based on the observations of existing drivers and their evolution, we consider it harmful to rely on drivers for device runtime PM. This paper identifies three pieces of information as essential to device runtime PM, and shows that they can be obtained without involving drivers, either by using a software-only approach, or more efficiently, by adding one register bit to each device. We thus suggest a structural change to the current Linux runtime PM framework, replacing the PM code in all applicable drivers with a single kernel module called the central PM agent. Experimental evaluations show that the central PM agent is just as effective as hand-tuned driver PM code. The paper also presents a tool called PowerAdvisor that simplifies driver PM efforts under the current Linux runtime PM framework. PowerAdvisor analyzes execution traces and suggests where to insert PM calls in driver source code. Despite being a best-effort tool, PowerAdvisor not only reproduces hand-tuned PM code from stock drivers, but also correctly suggests PM code never known before. Overall, our experience shows that it is promising to ultimately free driver developers from manual PM.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {239–252},
numpages = {14},
keywords = {power management, system-on-a-chip, mobile system, operating system}
}

@article{10.1145/2786763.2694360,
author = {Xu, Chao and Lin, Felix Xiaozhu and Wang, Yuyang and Zhong, Lin},
title = {Automated OS-Level Device Runtime Power Management},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694360},
doi = {10.1145/2786763.2694360},
abstract = {Non-CPU devices on a modern system-on-a-chip (SoC), ranging from accelerators to I/O controllers, account for a significant portion of the chip area. It is therefore vital for system energy efficiency that idle devices can enter a low-power state while still meeting the performance expectation. This is called device runtime Power Management (PM) for which individual device drivers in commodity OSes are held responsible today. Based on the observations of existing drivers and their evolution, we consider it harmful to rely on drivers for device runtime PM. This paper identifies three pieces of information as essential to device runtime PM, and shows that they can be obtained without involving drivers, either by using a software-only approach, or more efficiently, by adding one register bit to each device. We thus suggest a structural change to the current Linux runtime PM framework, replacing the PM code in all applicable drivers with a single kernel module called the central PM agent. Experimental evaluations show that the central PM agent is just as effective as hand-tuned driver PM code. The paper also presents a tool called PowerAdvisor that simplifies driver PM efforts under the current Linux runtime PM framework. PowerAdvisor analyzes execution traces and suggests where to insert PM calls in driver source code. Despite being a best-effort tool, PowerAdvisor not only reproduces hand-tuned PM code from stock drivers, but also correctly suggests PM code never known before. Overall, our experience shows that it is promising to ultimately free driver developers from manual PM.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {239–252},
numpages = {14},
keywords = {mobile system, system-on-a-chip, power management, operating system}
}

@inproceedings{10.1145/2694344.2694360,
author = {Xu, Chao and Lin, Felix Xiaozhu and Wang, Yuyang and Zhong, Lin},
title = {Automated OS-Level Device Runtime Power Management},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694360},
doi = {10.1145/2694344.2694360},
abstract = {Non-CPU devices on a modern system-on-a-chip (SoC), ranging from accelerators to I/O controllers, account for a significant portion of the chip area. It is therefore vital for system energy efficiency that idle devices can enter a low-power state while still meeting the performance expectation. This is called device runtime Power Management (PM) for which individual device drivers in commodity OSes are held responsible today. Based on the observations of existing drivers and their evolution, we consider it harmful to rely on drivers for device runtime PM. This paper identifies three pieces of information as essential to device runtime PM, and shows that they can be obtained without involving drivers, either by using a software-only approach, or more efficiently, by adding one register bit to each device. We thus suggest a structural change to the current Linux runtime PM framework, replacing the PM code in all applicable drivers with a single kernel module called the central PM agent. Experimental evaluations show that the central PM agent is just as effective as hand-tuned driver PM code. The paper also presents a tool called PowerAdvisor that simplifies driver PM efforts under the current Linux runtime PM framework. PowerAdvisor analyzes execution traces and suggests where to insert PM calls in driver source code. Despite being a best-effort tool, PowerAdvisor not only reproduces hand-tuned PM code from stock drivers, but also correctly suggests PM code never known before. Overall, our experience shows that it is promising to ultimately free driver developers from manual PM.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {239–252},
numpages = {14},
keywords = {system-on-a-chip, power management, operating system, mobile system},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694378,
author = {Goiri, \'{I}\~{n}igo and Nguyen, Thu D. and Bianchini, Ricardo},
title = {CoolAir: Temperature- and Variation-Aware Management for Free-Cooled Datacenters},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694378},
doi = {10.1145/2775054.2694378},
abstract = {Despite its benefits, free cooling may expose servers to high absolute temperatures, wide temperature variations, and high humidity when datacenters are sited at certain locations. Prior research (in non-free-cooled datacenters) has shown that high temperatures and/or wide temporal temperature variations can harm hardware reliability. In this paper, we identify the runtime management strategies required to limit absolute temperatures, temperature variations, humidity, and cooling energy in free-cooled datacenters. As the basis for our study, we propose CoolAir, a system that embodies these strategies. Using CoolAir and a real free-cooled datacenter prototype, we show that effective management requires cooling infrastructures that can act smoothly. In addition, we show that CoolAir can tightly manage temperature and significantly reduce temperature variation, often at a lower cooling cost than existing free-cooled datacenters. Perhaps most importantly, based on our results, we derive several principles and lessons that should guide the design of management systems for free-cooled datacenters of any size.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {253–265},
numpages = {13},
keywords = {datacenters, free cooling, energy management, thermal management}
}

@article{10.1145/2786763.2694378,
author = {Goiri, \'{I}\~{n}igo and Nguyen, Thu D. and Bianchini, Ricardo},
title = {CoolAir: Temperature- and Variation-Aware Management for Free-Cooled Datacenters},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694378},
doi = {10.1145/2786763.2694378},
abstract = {Despite its benefits, free cooling may expose servers to high absolute temperatures, wide temperature variations, and high humidity when datacenters are sited at certain locations. Prior research (in non-free-cooled datacenters) has shown that high temperatures and/or wide temporal temperature variations can harm hardware reliability. In this paper, we identify the runtime management strategies required to limit absolute temperatures, temperature variations, humidity, and cooling energy in free-cooled datacenters. As the basis for our study, we propose CoolAir, a system that embodies these strategies. Using CoolAir and a real free-cooled datacenter prototype, we show that effective management requires cooling infrastructures that can act smoothly. In addition, we show that CoolAir can tightly manage temperature and significantly reduce temperature variation, often at a lower cooling cost than existing free-cooled datacenters. Perhaps most importantly, based on our results, we derive several principles and lessons that should guide the design of management systems for free-cooled datacenters of any size.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {253–265},
numpages = {13},
keywords = {free cooling, thermal management, energy management, datacenters}
}

@inproceedings{10.1145/2694344.2694378,
author = {Goiri, \'{I}\~{n}igo and Nguyen, Thu D. and Bianchini, Ricardo},
title = {CoolAir: Temperature- and Variation-Aware Management for Free-Cooled Datacenters},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694378},
doi = {10.1145/2694344.2694378},
abstract = {Despite its benefits, free cooling may expose servers to high absolute temperatures, wide temperature variations, and high humidity when datacenters are sited at certain locations. Prior research (in non-free-cooled datacenters) has shown that high temperatures and/or wide temporal temperature variations can harm hardware reliability. In this paper, we identify the runtime management strategies required to limit absolute temperatures, temperature variations, humidity, and cooling energy in free-cooled datacenters. As the basis for our study, we propose CoolAir, a system that embodies these strategies. Using CoolAir and a real free-cooled datacenter prototype, we show that effective management requires cooling infrastructures that can act smoothly. In addition, we show that CoolAir can tightly manage temperature and significantly reduce temperature variation, often at a lower cooling cost than existing free-cooled datacenters. Perhaps most importantly, based on our results, we derive several principles and lessons that should guide the design of management systems for free-cooled datacenters of any size.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {253–265},
numpages = {13},
keywords = {datacenters, energy management, free cooling, thermal management},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694373,
author = {Mishra, Nikita and Zhang, Huazhe and Lafferty, John D. and Hoffmann, Henry},
title = {A Probabilistic Graphical Model-Based Approach for Minimizing Energy Under Performance Constraints},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694373},
doi = {10.1145/2775054.2694373},
abstract = {In many deployments, computer systems are underutilized -- meaning that applications have performance requirements that demand less than full system capacity. Ideally, we would take advantage of this under-utilization by allocating system resources so that the performance requirements are met and energy is minimized. This optimization problem is complicated by the fact that the performance and power consumption of various system configurations are often application -- or even input -- dependent. Thus, practically, minimizing energy for a performance constraint requires fast, accurate estimations of application-dependent performance and power tradeoffs. This paper investigates machine learning techniques that enable energy savings by learning Pareto-optimal power and performance tradeoffs. Specifically, we propose LEO, a probabilistic graphical model-based learning system that provides accurate online estimates of an application's power and performance as a function of system configuration. We compare LEO to (1) offline learning, (2) online learning, (3) a heuristic approach, and (4) the true optimal solution. We find that LEO produces the most accurate estimates and near optimal energy savings.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {267–281},
numpages = {15},
keywords = {probabilistic graphical models}
}

@article{10.1145/2786763.2694373,
author = {Mishra, Nikita and Zhang, Huazhe and Lafferty, John D. and Hoffmann, Henry},
title = {A Probabilistic Graphical Model-Based Approach for Minimizing Energy Under Performance Constraints},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694373},
doi = {10.1145/2786763.2694373},
abstract = {In many deployments, computer systems are underutilized -- meaning that applications have performance requirements that demand less than full system capacity. Ideally, we would take advantage of this under-utilization by allocating system resources so that the performance requirements are met and energy is minimized. This optimization problem is complicated by the fact that the performance and power consumption of various system configurations are often application -- or even input -- dependent. Thus, practically, minimizing energy for a performance constraint requires fast, accurate estimations of application-dependent performance and power tradeoffs. This paper investigates machine learning techniques that enable energy savings by learning Pareto-optimal power and performance tradeoffs. Specifically, we propose LEO, a probabilistic graphical model-based learning system that provides accurate online estimates of an application's power and performance as a function of system configuration. We compare LEO to (1) offline learning, (2) online learning, (3) a heuristic approach, and (4) the true optimal solution. We find that LEO produces the most accurate estimates and near optimal energy savings.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {267–281},
numpages = {15},
keywords = {probabilistic graphical models}
}

@inproceedings{10.1145/2694344.2694373,
author = {Mishra, Nikita and Zhang, Huazhe and Lafferty, John D. and Hoffmann, Henry},
title = {A Probabilistic Graphical Model-Based Approach for Minimizing Energy Under Performance Constraints},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694373},
doi = {10.1145/2694344.2694373},
abstract = {In many deployments, computer systems are underutilized -- meaning that applications have performance requirements that demand less than full system capacity. Ideally, we would take advantage of this under-utilization by allocating system resources so that the performance requirements are met and energy is minimized. This optimization problem is complicated by the fact that the performance and power consumption of various system configurations are often application -- or even input -- dependent. Thus, practically, minimizing energy for a performance constraint requires fast, accurate estimations of application-dependent performance and power tradeoffs. This paper investigates machine learning techniques that enable energy savings by learning Pareto-optimal power and performance tradeoffs. Specifically, we propose LEO, a probabilistic graphical model-based learning system that provides accurate online estimates of an application's power and performance as a function of system configuration. We compare LEO to (1) offline learning, (2) online learning, (3) a heuristic approach, and (4) the true optimal solution. We find that LEO produces the most accurate estimates and near optimal energy savings.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {267–281},
numpages = {15},
keywords = {probabilistic graphical models},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694377,
author = {Pang, Jun and Dwyer, Chris and Lebeck, Alvin R.},
title = {More is Less, Less is More: Molecular-Scale Photonic NoC Power Topologies},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694377},
doi = {10.1145/2775054.2694377},
abstract = {Molecular-scale Network-on-Chip (mNoC) crossbars use quantum dot LEDs as an on-chip light source, and chromophores to provide optical signal filtering for receivers. An mNoC reduces power consumption or enables scaling to larger crossbars for a reduced energy budget compared to current nanophotonic NoC crossbars. Since communication latency is reduced by using a high-radix crossbar, minimizing power consumption becomes a primary design target. Conventional Single Writer Multiple Reader (SWMR) photonic crossbar designs broadcast all packets, and incur the commensurate required power, even if only two nodes are communicating.This paper introduces power topologies, enabled by unique capabilities of mNoC technology, to reduce overall interconnect power consumption. A power topology corresponds to the logical connectivity provided by a given power mode. Broadcast is one power mode and it consumes the maximum power. Additional power modes consume less power but allow a source to communicate with only a statically defined, potentially non-contiguous, subset of nodes. Overall interconnect power is reduced if the more frequently communicating nodes use modes that consume less power, while less frequently communicating nodes use modes that consume more power. We also investigate thread mapping techniques to fully exploit power topologies. We explore various mNoC power topologies with one, two and four power modes for a radix-256 SWMR mNoC crossbar. Our results show that the combination of power topologies and intelligent thread mapping can reduce total mNoC power by up to 51% on average for a set of 12 SPLASH benchmarks. Furthermore performance is 10% better than conventional resonator-based photonic NoCs and energy is reduced by 72%.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {283–296},
numpages = {14},
keywords = {interconnection network, energy efficiency}
}

@article{10.1145/2786763.2694377,
author = {Pang, Jun and Dwyer, Chris and Lebeck, Alvin R.},
title = {More is Less, Less is More: Molecular-Scale Photonic NoC Power Topologies},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694377},
doi = {10.1145/2786763.2694377},
abstract = {Molecular-scale Network-on-Chip (mNoC) crossbars use quantum dot LEDs as an on-chip light source, and chromophores to provide optical signal filtering for receivers. An mNoC reduces power consumption or enables scaling to larger crossbars for a reduced energy budget compared to current nanophotonic NoC crossbars. Since communication latency is reduced by using a high-radix crossbar, minimizing power consumption becomes a primary design target. Conventional Single Writer Multiple Reader (SWMR) photonic crossbar designs broadcast all packets, and incur the commensurate required power, even if only two nodes are communicating.This paper introduces power topologies, enabled by unique capabilities of mNoC technology, to reduce overall interconnect power consumption. A power topology corresponds to the logical connectivity provided by a given power mode. Broadcast is one power mode and it consumes the maximum power. Additional power modes consume less power but allow a source to communicate with only a statically defined, potentially non-contiguous, subset of nodes. Overall interconnect power is reduced if the more frequently communicating nodes use modes that consume less power, while less frequently communicating nodes use modes that consume more power. We also investigate thread mapping techniques to fully exploit power topologies. We explore various mNoC power topologies with one, two and four power modes for a radix-256 SWMR mNoC crossbar. Our results show that the combination of power topologies and intelligent thread mapping can reduce total mNoC power by up to 51% on average for a set of 12 SPLASH benchmarks. Furthermore performance is 10% better than conventional resonator-based photonic NoCs and energy is reduced by 72%.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {283–296},
numpages = {14},
keywords = {energy efficiency, interconnection network}
}

@inproceedings{10.1145/2694344.2694377,
author = {Pang, Jun and Dwyer, Chris and Lebeck, Alvin R.},
title = {More is Less, Less is More: Molecular-Scale Photonic NoC Power Topologies},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694377},
doi = {10.1145/2694344.2694377},
abstract = {Molecular-scale Network-on-Chip (mNoC) crossbars use quantum dot LEDs as an on-chip light source, and chromophores to provide optical signal filtering for receivers. An mNoC reduces power consumption or enables scaling to larger crossbars for a reduced energy budget compared to current nanophotonic NoC crossbars. Since communication latency is reduced by using a high-radix crossbar, minimizing power consumption becomes a primary design target. Conventional Single Writer Multiple Reader (SWMR) photonic crossbar designs broadcast all packets, and incur the commensurate required power, even if only two nodes are communicating.This paper introduces power topologies, enabled by unique capabilities of mNoC technology, to reduce overall interconnect power consumption. A power topology corresponds to the logical connectivity provided by a given power mode. Broadcast is one power mode and it consumes the maximum power. Additional power modes consume less power but allow a source to communicate with only a statically defined, potentially non-contiguous, subset of nodes. Overall interconnect power is reduced if the more frequently communicating nodes use modes that consume less power, while less frequently communicating nodes use modes that consume more power. We also investigate thread mapping techniques to fully exploit power topologies. We explore various mNoC power topologies with one, two and four power modes for a radix-256 SWMR mNoC crossbar. Our results show that the combination of power topologies and intelligent thread mapping can reduce total mNoC power by up to 51% on average for a set of 12 SPLASH benchmarks. Furthermore performance is 10% better than conventional resonator-based photonic NoCs and energy is reduced by 72%.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {283–296},
numpages = {14},
keywords = {energy efficiency, interconnection network},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251033,
author = {Berger, Emery},
title = {Session Details: Session 4B: Reliability},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251033},
doi = {10.1145/3251033},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694348,
author = {Sridharan, Vilas and DeBardeleben, Nathan and Blanchard, Sean and Ferreira, Kurt B. and Stearley, Jon and Shalf, John and Gurumurthi, Sudhanva},
title = {Memory Errors in Modern Systems: The Good, The Bad, and The Ugly},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694348},
doi = {10.1145/2775054.2694348},
abstract = {Several recent publications have shown that hardware faults in the memory subsystem are commonplace. These faults are predicted to become more frequent in future systems that contain orders of magnitude more DRAM and SRAM than found in current memory subsystems. These memory subsystems will need to provide resilience techniques to tolerate these faults when deployed in high-performance computing systems and data centers containing tens of thousands of nodes. Therefore, it is critical to understand the efficacy of current hardware resilience techniques to determine whether they will be suitable for future systems. In this paper, we present a study of DRAM and SRAM faults and errors from the field. We use data from two leadership-class high-performance computer systems to analyze the reliability impact of hardware resilience schemes that are deployed in current systems. Our study has several key findings about the efficacy of many currently deployed reliability techniques such as DRAM ECC, DDR address/command parity, and SRAM ECC and parity. We also perform a methodological study, and find that counting errors instead of faults, a common practice among researchers and data center operators, can lead to incorrect conclusions about system reliability. Finally, we use our data to project the needs of future large-scale systems. We find that SRAM faults are unlikely to pose a significantly larger reliability threat in the future, while DRAM faults will be a major concern and stronger DRAM resilience schemes will be needed to maintain acceptable failure rates similar to those found on today's systems.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {297–310},
numpages = {14},
keywords = {reliability, large-scale systems, field studies}
}

@article{10.1145/2786763.2694348,
author = {Sridharan, Vilas and DeBardeleben, Nathan and Blanchard, Sean and Ferreira, Kurt B. and Stearley, Jon and Shalf, John and Gurumurthi, Sudhanva},
title = {Memory Errors in Modern Systems: The Good, The Bad, and The Ugly},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694348},
doi = {10.1145/2786763.2694348},
abstract = {Several recent publications have shown that hardware faults in the memory subsystem are commonplace. These faults are predicted to become more frequent in future systems that contain orders of magnitude more DRAM and SRAM than found in current memory subsystems. These memory subsystems will need to provide resilience techniques to tolerate these faults when deployed in high-performance computing systems and data centers containing tens of thousands of nodes. Therefore, it is critical to understand the efficacy of current hardware resilience techniques to determine whether they will be suitable for future systems. In this paper, we present a study of DRAM and SRAM faults and errors from the field. We use data from two leadership-class high-performance computer systems to analyze the reliability impact of hardware resilience schemes that are deployed in current systems. Our study has several key findings about the efficacy of many currently deployed reliability techniques such as DRAM ECC, DDR address/command parity, and SRAM ECC and parity. We also perform a methodological study, and find that counting errors instead of faults, a common practice among researchers and data center operators, can lead to incorrect conclusions about system reliability. Finally, we use our data to project the needs of future large-scale systems. We find that SRAM faults are unlikely to pose a significantly larger reliability threat in the future, while DRAM faults will be a major concern and stronger DRAM resilience schemes will be needed to maintain acceptable failure rates similar to those found on today's systems.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {297–310},
numpages = {14},
keywords = {field studies, large-scale systems, reliability}
}

@inproceedings{10.1145/2694344.2694348,
author = {Sridharan, Vilas and DeBardeleben, Nathan and Blanchard, Sean and Ferreira, Kurt B. and Stearley, Jon and Shalf, John and Gurumurthi, Sudhanva},
title = {Memory Errors in Modern Systems: The Good, The Bad, and The Ugly},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694348},
doi = {10.1145/2694344.2694348},
abstract = {Several recent publications have shown that hardware faults in the memory subsystem are commonplace. These faults are predicted to become more frequent in future systems that contain orders of magnitude more DRAM and SRAM than found in current memory subsystems. These memory subsystems will need to provide resilience techniques to tolerate these faults when deployed in high-performance computing systems and data centers containing tens of thousands of nodes. Therefore, it is critical to understand the efficacy of current hardware resilience techniques to determine whether they will be suitable for future systems. In this paper, we present a study of DRAM and SRAM faults and errors from the field. We use data from two leadership-class high-performance computer systems to analyze the reliability impact of hardware resilience schemes that are deployed in current systems. Our study has several key findings about the efficacy of many currently deployed reliability techniques such as DRAM ECC, DDR address/command parity, and SRAM ECC and parity. We also perform a methodological study, and find that counting errors instead of faults, a common practice among researchers and data center operators, can lead to incorrect conclusions about system reliability. Finally, we use our data to project the needs of future large-scale systems. We find that SRAM faults are unlikely to pose a significantly larger reliability threat in the future, while DRAM faults will be a major concern and stronger DRAM resilience schemes will be needed to maintain acceptable failure rates similar to those found on today's systems.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {297–310},
numpages = {14},
keywords = {reliability, large-scale systems, field studies},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694354,
author = {Yetim, Yavuz and Malik, Sharad and Martonosi, Margaret},
title = {CommGuard: Mitigating Communication Errors in Error-Prone Parallel Execution},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694354},
doi = {10.1145/2775054.2694354},
abstract = {As semiconductor technology scales towards ever-smaller transistor sizes, hardware fault rates are increasing. Since important application classes (e.g., multimedia, streaming workloads) are data-error-tolerant, recent research has proposed techniques that seek to save energy or improve yield by exploiting error tolerance at the architecture/microarchitecture level. Even seemingly error-tolerant applications, however, will crash or hang due to control-flow/memory addressing errors. In parallel computation, errors involving inter-thread communication can have equally catastrophic effects. Our work explores techniques that mitigate the impact of potentially catastrophic errors in parallel computation, while still garnering power, cost, or yield benefits from data error tolerance. Our proposed CommGuard solution uses FSM-based checkers to pad and discard data in order to maintain semantic alignment between program control flow and the data communicated between processors. CommGuard techniques are low overhead and they exploit application information already provided by some parallel programming languages (e.g. StreamIt). By converting potentially catastrophic communication errors into potentially tolerable data errors, CommGuard allows important streaming applications like JPEG and MP3 decoding to execute without crashing and to sustain good output quality, even for errors as frequent as every 500μs.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {311–323},
numpages = {13},
keywords = {parallel computing, high-level programming languages, application-level error tolerance}
}

@article{10.1145/2786763.2694354,
author = {Yetim, Yavuz and Malik, Sharad and Martonosi, Margaret},
title = {CommGuard: Mitigating Communication Errors in Error-Prone Parallel Execution},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694354},
doi = {10.1145/2786763.2694354},
abstract = {As semiconductor technology scales towards ever-smaller transistor sizes, hardware fault rates are increasing. Since important application classes (e.g., multimedia, streaming workloads) are data-error-tolerant, recent research has proposed techniques that seek to save energy or improve yield by exploiting error tolerance at the architecture/microarchitecture level. Even seemingly error-tolerant applications, however, will crash or hang due to control-flow/memory addressing errors. In parallel computation, errors involving inter-thread communication can have equally catastrophic effects. Our work explores techniques that mitigate the impact of potentially catastrophic errors in parallel computation, while still garnering power, cost, or yield benefits from data error tolerance. Our proposed CommGuard solution uses FSM-based checkers to pad and discard data in order to maintain semantic alignment between program control flow and the data communicated between processors. CommGuard techniques are low overhead and they exploit application information already provided by some parallel programming languages (e.g. StreamIt). By converting potentially catastrophic communication errors into potentially tolerable data errors, CommGuard allows important streaming applications like JPEG and MP3 decoding to execute without crashing and to sustain good output quality, even for errors as frequent as every 500μs.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {311–323},
numpages = {13},
keywords = {application-level error tolerance, high-level programming languages, parallel computing}
}

@inproceedings{10.1145/2694344.2694354,
author = {Yetim, Yavuz and Malik, Sharad and Martonosi, Margaret},
title = {CommGuard: Mitigating Communication Errors in Error-Prone Parallel Execution},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694354},
doi = {10.1145/2694344.2694354},
abstract = {As semiconductor technology scales towards ever-smaller transistor sizes, hardware fault rates are increasing. Since important application classes (e.g., multimedia, streaming workloads) are data-error-tolerant, recent research has proposed techniques that seek to save energy or improve yield by exploiting error tolerance at the architecture/microarchitecture level. Even seemingly error-tolerant applications, however, will crash or hang due to control-flow/memory addressing errors. In parallel computation, errors involving inter-thread communication can have equally catastrophic effects. Our work explores techniques that mitigate the impact of potentially catastrophic errors in parallel computation, while still garnering power, cost, or yield benefits from data error tolerance. Our proposed CommGuard solution uses FSM-based checkers to pad and discard data in order to maintain semantic alignment between program control flow and the data communicated between processors. CommGuard techniques are low overhead and they exploit application information already provided by some parallel programming languages (e.g. StreamIt). By converting potentially catastrophic communication errors into potentially tolerable data errors, CommGuard allows important streaming applications like JPEG and MP3 decoding to execute without crashing and to sustain good output quality, even for errors as frequent as every 500μs.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {311–323},
numpages = {13},
keywords = {parallel computing, application-level error tolerance, high-level programming languages},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694394,
author = {Kim, Dohyeong and Kwon, Yonghwi and Sumner, William N. and Zhang, Xiangyu and Xu, Dongyan},
title = {Dual Execution for On the Fly Fine Grained Execution Comparison},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694394},
doi = {10.1145/2775054.2694394},
abstract = {Execution comparison has many applications in debugging, malware analysis, software feature identification, and intrusion detection. Existing comparison techniques have various limitations. Some can only compare at the system event level and require executions to take the same input. Some require storing instruction traces that are very space-consuming and have difficulty dealing with non-determinism. In this paper, we propose a novel dual execution technique that allows on-the-fly comparison at the instruction level. Only differences between the executions are recorded. It allows executions to proceed in a coupled mode such that they share the same input sequence with the same timing, reducing nondeterminism. It also allows them to proceed in a decoupled mode such that the user can interact with each one differently. Decoupled executions can be recoupled to share the same future inputs and facilitate further comparison. We have implemented a prototype and applied it to identifying functional components for reuse, comparative debugging with new GDB primitives, and understanding real world regression failures. Our results show that dual execution is a critical enabling technique for execution comparison.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {325–338},
numpages = {14},
keywords = {execution comparison, dynamic analysis}
}

@article{10.1145/2786763.2694394,
author = {Kim, Dohyeong and Kwon, Yonghwi and Sumner, William N. and Zhang, Xiangyu and Xu, Dongyan},
title = {Dual Execution for On the Fly Fine Grained Execution Comparison},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694394},
doi = {10.1145/2786763.2694394},
abstract = {Execution comparison has many applications in debugging, malware analysis, software feature identification, and intrusion detection. Existing comparison techniques have various limitations. Some can only compare at the system event level and require executions to take the same input. Some require storing instruction traces that are very space-consuming and have difficulty dealing with non-determinism. In this paper, we propose a novel dual execution technique that allows on-the-fly comparison at the instruction level. Only differences between the executions are recorded. It allows executions to proceed in a coupled mode such that they share the same input sequence with the same timing, reducing nondeterminism. It also allows them to proceed in a decoupled mode such that the user can interact with each one differently. Decoupled executions can be recoupled to share the same future inputs and facilitate further comparison. We have implemented a prototype and applied it to identifying functional components for reuse, comparative debugging with new GDB primitives, and understanding real world regression failures. Our results show that dual execution is a critical enabling technique for execution comparison.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {325–338},
numpages = {14},
keywords = {execution comparison, dynamic analysis}
}

@inproceedings{10.1145/2694344.2694394,
author = {Kim, Dohyeong and Kwon, Yonghwi and Sumner, William N. and Zhang, Xiangyu and Xu, Dongyan},
title = {Dual Execution for On the Fly Fine Grained Execution Comparison},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694394},
doi = {10.1145/2694344.2694394},
abstract = {Execution comparison has many applications in debugging, malware analysis, software feature identification, and intrusion detection. Existing comparison techniques have various limitations. Some can only compare at the system event level and require executions to take the same input. Some require storing instruction traces that are very space-consuming and have difficulty dealing with non-determinism. In this paper, we propose a novel dual execution technique that allows on-the-fly comparison at the instruction level. Only differences between the executions are recorded. It allows executions to proceed in a coupled mode such that they share the same input sequence with the same timing, reducing nondeterminism. It also allows them to proceed in a decoupled mode such that the user can interact with each one differently. Decoupled executions can be recoupled to share the same future inputs and facilitate further comparison. We have implemented a prototype and applied it to identifying functional components for reuse, comparative debugging with new GDB primitives, and understanding real world regression failures. Our results show that dual execution is a critical enabling technique for execution comparison.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {325–338},
numpages = {14},
keywords = {dynamic analysis, execution comparison},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694390,
author = {Hosek, Petr and Cadar, Cristian},
title = {VARAN the Unbelievable: An Efficient N-Version Execution Framework},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694390},
doi = {10.1145/2775054.2694390},
abstract = {With the widespread availability of multi-core processors, running multiple diversified variants or several different versions of an application in parallel is becoming a viable approach for increasing the reliability and security of software systems. The key component of such N-version execution (NVX) systems is a runtime monitor that enables the execution of multiple versions in parallel. Unfortunately, existing monitors impose either a large performance overhead or rely on intrusive kernel-level changes. Moreover, none of the existing solutions scales well with the number of versions, since the runtime monitor acts as a performance bottleneck.In this paper, we introduce Varan, an NVX framework that combines selective binary rewriting with a novel event-streaming architecture to significantly reduce performance overhead and scale well with the number of versions, without relying on intrusive kernel modifications.Our evaluation shows that Varan can run NVX systems based on popular C10k network servers with only a modest performance overhead, and can be effectively used to increase software reliability using techniques such as transparent failover, live sanitization and multi-revision execution.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {339–353},
numpages = {15},
keywords = {record-replay, N-version execution, selective binary rewriting, event streaming, transparent failover, live sanitization, multi-revision execution}
}

@article{10.1145/2786763.2694390,
author = {Hosek, Petr and Cadar, Cristian},
title = {VARAN the Unbelievable: An Efficient N-Version Execution Framework},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694390},
doi = {10.1145/2786763.2694390},
abstract = {With the widespread availability of multi-core processors, running multiple diversified variants or several different versions of an application in parallel is becoming a viable approach for increasing the reliability and security of software systems. The key component of such N-version execution (NVX) systems is a runtime monitor that enables the execution of multiple versions in parallel. Unfortunately, existing monitors impose either a large performance overhead or rely on intrusive kernel-level changes. Moreover, none of the existing solutions scales well with the number of versions, since the runtime monitor acts as a performance bottleneck.In this paper, we introduce Varan, an NVX framework that combines selective binary rewriting with a novel event-streaming architecture to significantly reduce performance overhead and scale well with the number of versions, without relying on intrusive kernel modifications.Our evaluation shows that Varan can run NVX systems based on popular C10k network servers with only a modest performance overhead, and can be effectively used to increase software reliability using techniques such as transparent failover, live sanitization and multi-revision execution.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {339–353},
numpages = {15},
keywords = {event streaming, transparent failover, N-version execution, multi-revision execution, live sanitization, selective binary rewriting, record-replay}
}

@inproceedings{10.1145/2694344.2694390,
author = {Hosek, Petr and Cadar, Cristian},
title = {VARAN the Unbelievable: An Efficient N-Version Execution Framework},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694390},
doi = {10.1145/2694344.2694390},
abstract = {With the widespread availability of multi-core processors, running multiple diversified variants or several different versions of an application in parallel is becoming a viable approach for increasing the reliability and security of software systems. The key component of such N-version execution (NVX) systems is a runtime monitor that enables the execution of multiple versions in parallel. Unfortunately, existing monitors impose either a large performance overhead or rely on intrusive kernel-level changes. Moreover, none of the existing solutions scales well with the number of versions, since the runtime monitor acts as a performance bottleneck.In this paper, we introduce Varan, an NVX framework that combines selective binary rewriting with a novel event-streaming architecture to significantly reduce performance overhead and scale well with the number of versions, without relying on intrusive kernel modifications.Our evaluation shows that Varan can run NVX systems based on popular C10k network servers with only a modest performance overhead, and can be effectively used to increase software reliability using techniques such as transparent failover, live sanitization and multi-revision execution.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {339–353},
numpages = {15},
keywords = {selective binary rewriting, record-replay, transparent failover, multi-revision execution, live sanitization, event streaming, N-version execution},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251034,
author = {Gopalakrishnan, Ganesh},
title = {Session Details: Session 5A: I/O and Accelerators},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251034},
doi = {10.1145/3251034},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694355,
author = {Malka, Moshe and Amit, Nadav and Ben-Yehuda, Muli and Tsafrir, Dan},
title = {RIOMMU: Efficient IOMMU for I/O Devices That Employ Ring Buffers},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694355},
doi = {10.1145/2775054.2694355},
abstract = {The IOMMU allows the OS to encapsulate I/O devices in their own virtual memory spaces, thus restricting their DMAs to specific memory pages. The OS uses the IOMMU to protect itself against buggy drivers and malicious/errant devices. But the added protection comes at a cost, degrading the throughput of I/O-intensive workloads by up to an order of magnitude. This cost has motivated system designers to trade off some safety for performance, e.g., by leaving stale information in the IOTLB for a while so as to amortize costly invalidations. We observe that high-bandwidth devices---like network and PCIe SSD controllers---interact with the OS via circular ring buffers that induce a sequential, predictable workload. We design a ring IOMMU (rIOMMU) that leverages this characteristic by replacing the virtual memory page table hierarchy with a circular, flat table. A flat table is adequately supported by exactly one IOTLB entry, making every new translation an implicit invalidation of the former and thus requiring explicit invalidations only at the end of I/O bursts. Using standard networking benchmarks, we show that rIOMMU provides up to 7.56x higher throughput relative to the baseline IOMMU, and that it is within 0.77--1.00x the throughput of a system without IOMMU protection.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {355–368},
numpages = {14},
keywords = {I/O memory management unit}
}

@article{10.1145/2786763.2694355,
author = {Malka, Moshe and Amit, Nadav and Ben-Yehuda, Muli and Tsafrir, Dan},
title = {RIOMMU: Efficient IOMMU for I/O Devices That Employ Ring Buffers},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694355},
doi = {10.1145/2786763.2694355},
abstract = {The IOMMU allows the OS to encapsulate I/O devices in their own virtual memory spaces, thus restricting their DMAs to specific memory pages. The OS uses the IOMMU to protect itself against buggy drivers and malicious/errant devices. But the added protection comes at a cost, degrading the throughput of I/O-intensive workloads by up to an order of magnitude. This cost has motivated system designers to trade off some safety for performance, e.g., by leaving stale information in the IOTLB for a while so as to amortize costly invalidations. We observe that high-bandwidth devices---like network and PCIe SSD controllers---interact with the OS via circular ring buffers that induce a sequential, predictable workload. We design a ring IOMMU (rIOMMU) that leverages this characteristic by replacing the virtual memory page table hierarchy with a circular, flat table. A flat table is adequately supported by exactly one IOTLB entry, making every new translation an implicit invalidation of the former and thus requiring explicit invalidations only at the end of I/O bursts. Using standard networking benchmarks, we show that rIOMMU provides up to 7.56x higher throughput relative to the baseline IOMMU, and that it is within 0.77--1.00x the throughput of a system without IOMMU protection.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {355–368},
numpages = {14},
keywords = {I/O memory management unit}
}

@inproceedings{10.1145/2694344.2694355,
author = {Malka, Moshe and Amit, Nadav and Ben-Yehuda, Muli and Tsafrir, Dan},
title = {RIOMMU: Efficient IOMMU for I/O Devices That Employ Ring Buffers},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694355},
doi = {10.1145/2694344.2694355},
abstract = {The IOMMU allows the OS to encapsulate I/O devices in their own virtual memory spaces, thus restricting their DMAs to specific memory pages. The OS uses the IOMMU to protect itself against buggy drivers and malicious/errant devices. But the added protection comes at a cost, degrading the throughput of I/O-intensive workloads by up to an order of magnitude. This cost has motivated system designers to trade off some safety for performance, e.g., by leaving stale information in the IOTLB for a while so as to amortize costly invalidations. We observe that high-bandwidth devices---like network and PCIe SSD controllers---interact with the OS via circular ring buffers that induce a sequential, predictable workload. We design a ring IOMMU (rIOMMU) that leverages this characteristic by replacing the virtual memory page table hierarchy with a circular, flat table. A flat table is adequately supported by exactly one IOTLB entry, making every new translation an implicit invalidation of the former and thus requiring explicit invalidations only at the end of I/O bursts. Using standard networking benchmarks, we show that rIOMMU provides up to 7.56x higher throughput relative to the baseline IOMMU, and that it is within 0.77--1.00x the throughput of a system without IOMMU protection.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {355–368},
numpages = {14},
keywords = {I/O memory management unit},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694358,
author = {Liu, Daofu and Chen, Tianshi and Liu, Shaoli and Zhou, Jinhong and Zhou, Shengyuan and Teman, Olivier and Feng, Xiaobing and Zhou, Xuehai and Chen, Yunji},
title = {PuDianNao: A Polyvalent Machine Learning Accelerator},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694358},
doi = {10.1145/2775054.2694358},
abstract = {Machine Learning (ML) techniques are pervasive tools in various emerging commercial applications, but have to be accommodated by powerful computer systems to process very large data. Although general-purpose CPUs and GPUs have provided straightforward solutions, their energy-efficiencies are limited due to their excessive supports for flexibility. Hardware accelerators may achieve better energy-efficiencies, but each accelerator often accommodates only a single ML technique (family). According to the famous No-Free-Lunch theorem in the ML domain, however, an ML technique performs well on a dataset may perform poorly on another dataset, which implies that such accelerator may sometimes lead to poor learning accuracy. Even if regardless of the learning accuracy, such accelerator can still become inapplicable simply because the concrete ML task is altered, or the user chooses another ML technique.In this study, we present an ML accelerator called PuDianNao, which accommodates seven representative ML techniques, including k-means, k-nearest neighbors, naive bayes, support vector machine, linear regression, classification tree, and deep neural network. Benefited from our thorough analysis on computational primitives and locality properties of different ML techniques, PuDianNao can perform up to 1056 GOP/s (e.g., additions and multiplications) in an area of 3.51 mm^2, and consumes 596 mW only. Compared with the NVIDIA K20M GPU (28nm process), PuDianNao (65nm process) is 1.20x faster, and can reduce the energy by 128.41x.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {369–381},
numpages = {13},
keywords = {machine learning, computer architecture, accelerator}
}

@article{10.1145/2786763.2694358,
author = {Liu, Daofu and Chen, Tianshi and Liu, Shaoli and Zhou, Jinhong and Zhou, Shengyuan and Teman, Olivier and Feng, Xiaobing and Zhou, Xuehai and Chen, Yunji},
title = {PuDianNao: A Polyvalent Machine Learning Accelerator},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694358},
doi = {10.1145/2786763.2694358},
abstract = {Machine Learning (ML) techniques are pervasive tools in various emerging commercial applications, but have to be accommodated by powerful computer systems to process very large data. Although general-purpose CPUs and GPUs have provided straightforward solutions, their energy-efficiencies are limited due to their excessive supports for flexibility. Hardware accelerators may achieve better energy-efficiencies, but each accelerator often accommodates only a single ML technique (family). According to the famous No-Free-Lunch theorem in the ML domain, however, an ML technique performs well on a dataset may perform poorly on another dataset, which implies that such accelerator may sometimes lead to poor learning accuracy. Even if regardless of the learning accuracy, such accelerator can still become inapplicable simply because the concrete ML task is altered, or the user chooses another ML technique.In this study, we present an ML accelerator called PuDianNao, which accommodates seven representative ML techniques, including k-means, k-nearest neighbors, naive bayes, support vector machine, linear regression, classification tree, and deep neural network. Benefited from our thorough analysis on computational primitives and locality properties of different ML techniques, PuDianNao can perform up to 1056 GOP/s (e.g., additions and multiplications) in an area of 3.51 mm^2, and consumes 596 mW only. Compared with the NVIDIA K20M GPU (28nm process), PuDianNao (65nm process) is 1.20x faster, and can reduce the energy by 128.41x.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {369–381},
numpages = {13},
keywords = {machine learning, computer architecture, accelerator}
}

@inproceedings{10.1145/2694344.2694358,
author = {Liu, Daofu and Chen, Tianshi and Liu, Shaoli and Zhou, Jinhong and Zhou, Shengyuan and Teman, Olivier and Feng, Xiaobing and Zhou, Xuehai and Chen, Yunji},
title = {PuDianNao: A Polyvalent Machine Learning Accelerator},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694358},
doi = {10.1145/2694344.2694358},
abstract = {Machine Learning (ML) techniques are pervasive tools in various emerging commercial applications, but have to be accommodated by powerful computer systems to process very large data. Although general-purpose CPUs and GPUs have provided straightforward solutions, their energy-efficiencies are limited due to their excessive supports for flexibility. Hardware accelerators may achieve better energy-efficiencies, but each accelerator often accommodates only a single ML technique (family). According to the famous No-Free-Lunch theorem in the ML domain, however, an ML technique performs well on a dataset may perform poorly on another dataset, which implies that such accelerator may sometimes lead to poor learning accuracy. Even if regardless of the learning accuracy, such accelerator can still become inapplicable simply because the concrete ML task is altered, or the user chooses another ML technique.In this study, we present an ML accelerator called PuDianNao, which accommodates seven representative ML techniques, including k-means, k-nearest neighbors, naive bayes, support vector machine, linear regression, classification tree, and deep neural network. Benefited from our thorough analysis on computational primitives and locality properties of different ML techniques, PuDianNao can perform up to 1056 GOP/s (e.g., additions and multiplications) in an area of 3.51 mm^2, and consumes 596 mW only. Compared with the NVIDIA K20M GPU (28nm process), PuDianNao (65nm process) is 1.20x faster, and can reduce the energy by 128.41x.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {369–381},
numpages = {13},
keywords = {machine learning, accelerator, computer architecture},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251038,
author = {Blackburn, Steve},
title = {Session Details: Session 5B: Approximation},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251038},
doi = {10.1145/3251038},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694351,
author = {Goiri, Inigo and Bianchini, Ricardo and Nagarakatte, Santosh and Nguyen, Thu D.},
title = {ApproxHadoop: Bringing Approximations to MapReduce Frameworks},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694351},
doi = {10.1145/2775054.2694351},
abstract = {We propose and evaluate a framework for creating and running approximation-enabled MapReduce programs. Specifically, we propose approximation mechanisms that fit naturally into the MapReduce paradigm, including input data sampling, task dropping, and accepting and running a precise and a user-defined approximate version of the MapReduce code. We then show how to leverage statistical theories to compute error bounds for popular classes of MapReduce programs when approximating with input data sampling and/or task dropping. We implement the proposed mechanisms and error bound estimations in a prototype system called ApproxHadoop. Our evaluation uses MapReduce applications from different domains, including data analytics, scientific computing, video encoding, and machine learning. Our results show that ApproxHadoop can significantly reduce application execution time and/or energy consumption when the user is willing to tolerate small errors. For example, ApproxHadoop can reduce runtimes by up to 32x when the user can tolerate an error of 1% with 95% confidence. We conclude that our framework and system can make approximation easily accessible to many application domains using the MapReduce model.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {383–397},
numpages = {15},
keywords = {multi-stage sampling, approximation, extreme value theory, MapReduce}
}

@article{10.1145/2786763.2694351,
author = {Goiri, Inigo and Bianchini, Ricardo and Nagarakatte, Santosh and Nguyen, Thu D.},
title = {ApproxHadoop: Bringing Approximations to MapReduce Frameworks},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694351},
doi = {10.1145/2786763.2694351},
abstract = {We propose and evaluate a framework for creating and running approximation-enabled MapReduce programs. Specifically, we propose approximation mechanisms that fit naturally into the MapReduce paradigm, including input data sampling, task dropping, and accepting and running a precise and a user-defined approximate version of the MapReduce code. We then show how to leverage statistical theories to compute error bounds for popular classes of MapReduce programs when approximating with input data sampling and/or task dropping. We implement the proposed mechanisms and error bound estimations in a prototype system called ApproxHadoop. Our evaluation uses MapReduce applications from different domains, including data analytics, scientific computing, video encoding, and machine learning. Our results show that ApproxHadoop can significantly reduce application execution time and/or energy consumption when the user is willing to tolerate small errors. For example, ApproxHadoop can reduce runtimes by up to 32x when the user can tolerate an error of 1% with 95% confidence. We conclude that our framework and system can make approximation easily accessible to many application domains using the MapReduce model.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {383–397},
numpages = {15},
keywords = {approximation, extreme value theory, multi-stage sampling, MapReduce}
}

@inproceedings{10.1145/2694344.2694351,
author = {Goiri, Inigo and Bianchini, Ricardo and Nagarakatte, Santosh and Nguyen, Thu D.},
title = {ApproxHadoop: Bringing Approximations to MapReduce Frameworks},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694351},
doi = {10.1145/2694344.2694351},
abstract = {We propose and evaluate a framework for creating and running approximation-enabled MapReduce programs. Specifically, we propose approximation mechanisms that fit naturally into the MapReduce paradigm, including input data sampling, task dropping, and accepting and running a precise and a user-defined approximate version of the MapReduce code. We then show how to leverage statistical theories to compute error bounds for popular classes of MapReduce programs when approximating with input data sampling and/or task dropping. We implement the proposed mechanisms and error bound estimations in a prototype system called ApproxHadoop. Our evaluation uses MapReduce applications from different domains, including data analytics, scientific computing, video encoding, and machine learning. Our results show that ApproxHadoop can significantly reduce application execution time and/or energy consumption when the user is willing to tolerate small errors. For example, ApproxHadoop can reduce runtimes by up to 32x when the user can tolerate an error of 1% with 95% confidence. We conclude that our framework and system can make approximation easily accessible to many application domains using the MapReduce model.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {383–397},
numpages = {15},
keywords = {MapReduce, extreme value theory, approximation, multi-stage sampling},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694365,
author = {Ringenburg, Michael and Sampson, Adrian and Ackerman, Isaac and Ceze, Luis and Grossman, Dan},
title = {Monitoring and Debugging the Quality of Results in Approximate Programs},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694365},
doi = {10.1145/2775054.2694365},
abstract = {Energy efficiency is a key concern in the design of modern computer systems. One promising approach to energy-efficient computation, approximate computing, trades off output accuracy for significant gains in energy efficiency. However, debugging the actual cause of output quality problems in approximate programs is challenging. This paper presents dynamic techniques to debug and monitor the quality of approximate computations. We propose both offline debugging tools that instrument code to determine the key sources of output degradation and online approaches that monitor the quality of deployed applications.We present two offline debugging techniques and three online monitoring mechanisms. The first offline tool identifies correlations between output quality and the execution of individual approximate operations. The second tracks approximate operations that flow into a particular value. Our online monitoring mechanisms are complementary approaches designed for detecting quality problems in deployed applications, while still maintaining the energy savings from approximation.We present implementations of our techniques and describe their usage with seven applications. Our online monitors control output quality while still maintaining significant energy efficiency gains, and our offline tools provide new insights into the effects of approximation on output quality.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {399–411},
numpages = {13},
keywords = {approximate computing, debugging, monitoring}
}

@article{10.1145/2786763.2694365,
author = {Ringenburg, Michael and Sampson, Adrian and Ackerman, Isaac and Ceze, Luis and Grossman, Dan},
title = {Monitoring and Debugging the Quality of Results in Approximate Programs},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694365},
doi = {10.1145/2786763.2694365},
abstract = {Energy efficiency is a key concern in the design of modern computer systems. One promising approach to energy-efficient computation, approximate computing, trades off output accuracy for significant gains in energy efficiency. However, debugging the actual cause of output quality problems in approximate programs is challenging. This paper presents dynamic techniques to debug and monitor the quality of approximate computations. We propose both offline debugging tools that instrument code to determine the key sources of output degradation and online approaches that monitor the quality of deployed applications.We present two offline debugging techniques and three online monitoring mechanisms. The first offline tool identifies correlations between output quality and the execution of individual approximate operations. The second tracks approximate operations that flow into a particular value. Our online monitoring mechanisms are complementary approaches designed for detecting quality problems in deployed applications, while still maintaining the energy savings from approximation.We present implementations of our techniques and describe their usage with seven applications. Our online monitors control output quality while still maintaining significant energy efficiency gains, and our offline tools provide new insights into the effects of approximation on output quality.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {399–411},
numpages = {13},
keywords = {debugging, approximate computing, monitoring}
}

@inproceedings{10.1145/2694344.2694365,
author = {Ringenburg, Michael and Sampson, Adrian and Ackerman, Isaac and Ceze, Luis and Grossman, Dan},
title = {Monitoring and Debugging the Quality of Results in Approximate Programs},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694365},
doi = {10.1145/2694344.2694365},
abstract = {Energy efficiency is a key concern in the design of modern computer systems. One promising approach to energy-efficient computation, approximate computing, trades off output accuracy for significant gains in energy efficiency. However, debugging the actual cause of output quality problems in approximate programs is challenging. This paper presents dynamic techniques to debug and monitor the quality of approximate computations. We propose both offline debugging tools that instrument code to determine the key sources of output degradation and online approaches that monitor the quality of deployed applications.We present two offline debugging techniques and three online monitoring mechanisms. The first offline tool identifies correlations between output quality and the execution of individual approximate operations. The second tracks approximate operations that flow into a particular value. Our online monitoring mechanisms are complementary approaches designed for detecting quality problems in deployed applications, while still maintaining the energy savings from approximation.We present implementations of our techniques and describe their usage with seven applications. Our online monitors control output quality while still maintaining significant energy efficiency gains, and our offline tools provide new insights into the effects of approximation on output quality.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {399–411},
numpages = {13},
keywords = {monitoring, approximate computing, debugging},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251039,
author = {Ebcioglu, Kemal},
title = {Session Details: Keynote II: Keynote Address II},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251039},
doi = {10.1145/3251039},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694376,
author = {Banavar, Guruduth},
title = {Watson and the Era of Cognitive Computing},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694376},
doi = {10.1145/2775054.2694376},
abstract = {In the last decade, the availability of massive amounts of new data, and the development of new machine learning technologies, have augmented reasoning systems to give rise to a new class of computing systems. These "Cognitive Systems" learn from data, reason from models, and interact naturally with us, to perform complex tasks better than either humans or machines can do by themselves. In essence, cognitive systems help us perform like the best by penetrating the complexity of big data and leverage the power of models. One of the first cognitive systems, called Watson, demonstrated through a Jeopardy! exhibition match, that it was capable of answering complex factoid questions as effectively as the world's champions. Follow-on cognitive systems perform other tasks, such as discovery, reasoning, and multi-modal understanding in a variety of domains, such as healthcare, insurance, and education. We believe such cognitive systems will transform every industry and our everyday life for the better. In this talk, I will give an overview of the applications, the underlying capabilities, and some of the key challenges, of cognitive systems.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {413},
numpages = {1},
keywords = {big data, cognitive systems, cognitive computing, Watson}
}

@article{10.1145/2786763.2694376,
author = {Banavar, Guruduth},
title = {Watson and the Era of Cognitive Computing},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694376},
doi = {10.1145/2786763.2694376},
abstract = {In the last decade, the availability of massive amounts of new data, and the development of new machine learning technologies, have augmented reasoning systems to give rise to a new class of computing systems. These "Cognitive Systems" learn from data, reason from models, and interact naturally with us, to perform complex tasks better than either humans or machines can do by themselves. In essence, cognitive systems help us perform like the best by penetrating the complexity of big data and leverage the power of models. One of the first cognitive systems, called Watson, demonstrated through a Jeopardy! exhibition match, that it was capable of answering complex factoid questions as effectively as the world's champions. Follow-on cognitive systems perform other tasks, such as discovery, reasoning, and multi-modal understanding in a variety of domains, such as healthcare, insurance, and education. We believe such cognitive systems will transform every industry and our everyday life for the better. In this talk, I will give an overview of the applications, the underlying capabilities, and some of the key challenges, of cognitive systems.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {413},
numpages = {1},
keywords = {Watson, cognitive systems, cognitive computing, big data}
}

@inproceedings{10.1145/2694344.2694376,
author = {Banavar, Guruduth},
title = {Watson and the Era of Cognitive Computing},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694376},
doi = {10.1145/2694344.2694376},
abstract = {In the last decade, the availability of massive amounts of new data, and the development of new machine learning technologies, have augmented reasoning systems to give rise to a new class of computing systems. These "Cognitive Systems" learn from data, reason from models, and interact naturally with us, to perform complex tasks better than either humans or machines can do by themselves. In essence, cognitive systems help us perform like the best by penetrating the complexity of big data and leverage the power of models. One of the first cognitive systems, called Watson, demonstrated through a Jeopardy! exhibition match, that it was capable of answering complex factoid questions as effectively as the world's champions. Follow-on cognitive systems perform other tasks, such as discovery, reasoning, and multi-modal understanding in a variety of domains, such as healthcare, insurance, and education. We believe such cognitive systems will transform every industry and our everyday life for the better. In this talk, I will give an overview of the applications, the underlying capabilities, and some of the key challenges, of cognitive systems.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {413},
numpages = {1},
keywords = {cognitive computing, Watson, cognitive systems, big data},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251035,
author = {Shen, Xipeng},
title = {Session Details: Session 6A: Parallelism and Compilation},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251035},
doi = {10.1145/3251035},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694368,
author = {Stewart, Gordon and Gowda, Mahanth and Mainland, Geoffrey and Radunovic, Bozidar and Vytiniotis, Dimitrios and Agullo, Cristina Luengo},
title = {Ziria: A DSL for Wireless Systems Programming},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694368},
doi = {10.1145/2775054.2694368},
abstract = {Software-defined radio (SDR) brings the flexibility of software to wireless protocol design, promising an ideal platform for innovation and rapid protocol deployment. However, implementing modern wireless protocols on existing SDR platforms often requires careful hand-tuning of low-level code, which can undermine the advantages of software. Ziria is a new domain-specific language (DSL) that offers programming abstractions suitable for wireless physical (PHY) layer tasks while emphasizing the pipeline reconfiguration aspects of PHY programming. The Ziria compiler implements a rich set of specialized optimizations, such as lookup table generation and pipeline fusion. We also offer a novel -- due to pipeline reconfiguration -- algorithm to optimize the data widths of computations in Ziria pipelines. We demonstrate the programming flexibility of Ziria and the performance of the generated code through a detailed evaluation of a line-rate Ziria WiFi 802.11a/g implementation that is on par and in many cases outperforms a hand-tuned state-of-the-art C++ implementation on commodity CPUs.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {415–428},
numpages = {14},
keywords = {domain-specific languages, wireless networking, wifi}
}

@article{10.1145/2786763.2694368,
author = {Stewart, Gordon and Gowda, Mahanth and Mainland, Geoffrey and Radunovic, Bozidar and Vytiniotis, Dimitrios and Agullo, Cristina Luengo},
title = {Ziria: A DSL for Wireless Systems Programming},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694368},
doi = {10.1145/2786763.2694368},
abstract = {Software-defined radio (SDR) brings the flexibility of software to wireless protocol design, promising an ideal platform for innovation and rapid protocol deployment. However, implementing modern wireless protocols on existing SDR platforms often requires careful hand-tuning of low-level code, which can undermine the advantages of software. Ziria is a new domain-specific language (DSL) that offers programming abstractions suitable for wireless physical (PHY) layer tasks while emphasizing the pipeline reconfiguration aspects of PHY programming. The Ziria compiler implements a rich set of specialized optimizations, such as lookup table generation and pipeline fusion. We also offer a novel -- due to pipeline reconfiguration -- algorithm to optimize the data widths of computations in Ziria pipelines. We demonstrate the programming flexibility of Ziria and the performance of the generated code through a detailed evaluation of a line-rate Ziria WiFi 802.11a/g implementation that is on par and in many cases outperforms a hand-tuned state-of-the-art C++ implementation on commodity CPUs.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {415–428},
numpages = {14},
keywords = {wifi, wireless networking, domain-specific languages}
}

@inproceedings{10.1145/2694344.2694368,
author = {Stewart, Gordon and Gowda, Mahanth and Mainland, Geoffrey and Radunovic, Bozidar and Vytiniotis, Dimitrios and Agullo, Cristina Luengo},
title = {Ziria: A DSL for Wireless Systems Programming},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694368},
doi = {10.1145/2694344.2694368},
abstract = {Software-defined radio (SDR) brings the flexibility of software to wireless protocol design, promising an ideal platform for innovation and rapid protocol deployment. However, implementing modern wireless protocols on existing SDR platforms often requires careful hand-tuning of low-level code, which can undermine the advantages of software. Ziria is a new domain-specific language (DSL) that offers programming abstractions suitable for wireless physical (PHY) layer tasks while emphasizing the pipeline reconfiguration aspects of PHY programming. The Ziria compiler implements a rich set of specialized optimizations, such as lookup table generation and pipeline fusion. We also offer a novel -- due to pipeline reconfiguration -- algorithm to optimize the data widths of computations in Ziria pipelines. We demonstrate the programming flexibility of Ziria and the performance of the generated code through a detailed evaluation of a line-rate Ziria WiFi 802.11a/g implementation that is on par and in many cases outperforms a hand-tuned state-of-the-art C++ implementation on commodity CPUs.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {415–428},
numpages = {14},
keywords = {wifi, wireless networking, domain-specific languages},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@dataset{10.1145/review-2694344.2694368_R51166,
author = {De, Debraj},
title = {Review ID:R51166 for DOI: 10.1145/2694344.2694368},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-2694344.2694368_R51166}
}

@article{10.1145/2775054.2694364,
author = {Mullapudi, Ravi Teja and Vasista, Vinay and Bondhugula, Uday},
title = {PolyMage: Automatic Optimization for Image Processing Pipelines},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694364},
doi = {10.1145/2775054.2694364},
abstract = {This paper presents the design and implementation of PolyMage, a domain-specific language and compiler for image processing pipelines. An image processing pipeline can be viewed as a graph of interconnected stages which process images successively. Each stage typically performs one of point-wise, stencil, reduction or data-dependent operations on image pixels. Individual stages in a pipeline typically exhibit abundant data parallelism that can be exploited with relative ease. However, the stages also require high memory bandwidth preventing effective utilization of parallelism available on modern architectures. For applications that demand high performance, the traditional options are to use optimized libraries like OpenCV or to optimize manually. While using libraries precludes optimization across library routines, manual optimization accounting for both parallelism and locality is very tedious.The focus of our system, PolyMage, is on automatically generating high-performance implementations of image processing pipelines expressed in a high-level declarative language. Our optimization approach primarily relies on the transformation and code generation capabilities of the polyhedral compiler framework. To the best of our knowledge, this is the first model-driven compiler for image processing pipelines that performs complex fusion, tiling, and storage optimization automatically. Experimental results on a modern multicore system show that the performance achieved by our automatic approach is up to 1.81x better than that achieved through manual tuning in Halide, a state-of-the-art language and compiler for image processing pipelines. For a camera raw image processing pipeline, our performance is comparable to that of a hand-tuned implementation.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {429–443},
numpages = {15},
keywords = {polyhedral optimization, parallelism, domain-specific language, vectorization, tiling, multicores, locality, image processing}
}

@article{10.1145/2786763.2694364,
author = {Mullapudi, Ravi Teja and Vasista, Vinay and Bondhugula, Uday},
title = {PolyMage: Automatic Optimization for Image Processing Pipelines},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694364},
doi = {10.1145/2786763.2694364},
abstract = {This paper presents the design and implementation of PolyMage, a domain-specific language and compiler for image processing pipelines. An image processing pipeline can be viewed as a graph of interconnected stages which process images successively. Each stage typically performs one of point-wise, stencil, reduction or data-dependent operations on image pixels. Individual stages in a pipeline typically exhibit abundant data parallelism that can be exploited with relative ease. However, the stages also require high memory bandwidth preventing effective utilization of parallelism available on modern architectures. For applications that demand high performance, the traditional options are to use optimized libraries like OpenCV or to optimize manually. While using libraries precludes optimization across library routines, manual optimization accounting for both parallelism and locality is very tedious.The focus of our system, PolyMage, is on automatically generating high-performance implementations of image processing pipelines expressed in a high-level declarative language. Our optimization approach primarily relies on the transformation and code generation capabilities of the polyhedral compiler framework. To the best of our knowledge, this is the first model-driven compiler for image processing pipelines that performs complex fusion, tiling, and storage optimization automatically. Experimental results on a modern multicore system show that the performance achieved by our automatic approach is up to 1.81x better than that achieved through manual tuning in Halide, a state-of-the-art language and compiler for image processing pipelines. For a camera raw image processing pipeline, our performance is comparable to that of a hand-tuned implementation.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {429–443},
numpages = {15},
keywords = {locality, multicores, vectorization, domain-specific language, image processing, parallelism, polyhedral optimization, tiling}
}

@inproceedings{10.1145/2694344.2694364,
author = {Mullapudi, Ravi Teja and Vasista, Vinay and Bondhugula, Uday},
title = {PolyMage: Automatic Optimization for Image Processing Pipelines},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694364},
doi = {10.1145/2694344.2694364},
abstract = {This paper presents the design and implementation of PolyMage, a domain-specific language and compiler for image processing pipelines. An image processing pipeline can be viewed as a graph of interconnected stages which process images successively. Each stage typically performs one of point-wise, stencil, reduction or data-dependent operations on image pixels. Individual stages in a pipeline typically exhibit abundant data parallelism that can be exploited with relative ease. However, the stages also require high memory bandwidth preventing effective utilization of parallelism available on modern architectures. For applications that demand high performance, the traditional options are to use optimized libraries like OpenCV or to optimize manually. While using libraries precludes optimization across library routines, manual optimization accounting for both parallelism and locality is very tedious.The focus of our system, PolyMage, is on automatically generating high-performance implementations of image processing pipelines expressed in a high-level declarative language. Our optimization approach primarily relies on the transformation and code generation capabilities of the polyhedral compiler framework. To the best of our knowledge, this is the first model-driven compiler for image processing pipelines that performs complex fusion, tiling, and storage optimization automatically. Experimental results on a modern multicore system show that the performance achieved by our automatic approach is up to 1.81x better than that achieved through manual tuning in Halide, a state-of-the-art language and compiler for image processing pipelines. For a camera raw image processing pipeline, our performance is comparable to that of a hand-tuned implementation.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {429–443},
numpages = {15},
keywords = {tiling, image processing, vectorization, parallelism, polyhedral optimization, locality, multicores, domain-specific language},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694357,
author = {Heckey, Jeff and Patil, Shruti and JavadiAbhari, Ali and Holmes, Adam and Kudrow, Daniel and Brown, Kenneth R. and Franklin, Diana and Chong, Frederic T. and Martonosi, Margaret},
title = {Compiler Management of Communication and Parallelism for Quantum Computation},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694357},
doi = {10.1145/2775054.2694357},
abstract = {Quantum computing (QC) offers huge promise to accelerate a range of computationally intensive benchmarks. Quantum computing is limited, however, by the challenges of decoherence: i.e., a quantum state can only be maintained for short windows of time before it decoheres. While quantum error correction codes can protect against decoherence, fast execution time is the best defense against decoherence, so efficient architectures and effective scheduling algorithms are necessary. This paper proposes the Multi-SIMD QC architecture and then proposes and evaluates effective schedulers to map benchmark descriptions onto Multi-SIMD architectures. The Multi-SIMD model consists of a small number of SIMD regions, each of which may support operations on up to thousands of qubits per cycle.Efficient Multi-SIMD operation requires efficient scheduling. This work develops schedulers to reduce communication requirements of qubits between operating regions, while also improving parallelism.We find that communication to global memory is a dominant cost in QC. We also note that many quantum benchmarks have long serial operation paths (although each operation may be data parallel). To exploit this characteristic, we introduce Longest-Path-First Scheduling (LPFS) which pins operations to SIMD regions to keep data in-place and reduce communication to memory. The use of small, local scratchpad memories also further reduces communication. Our results show a 3% to 308% improvement for LPFS over conventional scheduling algorithms, and an additional 3% to 64% improvement using scratchpad memories. Our work is the most comprehensive software-to-quantum toolflow published to date, with efficient and practical scheduling techniques that reduce communication and increase parallelism for full-scale quantum code executing up to a trillion quantum gate operations.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {445–456},
numpages = {12},
keywords = {performance metrics, design languages, cached memories}
}

@article{10.1145/2786763.2694357,
author = {Heckey, Jeff and Patil, Shruti and JavadiAbhari, Ali and Holmes, Adam and Kudrow, Daniel and Brown, Kenneth R. and Franklin, Diana and Chong, Frederic T. and Martonosi, Margaret},
title = {Compiler Management of Communication and Parallelism for Quantum Computation},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694357},
doi = {10.1145/2786763.2694357},
abstract = {Quantum computing (QC) offers huge promise to accelerate a range of computationally intensive benchmarks. Quantum computing is limited, however, by the challenges of decoherence: i.e., a quantum state can only be maintained for short windows of time before it decoheres. While quantum error correction codes can protect against decoherence, fast execution time is the best defense against decoherence, so efficient architectures and effective scheduling algorithms are necessary. This paper proposes the Multi-SIMD QC architecture and then proposes and evaluates effective schedulers to map benchmark descriptions onto Multi-SIMD architectures. The Multi-SIMD model consists of a small number of SIMD regions, each of which may support operations on up to thousands of qubits per cycle.Efficient Multi-SIMD operation requires efficient scheduling. This work develops schedulers to reduce communication requirements of qubits between operating regions, while also improving parallelism.We find that communication to global memory is a dominant cost in QC. We also note that many quantum benchmarks have long serial operation paths (although each operation may be data parallel). To exploit this characteristic, we introduce Longest-Path-First Scheduling (LPFS) which pins operations to SIMD regions to keep data in-place and reduce communication to memory. The use of small, local scratchpad memories also further reduces communication. Our results show a 3% to 308% improvement for LPFS over conventional scheduling algorithms, and an additional 3% to 64% improvement using scratchpad memories. Our work is the most comprehensive software-to-quantum toolflow published to date, with efficient and practical scheduling techniques that reduce communication and increase parallelism for full-scale quantum code executing up to a trillion quantum gate operations.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {445–456},
numpages = {12},
keywords = {cached memories, design languages, performance metrics}
}

@inproceedings{10.1145/2694344.2694357,
author = {Heckey, Jeff and Patil, Shruti and JavadiAbhari, Ali and Holmes, Adam and Kudrow, Daniel and Brown, Kenneth R. and Franklin, Diana and Chong, Frederic T. and Martonosi, Margaret},
title = {Compiler Management of Communication and Parallelism for Quantum Computation},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694357},
doi = {10.1145/2694344.2694357},
abstract = {Quantum computing (QC) offers huge promise to accelerate a range of computationally intensive benchmarks. Quantum computing is limited, however, by the challenges of decoherence: i.e., a quantum state can only be maintained for short windows of time before it decoheres. While quantum error correction codes can protect against decoherence, fast execution time is the best defense against decoherence, so efficient architectures and effective scheduling algorithms are necessary. This paper proposes the Multi-SIMD QC architecture and then proposes and evaluates effective schedulers to map benchmark descriptions onto Multi-SIMD architectures. The Multi-SIMD model consists of a small number of SIMD regions, each of which may support operations on up to thousands of qubits per cycle.Efficient Multi-SIMD operation requires efficient scheduling. This work develops schedulers to reduce communication requirements of qubits between operating regions, while also improving parallelism.We find that communication to global memory is a dominant cost in QC. We also note that many quantum benchmarks have long serial operation paths (although each operation may be data parallel). To exploit this characteristic, we introduce Longest-Path-First Scheduling (LPFS) which pins operations to SIMD regions to keep data in-place and reduce communication to memory. The use of small, local scratchpad memories also further reduces communication. Our results show a 3% to 308% improvement for LPFS over conventional scheduling algorithms, and an additional 3% to 64% improvement using scratchpad memories. Our work is the most comprehensive software-to-quantum toolflow published to date, with efficient and practical scheduling techniques that reduce communication and increase parallelism for full-scale quantum code executing up to a trillion quantum gate operations.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {445–456},
numpages = {12},
keywords = {performance metrics, cached memories, design languages},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694363,
author = {Hassaan, Muhammad Amber and Nguyen, Donald D. and Pingali, Keshav K.},
title = {Kinetic Dependence Graphs},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694363},
doi = {10.1145/2775054.2694363},
abstract = {Task graphs or dependence graphs are used in runtime systems to schedule tasks for parallel execution. In problem domains such as dense linear algebra and signal processing, dependence graphs can be generated from a program by static analysis. However, in emerging problem domains such as graph analytics, the set of tasks and dependences between tasks in a program are complex functions of runtime values and cannot be determined statically. In this paper, we introduce a novel approach for exploiting parallelism in such programs. This approach is based on a data structure called the kinetic dependence graph (KDG), which consists of a dependence graph together with update rules that incrementally update the graph to reflect changes in the dependence structure whenever a task is completed.We have implemented a simple programming model that allows programmers to write these applications at a high level of abstraction, and a runtime within the Galois system [15] that builds the KDG automatically and executes the program in parallel. On a suite of programs that are difficult to parallelize otherwise, we have obtained speedups of up to 33 on 40 cores, out-performing third-party implementations in many cases.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {457–471},
numpages = {15},
keywords = {kinetic dependence graph, ordered algorithms, stable-source and unstable-source algorithms}
}

@article{10.1145/2786763.2694363,
author = {Hassaan, Muhammad Amber and Nguyen, Donald D. and Pingali, Keshav K.},
title = {Kinetic Dependence Graphs},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694363},
doi = {10.1145/2786763.2694363},
abstract = {Task graphs or dependence graphs are used in runtime systems to schedule tasks for parallel execution. In problem domains such as dense linear algebra and signal processing, dependence graphs can be generated from a program by static analysis. However, in emerging problem domains such as graph analytics, the set of tasks and dependences between tasks in a program are complex functions of runtime values and cannot be determined statically. In this paper, we introduce a novel approach for exploiting parallelism in such programs. This approach is based on a data structure called the kinetic dependence graph (KDG), which consists of a dependence graph together with update rules that incrementally update the graph to reflect changes in the dependence structure whenever a task is completed.We have implemented a simple programming model that allows programmers to write these applications at a high level of abstraction, and a runtime within the Galois system [15] that builds the KDG automatically and executes the program in parallel. On a suite of programs that are difficult to parallelize otherwise, we have obtained speedups of up to 33 on 40 cores, out-performing third-party implementations in many cases.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {457–471},
numpages = {15},
keywords = {kinetic dependence graph, stable-source and unstable-source algorithms, ordered algorithms}
}

@inproceedings{10.1145/2694344.2694363,
author = {Hassaan, Muhammad Amber and Nguyen, Donald D. and Pingali, Keshav K.},
title = {Kinetic Dependence Graphs},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694363},
doi = {10.1145/2694344.2694363},
abstract = {Task graphs or dependence graphs are used in runtime systems to schedule tasks for parallel execution. In problem domains such as dense linear algebra and signal processing, dependence graphs can be generated from a program by static analysis. However, in emerging problem domains such as graph analytics, the set of tasks and dependences between tasks in a program are complex functions of runtime values and cannot be determined statically. In this paper, we introduce a novel approach for exploiting parallelism in such programs. This approach is based on a data structure called the kinetic dependence graph (KDG), which consists of a dependence graph together with update rules that incrementally update the graph to reflect changes in the dependence structure whenever a task is completed.We have implemented a simple programming model that allows programmers to write these applications at a high level of abstraction, and a runtime within the Galois system [15] that builds the KDG automatically and executes the program in parallel. On a suite of programs that are difficult to parallelize otherwise, we have obtained speedups of up to 33 on 40 cores, out-performing third-party implementations in many cases.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {457–471},
numpages = {15},
keywords = {ordered algorithms, kinetic dependence graph, stable-source and unstable-source algorithms},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251042,
author = {Torrellas, Josep},
title = {Session Details: Session 6B: Testing and Tainting, Verification and Security},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251042},
doi = {10.1145/3251042},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694389,
author = {Sidiroglou-Douskos, Stelios and Lahtinen, Eric and Rittenhouse, Nathan and Piselli, Paolo and Long, Fan and Kim, Deokhwan and Rinard, Martin},
title = {Targeted Automatic Integer Overflow Discovery Using Goal-Directed Conditional Branch Enforcement},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694389},
doi = {10.1145/2775054.2694389},
abstract = {We present a new technique and system, DIODE, for auto- matically generating inputs that trigger overflows at memory allocation sites. DIODE is designed to identify relevant sanity checks that inputs must satisfy to trigger overflows at target memory allocation sites, then generate inputs that satisfy these sanity checks to successfully trigger the overflow. DIODE works with off-the-shelf, production x86 binaries. Our results show that, for our benchmark set of applications, and for every target memory allocation site exercised by our seed inputs (which the applications process correctly with no overflows), either 1) DIODE is able to generate an input that triggers an overflow at that site or 2) there is no input that would trigger an overflow for the observed target expression at that site.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {473–486},
numpages = {14},
keywords = {integer overflow, targeted symbolic execution, bug detection}
}

@article{10.1145/2786763.2694389,
author = {Sidiroglou-Douskos, Stelios and Lahtinen, Eric and Rittenhouse, Nathan and Piselli, Paolo and Long, Fan and Kim, Deokhwan and Rinard, Martin},
title = {Targeted Automatic Integer Overflow Discovery Using Goal-Directed Conditional Branch Enforcement},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694389},
doi = {10.1145/2786763.2694389},
abstract = {We present a new technique and system, DIODE, for auto- matically generating inputs that trigger overflows at memory allocation sites. DIODE is designed to identify relevant sanity checks that inputs must satisfy to trigger overflows at target memory allocation sites, then generate inputs that satisfy these sanity checks to successfully trigger the overflow. DIODE works with off-the-shelf, production x86 binaries. Our results show that, for our benchmark set of applications, and for every target memory allocation site exercised by our seed inputs (which the applications process correctly with no overflows), either 1) DIODE is able to generate an input that triggers an overflow at that site or 2) there is no input that would trigger an overflow for the observed target expression at that site.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {473–486},
numpages = {14},
keywords = {bug detection, integer overflow, targeted symbolic execution}
}

@inproceedings{10.1145/2694344.2694389,
author = {Sidiroglou-Douskos, Stelios and Lahtinen, Eric and Rittenhouse, Nathan and Piselli, Paolo and Long, Fan and Kim, Deokhwan and Rinard, Martin},
title = {Targeted Automatic Integer Overflow Discovery Using Goal-Directed Conditional Branch Enforcement},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694389},
doi = {10.1145/2694344.2694389},
abstract = {We present a new technique and system, DIODE, for auto- matically generating inputs that trigger overflows at memory allocation sites. DIODE is designed to identify relevant sanity checks that inputs must satisfy to trigger overflows at target memory allocation sites, then generate inputs that satisfy these sanity checks to successfully trigger the overflow. DIODE works with off-the-shelf, production x86 binaries. Our results show that, for our benchmark set of applications, and for every target memory allocation site exercised by our seed inputs (which the applications process correctly with no overflows), either 1) DIODE is able to generate an input that triggers an overflow at that site or 2) there is no input that would trigger an overflow for the observed target expression at that site.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {473–486},
numpages = {14},
keywords = {bug detection, targeted symbolic execution, integer overflow},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694383,
author = {Dhawan, Udit and Hritcu, Catalin and Rubin, Raphael and Vasilakis, Nikos and Chiricescu, Silviu and Smith, Jonathan M. and Knight, Thomas F. and Pierce, Benjamin C. and DeHon, Andre},
title = {Architectural Support for Software-Defined Metadata Processing},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694383},
doi = {10.1145/2775054.2694383},
abstract = {Optimized hardware for propagating and checking software-programmable metadata tags can achieve low runtime overhead. We generalize prior work on hardware tagging by considering a generic architecture that supports software-defined policies over metadata of arbitrary size and complexity; we introduce several novel microarchitectural optimizations that keep the overhead of this rich processing low. Our model thus achieves the efficiency of previous hardware-based approaches with the flexibility of the software-based ones. We demonstrate this by using it to enforce four diverse safety and security policies---spatial and temporal memory safety, taint tracking, control-flow integrity, and code and data separation---plus a composite policy that enforces all of them simultaneously. Experiments on SPEC CPU2006 benchmarks with a PUMP-enhanced RISC processor show modest impact on runtime (typically under 10%) and power ceiling (less than 10%), in return for some increase in energy usage (typically under 60%) and area for on-chip memory structures (110%).},
journal = {SIGPLAN Not.},
month = {mar},
pages = {487–502},
numpages = {16},
keywords = {CFI, memory safety, taint tracking, security, tagged architecture, metadata}
}

@article{10.1145/2786763.2694383,
author = {Dhawan, Udit and Hritcu, Catalin and Rubin, Raphael and Vasilakis, Nikos and Chiricescu, Silviu and Smith, Jonathan M. and Knight, Thomas F. and Pierce, Benjamin C. and DeHon, Andre},
title = {Architectural Support for Software-Defined Metadata Processing},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694383},
doi = {10.1145/2786763.2694383},
abstract = {Optimized hardware for propagating and checking software-programmable metadata tags can achieve low runtime overhead. We generalize prior work on hardware tagging by considering a generic architecture that supports software-defined policies over metadata of arbitrary size and complexity; we introduce several novel microarchitectural optimizations that keep the overhead of this rich processing low. Our model thus achieves the efficiency of previous hardware-based approaches with the flexibility of the software-based ones. We demonstrate this by using it to enforce four diverse safety and security policies---spatial and temporal memory safety, taint tracking, control-flow integrity, and code and data separation---plus a composite policy that enforces all of them simultaneously. Experiments on SPEC CPU2006 benchmarks with a PUMP-enhanced RISC processor show modest impact on runtime (typically under 10%) and power ceiling (less than 10%), in return for some increase in energy usage (typically under 60%) and area for on-chip memory structures (110%).},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {487–502},
numpages = {16},
keywords = {tagged architecture, CFI, memory safety, security, taint tracking, metadata}
}

@inproceedings{10.1145/2694344.2694383,
author = {Dhawan, Udit and Hritcu, Catalin and Rubin, Raphael and Vasilakis, Nikos and Chiricescu, Silviu and Smith, Jonathan M. and Knight, Thomas F. and Pierce, Benjamin C. and DeHon, Andre},
title = {Architectural Support for Software-Defined Metadata Processing},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694383},
doi = {10.1145/2694344.2694383},
abstract = {Optimized hardware for propagating and checking software-programmable metadata tags can achieve low runtime overhead. We generalize prior work on hardware tagging by considering a generic architecture that supports software-defined policies over metadata of arbitrary size and complexity; we introduce several novel microarchitectural optimizations that keep the overhead of this rich processing low. Our model thus achieves the efficiency of previous hardware-based approaches with the flexibility of the software-based ones. We demonstrate this by using it to enforce four diverse safety and security policies---spatial and temporal memory safety, taint tracking, control-flow integrity, and code and data separation---plus a composite policy that enforces all of them simultaneously. Experiments on SPEC CPU2006 benchmarks with a PUMP-enhanced RISC processor show modest impact on runtime (typically under 10%) and power ceiling (less than 10%), in return for some increase in energy usage (typically under 60%) and area for on-chip memory structures (110%).},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {487–502},
numpages = {16},
keywords = {memory safety, CFI, security, taint tracking, metadata, tagged architecture},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694372,
author = {Zhang, Danfeng and Wang, Yao and Suh, G. Edward and Myers, Andrew C.},
title = {A Hardware Design Language for Timing-Sensitive Information-Flow Security},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694372},
doi = {10.1145/2775054.2694372},
abstract = {Information security can be compromised by leakage via low-level hardware features. One recently prominent example is cache probing attacks, which rely on timing channels created by caches. We introduce a hardware design language, SecVerilog, which makes it possible to statically analyze information flow at the hardware level. With SecVerilog, systems can be built with verifiable control of timing channels and other information channels. SecVerilog is Verilog, extended with expressive type annotations that enable precise reasoning about information flow. It also comes with rigorous formal assurance: we prove that SecVerilog enforces timing-sensitive noninterference and thus ensures secure information flow. By building a secure MIPS processor and its caches, we demonstrate that SecVerilog makes it possible to build complex hardware designs with verified security, yet with low overhead in time, space, and HW designer effort.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {503–516},
numpages = {14},
keywords = {information flow control, timing channels, dependent types, hardware description language}
}

@article{10.1145/2786763.2694372,
author = {Zhang, Danfeng and Wang, Yao and Suh, G. Edward and Myers, Andrew C.},
title = {A Hardware Design Language for Timing-Sensitive Information-Flow Security},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694372},
doi = {10.1145/2786763.2694372},
abstract = {Information security can be compromised by leakage via low-level hardware features. One recently prominent example is cache probing attacks, which rely on timing channels created by caches. We introduce a hardware design language, SecVerilog, which makes it possible to statically analyze information flow at the hardware level. With SecVerilog, systems can be built with verifiable control of timing channels and other information channels. SecVerilog is Verilog, extended with expressive type annotations that enable precise reasoning about information flow. It also comes with rigorous formal assurance: we prove that SecVerilog enforces timing-sensitive noninterference and thus ensures secure information flow. By building a secure MIPS processor and its caches, we demonstrate that SecVerilog makes it possible to build complex hardware designs with verified security, yet with low overhead in time, space, and HW designer effort.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {503–516},
numpages = {14},
keywords = {dependent types, information flow control, hardware description language, timing channels}
}

@inproceedings{10.1145/2694344.2694372,
author = {Zhang, Danfeng and Wang, Yao and Suh, G. Edward and Myers, Andrew C.},
title = {A Hardware Design Language for Timing-Sensitive Information-Flow Security},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694372},
doi = {10.1145/2694344.2694372},
abstract = {Information security can be compromised by leakage via low-level hardware features. One recently prominent example is cache probing attacks, which rely on timing channels created by caches. We introduce a hardware design language, SecVerilog, which makes it possible to statically analyze information flow at the hardware level. With SecVerilog, systems can be built with verifiable control of timing channels and other information channels. SecVerilog is Verilog, extended with expressive type annotations that enable precise reasoning about information flow. It also comes with rigorous formal assurance: we prove that SecVerilog enforces timing-sensitive noninterference and thus ensures secure information flow. By building a secure MIPS processor and its caches, we demonstrate that SecVerilog makes it possible to build complex hardware designs with verified security, yet with low overhead in time, space, and HW designer effort.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {503–516},
numpages = {14},
keywords = {information flow control, hardware description language, dependent types, timing channels},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694366,
author = {Hicks, Matthew and Sturton, Cynthia and King, Samuel T. and Smith, Jonathan M.},
title = {SPECS: A Lightweight Runtime Mechanism for Protecting Software from Security-Critical Processor Bugs},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694366},
doi = {10.1145/2775054.2694366},
abstract = {Processor implementation errata remain a problem, and worse, a subset of these bugs are security-critical. We classified 7 years of errata from recent commercial processors to understand the magnitude and severity of this problem, and found that of 301 errata analyzed, 28 are security-critical. We propose the SECURITY-CRITICAL PROCESSOR ER- RATA CATCHING SYSTEM (SPECS) as a low-overhead solution to this problem. SPECS employs a dynamic verification strategy that is made lightweight by limiting protection to only security-critical processor state. As a proof-of- concept, we implement a hardware prototype of SPECS in an open source processor. Using this prototype, we evaluate SPECS against a set of 14 bugs inspired by the types of security-critical errata we discovered in the classification phase. The evaluation shows that SPECS is 86% effective as a defense when deployed using only ISA-level state; incurs less than 5% area and power overhead; and has no software run-time overhead.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {517–529},
numpages = {13},
keywords = {processor errata, security-critical processor errata, hardware security exploits}
}

@article{10.1145/2786763.2694366,
author = {Hicks, Matthew and Sturton, Cynthia and King, Samuel T. and Smith, Jonathan M.},
title = {SPECS: A Lightweight Runtime Mechanism for Protecting Software from Security-Critical Processor Bugs},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694366},
doi = {10.1145/2786763.2694366},
abstract = {Processor implementation errata remain a problem, and worse, a subset of these bugs are security-critical. We classified 7 years of errata from recent commercial processors to understand the magnitude and severity of this problem, and found that of 301 errata analyzed, 28 are security-critical. We propose the SECURITY-CRITICAL PROCESSOR ER- RATA CATCHING SYSTEM (SPECS) as a low-overhead solution to this problem. SPECS employs a dynamic verification strategy that is made lightweight by limiting protection to only security-critical processor state. As a proof-of- concept, we implement a hardware prototype of SPECS in an open source processor. Using this prototype, we evaluate SPECS against a set of 14 bugs inspired by the types of security-critical errata we discovered in the classification phase. The evaluation shows that SPECS is 86% effective as a defense when deployed using only ISA-level state; incurs less than 5% area and power overhead; and has no software run-time overhead.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {517–529},
numpages = {13},
keywords = {hardware security exploits, security-critical processor errata, processor errata}
}

@inproceedings{10.1145/2694344.2694366,
author = {Hicks, Matthew and Sturton, Cynthia and King, Samuel T. and Smith, Jonathan M.},
title = {SPECS: A Lightweight Runtime Mechanism for Protecting Software from Security-Critical Processor Bugs},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694366},
doi = {10.1145/2694344.2694366},
abstract = {Processor implementation errata remain a problem, and worse, a subset of these bugs are security-critical. We classified 7 years of errata from recent commercial processors to understand the magnitude and severity of this problem, and found that of 301 errata analyzed, 28 are security-critical. We propose the SECURITY-CRITICAL PROCESSOR ER- RATA CATCHING SYSTEM (SPECS) as a low-overhead solution to this problem. SPECS employs a dynamic verification strategy that is made lightweight by limiting protection to only security-critical processor state. As a proof-of- concept, we implement a hardware prototype of SPECS in an open source processor. Using this prototype, we evaluate SPECS against a set of 14 bugs inspired by the types of security-critical errata we discovered in the classification phase. The evaluation shows that SPECS is 86% effective as a defense when deployed using only ISA-level state; incurs less than 5% area and power overhead; and has no software run-time overhead.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {517–529},
numpages = {13},
keywords = {processor errata, security-critical processor errata, hardware security exploits},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251040,
author = {Boehm, Hans},
title = {Session Details: Session 7A: Memory Models II},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251040},
doi = {10.1145/3251040},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694388,
author = {Duan, Yuelu and Honarmand, Nima and Torrellas, Josep},
title = {Asymmetric Memory Fences: Optimizing Both Performance and Implementability},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694388},
doi = {10.1145/2775054.2694388},
abstract = {There have been several recent efforts to improve the performance of fences. The most aggressive designs allow post-fence accesses to retire and complete before the fence completes. Unfortunately, such designs present implementation difficulties due to their reliance on global state and structures.This paper's goal is to optimize both the performance and the implementability of fences. We start-off with a design like the most aggressive ones but without the global state. We call it Weak Fence or wF. Since the concurrent execution of multiple wFs can deadlock, we combine wFs with a conventional fence (i.e., Strong Fence or sF) for the less performance-critical thread(s). We call the result an Asymmetric fence group. We also propose a taxonomy of Asymmetric fence groups under TSO. Compared to past aggressive fences, Asymmetric fence groups both are substantially easier to implement and have higher average performance. The two main designs presented (WS+ and W+) speed-up workloads under TSO by an average of 13% and 21%, respectively, over conventional fences.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {531–543},
numpages = {13},
keywords = {fences, sequential consistency, parallel programming, shared-memory machines, synchronization}
}

@article{10.1145/2786763.2694388,
author = {Duan, Yuelu and Honarmand, Nima and Torrellas, Josep},
title = {Asymmetric Memory Fences: Optimizing Both Performance and Implementability},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694388},
doi = {10.1145/2786763.2694388},
abstract = {There have been several recent efforts to improve the performance of fences. The most aggressive designs allow post-fence accesses to retire and complete before the fence completes. Unfortunately, such designs present implementation difficulties due to their reliance on global state and structures.This paper's goal is to optimize both the performance and the implementability of fences. We start-off with a design like the most aggressive ones but without the global state. We call it Weak Fence or wF. Since the concurrent execution of multiple wFs can deadlock, we combine wFs with a conventional fence (i.e., Strong Fence or sF) for the less performance-critical thread(s). We call the result an Asymmetric fence group. We also propose a taxonomy of Asymmetric fence groups under TSO. Compared to past aggressive fences, Asymmetric fence groups both are substantially easier to implement and have higher average performance. The two main designs presented (WS+ and W+) speed-up workloads under TSO by an average of 13% and 21%, respectively, over conventional fences.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {531–543},
numpages = {13},
keywords = {parallel programming, synchronization, shared-memory machines, fences, sequential consistency}
}

@inproceedings{10.1145/2694344.2694388,
author = {Duan, Yuelu and Honarmand, Nima and Torrellas, Josep},
title = {Asymmetric Memory Fences: Optimizing Both Performance and Implementability},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694388},
doi = {10.1145/2694344.2694388},
abstract = {There have been several recent efforts to improve the performance of fences. The most aggressive designs allow post-fence accesses to retire and complete before the fence completes. Unfortunately, such designs present implementation difficulties due to their reliance on global state and structures.This paper's goal is to optimize both the performance and the implementability of fences. We start-off with a design like the most aggressive ones but without the global state. We call it Weak Fence or wF. Since the concurrent execution of multiple wFs can deadlock, we combine wFs with a conventional fence (i.e., Strong Fence or sF) for the less performance-critical thread(s). We call the result an Asymmetric fence group. We also propose a taxonomy of Asymmetric fence groups under TSO. Compared to past aggressive fences, Asymmetric fence groups both are substantially easier to implement and have higher average performance. The two main designs presented (WS+ and W+) speed-up workloads under TSO by an average of 13% and 21%, respectively, over conventional fences.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {531–543},
numpages = {13},
keywords = {fences, parallel programming, synchronization, sequential consistency, shared-memory machines},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694356,
author = {Sung, Hyojin and Adve, Sarita V.},
title = {DeNovoSync: Efficient Support for Arbitrary Synchronization without Writer-Initiated Invalidations},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694356},
doi = {10.1145/2775054.2694356},
abstract = {Current shared-memory hardware is complex and inefficient. Prior work on the DeNovo coherence protocol showed that disciplined shared-memory programming models can enable more complexity-, performance-, and energy-efficient hardware than the state-of-the-art MESI protocol. DeNovo, however, severely restricted the synchronization constructs an application can support. This paper proposes DeNovoSync, a technique to support arbitrary synchronization in DeNovo. The key challenge is that DeNovo exploits race-freedom to use reader-initiated local self-invalidations (instead of conventional writer-initiated remote cache invalidations) to ensure coherence. Synchronization accesses are inherently racy and not directly amenable to self-invalidations. DeNovoSync addresses this challenge using a novel combination of registration of all synchronization reads with a judicious hardware backoff to limit unnecessary registrations. For a wide variety of synchronization constructs and applications, compared to MESI, DeNovoSync shows comparable or up to 22% lower execution time and up to 58% lower network traffic, enabling DeNovo's advantages for a much broader class of software than previously possible.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {545–559},
numpages = {15},
keywords = {consistency, cache coherence, synchronization, shared memory}
}

@article{10.1145/2786763.2694356,
author = {Sung, Hyojin and Adve, Sarita V.},
title = {DeNovoSync: Efficient Support for Arbitrary Synchronization without Writer-Initiated Invalidations},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694356},
doi = {10.1145/2786763.2694356},
abstract = {Current shared-memory hardware is complex and inefficient. Prior work on the DeNovo coherence protocol showed that disciplined shared-memory programming models can enable more complexity-, performance-, and energy-efficient hardware than the state-of-the-art MESI protocol. DeNovo, however, severely restricted the synchronization constructs an application can support. This paper proposes DeNovoSync, a technique to support arbitrary synchronization in DeNovo. The key challenge is that DeNovo exploits race-freedom to use reader-initiated local self-invalidations (instead of conventional writer-initiated remote cache invalidations) to ensure coherence. Synchronization accesses are inherently racy and not directly amenable to self-invalidations. DeNovoSync addresses this challenge using a novel combination of registration of all synchronization reads with a judicious hardware backoff to limit unnecessary registrations. For a wide variety of synchronization constructs and applications, compared to MESI, DeNovoSync shows comparable or up to 22% lower execution time and up to 58% lower network traffic, enabling DeNovo's advantages for a much broader class of software than previously possible.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {545–559},
numpages = {15},
keywords = {synchronization, cache coherence, shared memory, consistency}
}

@inproceedings{10.1145/2694344.2694356,
author = {Sung, Hyojin and Adve, Sarita V.},
title = {DeNovoSync: Efficient Support for Arbitrary Synchronization without Writer-Initiated Invalidations},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694356},
doi = {10.1145/2694344.2694356},
abstract = {Current shared-memory hardware is complex and inefficient. Prior work on the DeNovo coherence protocol showed that disciplined shared-memory programming models can enable more complexity-, performance-, and energy-efficient hardware than the state-of-the-art MESI protocol. DeNovo, however, severely restricted the synchronization constructs an application can support. This paper proposes DeNovoSync, a technique to support arbitrary synchronization in DeNovo. The key challenge is that DeNovo exploits race-freedom to use reader-initiated local self-invalidations (instead of conventional writer-initiated remote cache invalidations) to ensure coherence. Synchronization accesses are inherently racy and not directly amenable to self-invalidations. DeNovoSync addresses this challenge using a novel combination of registration of all synchronization reads with a judicious hardware backoff to limit unnecessary registrations. For a wide variety of synchronization constructs and applications, compared to MESI, DeNovoSync shows comparable or up to 22% lower execution time and up to 58% lower network traffic, enabling DeNovo's advantages for a much broader class of software than previously possible.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {545–559},
numpages = {15},
keywords = {shared memory, cache coherence, consistency, synchronization},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694379,
author = {Sengupta, Aritra and Biswas, Swarnendu and Zhang, Minjia and Bond, Michael D. and Kulkarni, Milind},
title = {Hybrid Static–Dynamic Analysis for Statically Bounded Region Serializability},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694379},
doi = {10.1145/2775054.2694379},
abstract = {Data races are common. They are difficult to detect, avoid, or eliminate, and programmers sometimes introduce them intentionally. However, shared-memory programs with data races have unexpected, erroneous behaviors. Intentional and unintentional data races lead to atomicity and sequential consistency (SC) violations, and they make it more difficult to understand, test, and verify software. Existing approaches for providing stronger guarantees for racy executions add high run-time overhead and/or rely on custom hardware. This paper shows how to provide stronger semantics for racy programs while providing relatively good performance on commodity systems. A novel hybrid static--dynamic analysis called emph{EnfoRSer} provides end-to-end support for a memory model called emph{statically bounded region serializability} (SBRS) that is not only stronger than weak memory models but is strictly stronger than SC. EnfoRSer uses static compiler analysis to transform regions, and dynamic analysis to detect and resolve conflicts at run time. By demonstrating commodity support for a reasonably strong memory model with reasonable overheads, we show its potential as an always-on execution model.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {561–575},
numpages = {15},
keywords = {static analysis, synchronization, memory models, region serializability, atomicity, dynamic analysis}
}

@article{10.1145/2786763.2694379,
author = {Sengupta, Aritra and Biswas, Swarnendu and Zhang, Minjia and Bond, Michael D. and Kulkarni, Milind},
title = {Hybrid Static–Dynamic Analysis for Statically Bounded Region Serializability},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694379},
doi = {10.1145/2786763.2694379},
abstract = {Data races are common. They are difficult to detect, avoid, or eliminate, and programmers sometimes introduce them intentionally. However, shared-memory programs with data races have unexpected, erroneous behaviors. Intentional and unintentional data races lead to atomicity and sequential consistency (SC) violations, and they make it more difficult to understand, test, and verify software. Existing approaches for providing stronger guarantees for racy executions add high run-time overhead and/or rely on custom hardware. This paper shows how to provide stronger semantics for racy programs while providing relatively good performance on commodity systems. A novel hybrid static--dynamic analysis called emph{EnfoRSer} provides end-to-end support for a memory model called emph{statically bounded region serializability} (SBRS) that is not only stronger than weak memory models but is strictly stronger than SC. EnfoRSer uses static compiler analysis to transform regions, and dynamic analysis to detect and resolve conflicts at run time. By demonstrating commodity support for a reasonably strong memory model with reasonable overheads, we show its potential as an always-on execution model.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {561–575},
numpages = {15},
keywords = {memory models, static analysis, region serializability, dynamic analysis, synchronization, atomicity}
}

@inproceedings{10.1145/2694344.2694379,
author = {Sengupta, Aritra and Biswas, Swarnendu and Zhang, Minjia and Bond, Michael D. and Kulkarni, Milind},
title = {Hybrid Static–Dynamic Analysis for Statically Bounded Region Serializability},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694379},
doi = {10.1145/2694344.2694379},
abstract = {Data races are common. They are difficult to detect, avoid, or eliminate, and programmers sometimes introduce them intentionally. However, shared-memory programs with data races have unexpected, erroneous behaviors. Intentional and unintentional data races lead to atomicity and sequential consistency (SC) violations, and they make it more difficult to understand, test, and verify software. Existing approaches for providing stronger guarantees for racy executions add high run-time overhead and/or rely on custom hardware. This paper shows how to provide stronger semantics for racy programs while providing relatively good performance on commodity systems. A novel hybrid static--dynamic analysis called emph{EnfoRSer} provides end-to-end support for a memory model called emph{statically bounded region serializability} (SBRS) that is not only stronger than weak memory models but is strictly stronger than SC. EnfoRSer uses static compiler analysis to transform regions, and dynamic analysis to detect and resolve conflicts at run time. By demonstrating commodity support for a reasonably strong memory model with reasonable overheads, we show its potential as an always-on execution model.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {561–575},
numpages = {15},
keywords = {static analysis, atomicity, region serializability, dynamic analysis, synchronization, memory models},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251036,
author = {Ozturk, Ozcan},
title = {Session Details: Session 7B: GPUs},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251036},
doi = {10.1145/3251036},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694391,
author = {Alglave, Jade and Batty, Mark and Donaldson, Alastair F. and Gopalakrishnan, Ganesh and Ketema, Jeroen and Poetzl, Daniel and Sorensen, Tyler and Wickerson, John},
title = {GPU Concurrency: Weak Behaviours and Programming Assumptions},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694391},
doi = {10.1145/2775054.2694391},
abstract = {Concurrency is pervasive and perplexing, particularly on graphics processing units (GPUs). Current specifications of languages and hardware are inconclusive; thus programmers often rely on folklore assumptions when writing software.To remedy this state of affairs, we conducted a large empirical study of the concurrent behaviour of deployed GPUs. Armed with litmus tests (i.e. short concurrent programs), we questioned the assumptions in programming guides and vendor documentation about the guarantees provided by hardware. We developed a tool to generate thousands of litmus tests and run them under stressful workloads. We observed a litany of previously elusive weak behaviours, and exposed folklore beliefs about GPU programming---often supported by official tutorials---as false.As a way forward, we propose a model of Nvidia GPU hardware, which correctly models every behaviour witnessed in our experiments. The model is a variant of SPARC Relaxed Memory Order (RMO), structured following the GPU concurrency hierarchy.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {577–591},
numpages = {15},
keywords = {formal model, Nvidia PTX, memory consistency, openCL, litmus testing, GPU, test generation}
}

@article{10.1145/2786763.2694391,
author = {Alglave, Jade and Batty, Mark and Donaldson, Alastair F. and Gopalakrishnan, Ganesh and Ketema, Jeroen and Poetzl, Daniel and Sorensen, Tyler and Wickerson, John},
title = {GPU Concurrency: Weak Behaviours and Programming Assumptions},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694391},
doi = {10.1145/2786763.2694391},
abstract = {Concurrency is pervasive and perplexing, particularly on graphics processing units (GPUs). Current specifications of languages and hardware are inconclusive; thus programmers often rely on folklore assumptions when writing software.To remedy this state of affairs, we conducted a large empirical study of the concurrent behaviour of deployed GPUs. Armed with litmus tests (i.e. short concurrent programs), we questioned the assumptions in programming guides and vendor documentation about the guarantees provided by hardware. We developed a tool to generate thousands of litmus tests and run them under stressful workloads. We observed a litany of previously elusive weak behaviours, and exposed folklore beliefs about GPU programming---often supported by official tutorials---as false.As a way forward, we propose a model of Nvidia GPU hardware, which correctly models every behaviour witnessed in our experiments. The model is a variant of SPARC Relaxed Memory Order (RMO), structured following the GPU concurrency hierarchy.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {577–591},
numpages = {15},
keywords = {litmus testing, formal model, openCL, memory consistency, Nvidia PTX, test generation, GPU}
}

@inproceedings{10.1145/2694344.2694391,
author = {Alglave, Jade and Batty, Mark and Donaldson, Alastair F. and Gopalakrishnan, Ganesh and Ketema, Jeroen and Poetzl, Daniel and Sorensen, Tyler and Wickerson, John},
title = {GPU Concurrency: Weak Behaviours and Programming Assumptions},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694391},
doi = {10.1145/2694344.2694391},
abstract = {Concurrency is pervasive and perplexing, particularly on graphics processing units (GPUs). Current specifications of languages and hardware are inconclusive; thus programmers often rely on folklore assumptions when writing software.To remedy this state of affairs, we conducted a large empirical study of the concurrent behaviour of deployed GPUs. Armed with litmus tests (i.e. short concurrent programs), we questioned the assumptions in programming guides and vendor documentation about the guarantees provided by hardware. We developed a tool to generate thousands of litmus tests and run them under stressful workloads. We observed a litany of previously elusive weak behaviours, and exposed folklore beliefs about GPU programming---often supported by official tutorials---as false.As a way forward, we propose a model of Nvidia GPU hardware, which correctly models every behaviour witnessed in our experiments. The model is a variant of SPARC Relaxed Memory Order (RMO), structured following the GPU concurrency hierarchy.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {577–591},
numpages = {15},
keywords = {memory consistency, Nvidia PTX, formal model, GPU, openCL, test generation, litmus testing},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@dataset{10.1145/review-2694344.2694391_R51156,
author = {Murthy, Karthik S},
title = {Review ID:R51156 for DOI: 10.1145/2694344.2694391},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-2694344.2694391_R51156}
}

@article{10.1145/2775054.2694346,
author = {Park, Jason Jong Kyu and Park, Yongjun and Mahlke, Scott},
title = {Chimera: Collaborative Preemption for Multitasking on a Shared GPU},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694346},
doi = {10.1145/2775054.2694346},
abstract = {The demand for multitasking on graphics processing units (GPUs) is constantly increasing as they have become one of the default components on modern computer systems along with traditional processors (CPUs). Preemptive multitasking on CPUs has been primarily supported through context switching. However, the same preemption strategy incurs substantial overhead due to the large context in GPUs. The overhead comes in two dimensions: a preempting kernel suffers from a long preemption latency, and the system throughput is wasted during the switch. Without precise control over the large preemption overhead, multitasking on GPUs has little use for applications with strict latency requirements.In this paper, we propose Chimera, a collaborative preemption approach that can precisely control the overhead for multitasking on GPUs. Chimera first introduces streaming multiprocessor (SM) flushing, which can instantly preempt an SM by detecting and exploiting idempotent execution. Chimera utilizes flushing collaboratively with two previously proposed preemption techniques for GPUs, namely context switching and draining to minimize throughput overhead while achieving a required preemption latency. Evaluations show that Chimera violates the deadline for only 0.2% of preemption requests when a 15us preemption latency constraint is used. For multi-programmed workloads, Chimera can improve the average normalized turnaround time by 5.5x, and system throughput by 12.2%.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {593–606},
numpages = {14},
keywords = {preemptive multitasking, context switch, idempotence, graphics processing unit}
}

@article{10.1145/2786763.2694346,
author = {Park, Jason Jong Kyu and Park, Yongjun and Mahlke, Scott},
title = {Chimera: Collaborative Preemption for Multitasking on a Shared GPU},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694346},
doi = {10.1145/2786763.2694346},
abstract = {The demand for multitasking on graphics processing units (GPUs) is constantly increasing as they have become one of the default components on modern computer systems along with traditional processors (CPUs). Preemptive multitasking on CPUs has been primarily supported through context switching. However, the same preemption strategy incurs substantial overhead due to the large context in GPUs. The overhead comes in two dimensions: a preempting kernel suffers from a long preemption latency, and the system throughput is wasted during the switch. Without precise control over the large preemption overhead, multitasking on GPUs has little use for applications with strict latency requirements.In this paper, we propose Chimera, a collaborative preemption approach that can precisely control the overhead for multitasking on GPUs. Chimera first introduces streaming multiprocessor (SM) flushing, which can instantly preempt an SM by detecting and exploiting idempotent execution. Chimera utilizes flushing collaboratively with two previously proposed preemption techniques for GPUs, namely context switching and draining to minimize throughput overhead while achieving a required preemption latency. Evaluations show that Chimera violates the deadline for only 0.2% of preemption requests when a 15us preemption latency constraint is used. For multi-programmed workloads, Chimera can improve the average normalized turnaround time by 5.5x, and system throughput by 12.2%.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {593–606},
numpages = {14},
keywords = {preemptive multitasking, idempotence, graphics processing unit, context switch}
}

@inproceedings{10.1145/2694344.2694346,
author = {Park, Jason Jong Kyu and Park, Yongjun and Mahlke, Scott},
title = {Chimera: Collaborative Preemption for Multitasking on a Shared GPU},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694346},
doi = {10.1145/2694344.2694346},
abstract = {The demand for multitasking on graphics processing units (GPUs) is constantly increasing as they have become one of the default components on modern computer systems along with traditional processors (CPUs). Preemptive multitasking on CPUs has been primarily supported through context switching. However, the same preemption strategy incurs substantial overhead due to the large context in GPUs. The overhead comes in two dimensions: a preempting kernel suffers from a long preemption latency, and the system throughput is wasted during the switch. Without precise control over the large preemption overhead, multitasking on GPUs has little use for applications with strict latency requirements.In this paper, we propose Chimera, a collaborative preemption approach that can precisely control the overhead for multitasking on GPUs. Chimera first introduces streaming multiprocessor (SM) flushing, which can instantly preempt an SM by detecting and exploiting idempotent execution. Chimera utilizes flushing collaboratively with two previously proposed preemption techniques for GPUs, namely context switching and draining to minimize throughput overhead while achieving a required preemption latency. Evaluations show that Chimera violates the deadline for only 0.2% of preemption requests when a 15us preemption latency constraint is used. For multi-programmed workloads, Chimera can improve the average normalized turnaround time by 5.5x, and system throughput by 12.2%.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {593–606},
numpages = {14},
keywords = {graphics processing unit, context switch, preemptive multitasking, idempotence},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694381,
author = {Agarwal, Neha and Nellans, David and Stephenson, Mark and O'Connor, Mike and Keckler, Stephen W.},
title = {Page Placement Strategies for GPUs within Heterogeneous Memory Systems},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694381},
doi = {10.1145/2775054.2694381},
abstract = {Systems from smartphones to supercomputers are increasingly heterogeneous, being composed of both CPUs and GPUs. To maximize cost and energy efficiency, these systems will increasingly use globally-addressable heterogeneous memory systems, making choices about memory page placement critical to performance. In this work we show that current page placement policies are not sufficient to maximize GPU performance in these heterogeneous memory systems. We propose two new page placement policies that improve GPU performance: one application agnostic and one using application profile information. Our application agnostic policy, bandwidth-aware (BW-AWARE) placement, maximizes GPU throughput by balancing page placement across the memories based on the aggregate memory bandwidth available in a system. Our simulation-based results show that BW-AWARE placement outperforms the existing Linux INTERLEAVE and LOCAL policies by 35% and 18% on average for GPU compute workloads. We build upon BW-AWARE placement by developing a compiler-based profiling mechanism that provides programmers with information about GPU application data structure access patterns. Combining this information with simple program-annotated hints about memory placement, our hint-based page placement approach performs within 90% of oracular page placement on average, largely mitigating the need for costly dynamic page tracking and migration.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {607–618},
numpages = {12},
keywords = {linux, bandwidth, page placement, program annotation}
}

@article{10.1145/2786763.2694381,
author = {Agarwal, Neha and Nellans, David and Stephenson, Mark and O'Connor, Mike and Keckler, Stephen W.},
title = {Page Placement Strategies for GPUs within Heterogeneous Memory Systems},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694381},
doi = {10.1145/2786763.2694381},
abstract = {Systems from smartphones to supercomputers are increasingly heterogeneous, being composed of both CPUs and GPUs. To maximize cost and energy efficiency, these systems will increasingly use globally-addressable heterogeneous memory systems, making choices about memory page placement critical to performance. In this work we show that current page placement policies are not sufficient to maximize GPU performance in these heterogeneous memory systems. We propose two new page placement policies that improve GPU performance: one application agnostic and one using application profile information. Our application agnostic policy, bandwidth-aware (BW-AWARE) placement, maximizes GPU throughput by balancing page placement across the memories based on the aggregate memory bandwidth available in a system. Our simulation-based results show that BW-AWARE placement outperforms the existing Linux INTERLEAVE and LOCAL policies by 35% and 18% on average for GPU compute workloads. We build upon BW-AWARE placement by developing a compiler-based profiling mechanism that provides programmers with information about GPU application data structure access patterns. Combining this information with simple program-annotated hints about memory placement, our hint-based page placement approach performs within 90% of oracular page placement on average, largely mitigating the need for costly dynamic page tracking and migration.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {607–618},
numpages = {12},
keywords = {linux, page placement, bandwidth, program annotation}
}

@inproceedings{10.1145/2694344.2694381,
author = {Agarwal, Neha and Nellans, David and Stephenson, Mark and O'Connor, Mike and Keckler, Stephen W.},
title = {Page Placement Strategies for GPUs within Heterogeneous Memory Systems},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694381},
doi = {10.1145/2694344.2694381},
abstract = {Systems from smartphones to supercomputers are increasingly heterogeneous, being composed of both CPUs and GPUs. To maximize cost and energy efficiency, these systems will increasingly use globally-addressable heterogeneous memory systems, making choices about memory page placement critical to performance. In this work we show that current page placement policies are not sufficient to maximize GPU performance in these heterogeneous memory systems. We propose two new page placement policies that improve GPU performance: one application agnostic and one using application profile information. Our application agnostic policy, bandwidth-aware (BW-AWARE) placement, maximizes GPU throughput by balancing page placement across the memories based on the aggregate memory bandwidth available in a system. Our simulation-based results show that BW-AWARE placement outperforms the existing Linux INTERLEAVE and LOCAL policies by 35% and 18% on average for GPU compute workloads. We build upon BW-AWARE placement by developing a compiler-based profiling mechanism that provides programmers with information about GPU application data structure access patterns. Combining this information with simple program-annotated hints about memory placement, our hint-based page placement approach performs within 90% of oracular page placement on average, largely mitigating the need for costly dynamic page tracking and migration.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {607–618},
numpages = {12},
keywords = {bandwidth, program annotation, page placement, linux},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251037,
author = {Unsal, Osman},
title = {Session Details: Session 8A: Scalable Parallelism},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251037},
doi = {10.1145/3251037},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694369,
author = {Zhao, Zhijia and Shen, Xipeng},
title = {On-the-Fly Principled Speculation for FSM Parallelization},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694369},
doi = {10.1145/2775054.2694369},
abstract = {Finite State Machine (FSM) is the backbone of an important class of applications in many domains. Its parallelization has been extremely difficult due to inherent strong dependences in the computation. Recently, principled speculation shows good promise to solve the problem. However, the reliance on offline training makes the approach inconvenient to adopt and hard to apply to many practical FSM applications, which often deal with a large variety of inputs different from training inputs. This work presents an assembly of techniques that completely remove the needs for offline training. The techniques include a set of theoretical results on inherent properties of FSMs, and two newly designed dynamic optimizations for efficient FSM characterization. The new techniques, for the first time, make principle speculation applicable on the fly, and enables swift, automatic configuration of speculative parallelizations to best suit a given FSM and its current input. They eliminate the fundamental barrier for practical adoption of principle speculation for FSM parallelization. Experiments show that the new techniques give significantly higher speedups for some difficult FSM applications in the presence of input changes.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {619–630},
numpages = {12},
keywords = {online profiling, multicore, speculative parallelization, finite state machine, FSM, DFA}
}

@article{10.1145/2786763.2694369,
author = {Zhao, Zhijia and Shen, Xipeng},
title = {On-the-Fly Principled Speculation for FSM Parallelization},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694369},
doi = {10.1145/2786763.2694369},
abstract = {Finite State Machine (FSM) is the backbone of an important class of applications in many domains. Its parallelization has been extremely difficult due to inherent strong dependences in the computation. Recently, principled speculation shows good promise to solve the problem. However, the reliance on offline training makes the approach inconvenient to adopt and hard to apply to many practical FSM applications, which often deal with a large variety of inputs different from training inputs. This work presents an assembly of techniques that completely remove the needs for offline training. The techniques include a set of theoretical results on inherent properties of FSMs, and two newly designed dynamic optimizations for efficient FSM characterization. The new techniques, for the first time, make principle speculation applicable on the fly, and enables swift, automatic configuration of speculative parallelizations to best suit a given FSM and its current input. They eliminate the fundamental barrier for practical adoption of principle speculation for FSM parallelization. Experiments show that the new techniques give significantly higher speedups for some difficult FSM applications in the presence of input changes.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {619–630},
numpages = {12},
keywords = {FSM, DFA, online profiling, multicore, speculative parallelization, finite state machine}
}

@inproceedings{10.1145/2694344.2694369,
author = {Zhao, Zhijia and Shen, Xipeng},
title = {On-the-Fly Principled Speculation for FSM Parallelization},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694369},
doi = {10.1145/2694344.2694369},
abstract = {Finite State Machine (FSM) is the backbone of an important class of applications in many domains. Its parallelization has been extremely difficult due to inherent strong dependences in the computation. Recently, principled speculation shows good promise to solve the problem. However, the reliance on offline training makes the approach inconvenient to adopt and hard to apply to many practical FSM applications, which often deal with a large variety of inputs different from training inputs. This work presents an assembly of techniques that completely remove the needs for offline training. The techniques include a set of theoretical results on inherent properties of FSMs, and two newly designed dynamic optimizations for efficient FSM characterization. The new techniques, for the first time, make principle speculation applicable on the fly, and enables swift, automatic configuration of speculative parallelizations to best suit a given FSM and its current input. They eliminate the fundamental barrier for practical adoption of principle speculation for FSM parallelization. Experiments show that the new techniques give significantly higher speedups for some difficult FSM applications in the presence of input changes.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {619–630},
numpages = {12},
keywords = {speculative parallelization, DFA, online profiling, multicore, finite state machine, FSM},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694359,
author = {David, Tudor and Guerraoui, Rachid and Trigonakis, Vasileios},
title = {Asynchronized Concurrency: The Secret to Scaling Concurrent Search Data Structures},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694359},
doi = {10.1145/2775054.2694359},
abstract = {We introduce "asynchronized concurrency (ASCY)," a paradigm consisting of four complementary programming patterns. ASCY calls for the design of concurrent search data structures (CSDSs) to resemble that of their sequential counterparts. We argue that ASCY leads to implementations which are portably scalable: they scale across different types of hardware platforms, including single and multi-socket ones, for various classes of workloads, such as read-only and read-write, and according to different performance metrics, including throughput, latency, and energy. We substantiate our thesis through the most exhaustive evaluation of CSDSs to date, involving 6 platforms, 22 state-of-the-art CSDS algorithms, 10 re-engineered state-of-the-art CSDS algorithms following the ASCY patterns, and 2 new CSDS algorithms designed with ASCY in mind. We observe up to 30% improvements in throughput in the re-engineered algorithms, while our new algorithms out-perform the state-of-the-art alternatives.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {631–644},
numpages = {14},
keywords = {portability, scalability, concurrent data structures, multi-cores}
}

@article{10.1145/2786763.2694359,
author = {David, Tudor and Guerraoui, Rachid and Trigonakis, Vasileios},
title = {Asynchronized Concurrency: The Secret to Scaling Concurrent Search Data Structures},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694359},
doi = {10.1145/2786763.2694359},
abstract = {We introduce "asynchronized concurrency (ASCY)," a paradigm consisting of four complementary programming patterns. ASCY calls for the design of concurrent search data structures (CSDSs) to resemble that of their sequential counterparts. We argue that ASCY leads to implementations which are portably scalable: they scale across different types of hardware platforms, including single and multi-socket ones, for various classes of workloads, such as read-only and read-write, and according to different performance metrics, including throughput, latency, and energy. We substantiate our thesis through the most exhaustive evaluation of CSDSs to date, involving 6 platforms, 22 state-of-the-art CSDS algorithms, 10 re-engineered state-of-the-art CSDS algorithms following the ASCY patterns, and 2 new CSDS algorithms designed with ASCY in mind. We observe up to 30% improvements in throughput in the re-engineered algorithms, while our new algorithms out-perform the state-of-the-art alternatives.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {631–644},
numpages = {14},
keywords = {scalability, portability, concurrent data structures, multi-cores}
}

@inproceedings{10.1145/2694344.2694359,
author = {David, Tudor and Guerraoui, Rachid and Trigonakis, Vasileios},
title = {Asynchronized Concurrency: The Secret to Scaling Concurrent Search Data Structures},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694359},
doi = {10.1145/2694344.2694359},
abstract = {We introduce "asynchronized concurrency (ASCY)," a paradigm consisting of four complementary programming patterns. ASCY calls for the design of concurrent search data structures (CSDSs) to resemble that of their sequential counterparts. We argue that ASCY leads to implementations which are portably scalable: they scale across different types of hardware platforms, including single and multi-socket ones, for various classes of workloads, such as read-only and read-write, and according to different performance metrics, including throughput, latency, and energy. We substantiate our thesis through the most exhaustive evaluation of CSDSs to date, involving 6 platforms, 22 state-of-the-art CSDS algorithms, 10 re-engineered state-of-the-art CSDS algorithms following the ASCY patterns, and 2 new CSDS algorithms designed with ASCY in mind. We observe up to 30% improvements in throughput in the re-engineered algorithms, while our new algorithms out-perform the state-of-the-art alternatives.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {631–644},
numpages = {14},
keywords = {portability, scalability, multi-cores, concurrent data structures},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694371,
author = {Bhatotia, Pramod and Fonseca, Pedro and Acar, Umut A. and Brandenburg, Bj\"{o}rn B. and Rodrigues, Rodrigo},
title = {IThreads: A Threading Library for Parallel Incremental Computation},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694371},
doi = {10.1145/2775054.2694371},
abstract = {Incremental computation strives for efficient successive runs of applications by re-executing only those parts of the computation that are affected by a given input change instead of recomputing everything from scratch. To realize these benefits automatically, we describe iThreads, a threading library for parallel incremental computation. iThreads supports unmodified shared-memory multithreaded programs: it can be used as a replacement for pthreads by a simple exchange of dynamically linked libraries, without even recompiling the application code. To enable such an interface, we designed algorithms and an implementation to operate at the compiled binary code level by leveraging MMU-assisted memory access tracking and process-based thread isolation. Our evaluation on a multicore platform using applications from the PARSEC and Phoenix benchmarks and two case-studies shows significant performance gains.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {645–659},
numpages = {15},
keywords = {release consistency (RC) memory model, self-adjusting computation, shared-memory multithreading, incremental computation, memoization, concurrent dynamic dependence graph (CDDG)}
}

@article{10.1145/2786763.2694371,
author = {Bhatotia, Pramod and Fonseca, Pedro and Acar, Umut A. and Brandenburg, Bj\"{o}rn B. and Rodrigues, Rodrigo},
title = {IThreads: A Threading Library for Parallel Incremental Computation},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694371},
doi = {10.1145/2786763.2694371},
abstract = {Incremental computation strives for efficient successive runs of applications by re-executing only those parts of the computation that are affected by a given input change instead of recomputing everything from scratch. To realize these benefits automatically, we describe iThreads, a threading library for parallel incremental computation. iThreads supports unmodified shared-memory multithreaded programs: it can be used as a replacement for pthreads by a simple exchange of dynamically linked libraries, without even recompiling the application code. To enable such an interface, we designed algorithms and an implementation to operate at the compiled binary code level by leveraging MMU-assisted memory access tracking and process-based thread isolation. Our evaluation on a multicore platform using applications from the PARSEC and Phoenix benchmarks and two case-studies shows significant performance gains.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {645–659},
numpages = {15},
keywords = {self-adjusting computation, incremental computation, release consistency (RC) memory model, shared-memory multithreading, concurrent dynamic dependence graph (CDDG), memoization}
}

@inproceedings{10.1145/2694344.2694371,
author = {Bhatotia, Pramod and Fonseca, Pedro and Acar, Umut A. and Brandenburg, Bj\"{o}rn B. and Rodrigues, Rodrigo},
title = {IThreads: A Threading Library for Parallel Incremental Computation},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694371},
doi = {10.1145/2694344.2694371},
abstract = {Incremental computation strives for efficient successive runs of applications by re-executing only those parts of the computation that are affected by a given input change instead of recomputing everything from scratch. To realize these benefits automatically, we describe iThreads, a threading library for parallel incremental computation. iThreads supports unmodified shared-memory multithreaded programs: it can be used as a replacement for pthreads by a simple exchange of dynamically linked libraries, without even recompiling the application code. To enable such an interface, we designed algorithms and an implementation to operate at the compiled binary code level by leveraging MMU-assisted memory access tracking and process-based thread isolation. Our evaluation on a multicore platform using applications from the PARSEC and Phoenix benchmarks and two case-studies shows significant performance gains.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {645–659},
numpages = {15},
keywords = {shared-memory multithreading, incremental computation, release consistency (RC) memory model, concurrent dynamic dependence graph (CDDG), memoization, self-adjusting computation},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@inproceedings{10.1145/3251041,
author = {Buyuktosunoglu, Alper},
title = {Session Details: Session 8B: Memory Management},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251041},
doi = {10.1145/3251041},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694361,
author = {Gidra, Lokesh and Thomas, Ga\"{e}l and Sopena, Julien and Shapiro, Marc and Nguyen, Nhan},
title = {NumaGiC: A Garbage Collector for Big Data on Big NUMA Machines},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694361},
doi = {10.1145/2775054.2694361},
abstract = {On contemporary cache-coherent Non-Uniform Memory Access (ccNUMA) architectures, applications with a large memory footprint suffer from the cost of the garbage collector (GC), because, as the GC scans the reference graph, it makes many remote memory accesses, saturating the interconnect between memory nodes. We address this problem with NumaGiC, a GC with a mostly-distributed design. In order to maximise memory access locality during collection, a GC thread avoids accessing a different memory node, instead notifying a remote GC thread with a message; nonetheless, NumaGiC avoids the drawbacks of a pure distributed design, which tends to decrease parallelism. We compare NumaGiC with Parallel Scavenge and NAPS on two different ccNUMA architectures running on the Hotspot Java Virtual Machine of OpenJDK 7. On Spark and Neo4j, two industry-strength analytics applications, with heap sizes ranging from 160GB to 350GB, and on SPECjbb2013 and SPECjbb2005, ourgc improves overall performance by up to 45% over NAPS (up to 94% over Parallel Scavenge), and increases the performance of the collector itself by up to 3.6x over NAPS (up to 5.4x over Parallel Scavenge).},
journal = {SIGPLAN Not.},
month = {mar},
pages = {661–673},
numpages = {13},
keywords = {NUMA, garbage collection, multicore}
}

@article{10.1145/2786763.2694361,
author = {Gidra, Lokesh and Thomas, Ga\"{e}l and Sopena, Julien and Shapiro, Marc and Nguyen, Nhan},
title = {NumaGiC: A Garbage Collector for Big Data on Big NUMA Machines},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694361},
doi = {10.1145/2786763.2694361},
abstract = {On contemporary cache-coherent Non-Uniform Memory Access (ccNUMA) architectures, applications with a large memory footprint suffer from the cost of the garbage collector (GC), because, as the GC scans the reference graph, it makes many remote memory accesses, saturating the interconnect between memory nodes. We address this problem with NumaGiC, a GC with a mostly-distributed design. In order to maximise memory access locality during collection, a GC thread avoids accessing a different memory node, instead notifying a remote GC thread with a message; nonetheless, NumaGiC avoids the drawbacks of a pure distributed design, which tends to decrease parallelism. We compare NumaGiC with Parallel Scavenge and NAPS on two different ccNUMA architectures running on the Hotspot Java Virtual Machine of OpenJDK 7. On Spark and Neo4j, two industry-strength analytics applications, with heap sizes ranging from 160GB to 350GB, and on SPECjbb2013 and SPECjbb2005, ourgc improves overall performance by up to 45% over NAPS (up to 94% over Parallel Scavenge), and increases the performance of the collector itself by up to 3.6x over NAPS (up to 5.4x over Parallel Scavenge).},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {661–673},
numpages = {13},
keywords = {garbage collection, multicore, NUMA}
}

@inproceedings{10.1145/2694344.2694361,
author = {Gidra, Lokesh and Thomas, Ga\"{e}l and Sopena, Julien and Shapiro, Marc and Nguyen, Nhan},
title = {NumaGiC: A Garbage Collector for Big Data on Big NUMA Machines},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694361},
doi = {10.1145/2694344.2694361},
abstract = {On contemporary cache-coherent Non-Uniform Memory Access (ccNUMA) architectures, applications with a large memory footprint suffer from the cost of the garbage collector (GC), because, as the GC scans the reference graph, it makes many remote memory accesses, saturating the interconnect between memory nodes. We address this problem with NumaGiC, a GC with a mostly-distributed design. In order to maximise memory access locality during collection, a GC thread avoids accessing a different memory node, instead notifying a remote GC thread with a message; nonetheless, NumaGiC avoids the drawbacks of a pure distributed design, which tends to decrease parallelism. We compare NumaGiC with Parallel Scavenge and NAPS on two different ccNUMA architectures running on the Hotspot Java Virtual Machine of OpenJDK 7. On Spark and Neo4j, two industry-strength analytics applications, with heap sizes ranging from 160GB to 350GB, and on SPECjbb2013 and SPECjbb2005, ourgc improves overall performance by up to 45% over NAPS (up to 94% over Parallel Scavenge), and increases the performance of the collector itself by up to 3.6x over NAPS (up to 5.4x over Parallel Scavenge).},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {661–673},
numpages = {13},
keywords = {garbage collection, NUMA, multicore},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694345,
author = {Nguyen, Khanh and Wang, Kai and Bu, Yingyi and Fang, Lu and Hu, Jianfei and Xu, Guoqing},
title = {FACADE: A Compiler and Runtime for (Almost) Object-Bounded Big Data Applications},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694345},
doi = {10.1145/2775054.2694345},
abstract = {The past decade has witnessed the increasing demands on data-driven business intelligence that led to the proliferation of data-intensive applications. A managed object-oriented programming language such as Java is often the developer's choice for implementing such applications, due to its quick development cycle and rich community resource. While the use of such languages makes programming easier, their automated memory management comes at a cost. When the managed runtime meets Big Data, this cost is significantly magnified and becomes a scalability-prohibiting bottleneck. This paper presents a novel compiler framework, called Facade, that can generate highly-efficient data manipulation code by automatically transforming the data path of an existing Big Data application. The key treatment is that in the generated code, the number of runtime heap objects created for data types in each thread is (almost) statically bounded, leading to significantly reduced memory management cost and improved scalability. We have implemented Facade and used it to transform 7 common applications on 3 real-world, already well-optimized Big Data frameworks: GraphChi, Hyracks, and GPS. Our experimental results are very positive: the generated programs have (1) achieved a 3%--48% execution time reduction and an up to 88X GC reduction; (2) consumed up to 50% less memory, and (3) scaled to much larger datasets.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {675–690},
numpages = {16},
keywords = {performance optimization, managed languages, memory management, big data applications}
}

@article{10.1145/2786763.2694345,
author = {Nguyen, Khanh and Wang, Kai and Bu, Yingyi and Fang, Lu and Hu, Jianfei and Xu, Guoqing},
title = {FACADE: A Compiler and Runtime for (Almost) Object-Bounded Big Data Applications},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694345},
doi = {10.1145/2786763.2694345},
abstract = {The past decade has witnessed the increasing demands on data-driven business intelligence that led to the proliferation of data-intensive applications. A managed object-oriented programming language such as Java is often the developer's choice for implementing such applications, due to its quick development cycle and rich community resource. While the use of such languages makes programming easier, their automated memory management comes at a cost. When the managed runtime meets Big Data, this cost is significantly magnified and becomes a scalability-prohibiting bottleneck. This paper presents a novel compiler framework, called Facade, that can generate highly-efficient data manipulation code by automatically transforming the data path of an existing Big Data application. The key treatment is that in the generated code, the number of runtime heap objects created for data types in each thread is (almost) statically bounded, leading to significantly reduced memory management cost and improved scalability. We have implemented Facade and used it to transform 7 common applications on 3 real-world, already well-optimized Big Data frameworks: GraphChi, Hyracks, and GPS. Our experimental results are very positive: the generated programs have (1) achieved a 3%--48% execution time reduction and an up to 88X GC reduction; (2) consumed up to 50% less memory, and (3) scaled to much larger datasets.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {675–690},
numpages = {16},
keywords = {memory management, managed languages, big data applications, performance optimization}
}

@inproceedings{10.1145/2694344.2694345,
author = {Nguyen, Khanh and Wang, Kai and Bu, Yingyi and Fang, Lu and Hu, Jianfei and Xu, Guoqing},
title = {FACADE: A Compiler and Runtime for (Almost) Object-Bounded Big Data Applications},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694345},
doi = {10.1145/2694344.2694345},
abstract = {The past decade has witnessed the increasing demands on data-driven business intelligence that led to the proliferation of data-intensive applications. A managed object-oriented programming language such as Java is often the developer's choice for implementing such applications, due to its quick development cycle and rich community resource. While the use of such languages makes programming easier, their automated memory management comes at a cost. When the managed runtime meets Big Data, this cost is significantly magnified and becomes a scalability-prohibiting bottleneck. This paper presents a novel compiler framework, called Facade, that can generate highly-efficient data manipulation code by automatically transforming the data path of an existing Big Data application. The key treatment is that in the generated code, the number of runtime heap objects created for data types in each thread is (almost) statically bounded, leading to significantly reduced memory management cost and improved scalability. We have implemented Facade and used it to transform 7 common applications on 3 real-world, already well-optimized Big Data frameworks: GraphChi, Hyracks, and GPS. Our experimental results are very positive: the generated programs have (1) achieved a 3%--48% execution time reduction and an up to 88X GC reduction; (2) consumed up to 50% less memory, and (3) scaled to much larger datasets.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {675–690},
numpages = {16},
keywords = {memory management, managed languages, performance optimization, big data applications},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

@article{10.1145/2775054.2694392,
author = {Agrawal, Varun and Dabral, Abhiroop and Palit, Tapti and Shen, Yongming and Ferdman, Michael},
title = {Architectural Support for Dynamic Linking},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775054.2694392},
doi = {10.1145/2775054.2694392},
abstract = {All software in use today relies on libraries, including standard libraries (e.g., C, C++) and application-specific libraries (e.g., libxml, libpng). Most libraries are loaded in memory and dynamically linked when programs are launched, resolving symbol addresses across the applications and libraries. Dynamic linking has many benefits: It allows code to be reused between applications, conserves memory (because only one copy of a library is kept in memory for all the applications that share it), and allows libraries to be patched and updated without modifying programs, among numerous other benefits. However, these benefits come at the cost of performance. For every call made to a function in a dynamically linked library, a trampoline is used to read the function address from a lookup table and branch to the function, incurring memory load and branch operations. Static linking avoids this performance penalty, but loses all the benefits of dynamic linking. Given its myriad benefits, dynamic linking is the predominant choice today, despite the performance cost. In this work, we propose a speculative hardware mechanism to optimize dynamic linking by avoiding executing the trampolines for library function calls, providing the benefits of dynamic linking with the performance of static linking. Speculatively skipping the memory load and branch operations of the library call trampolines improves performance by reducing the number of executed instructions and gains additional performance by reducing pressure on the instruction and data caches, TLBs, and branch predictors. Because the indirect targets of library call trampolines do not change during program execution, our speculative mechanism never misspeculates in practice. We evaluate our technique on real hardware with production software and observe up to 4% speedup using only 1.5KB of on-chip storage.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {691–702},
numpages = {12},
keywords = {hardware memoization, instruction elision, branch prediction, dynamic linking}
}

@article{10.1145/2786763.2694392,
author = {Agrawal, Varun and Dabral, Abhiroop and Palit, Tapti and Shen, Yongming and Ferdman, Michael},
title = {Architectural Support for Dynamic Linking},
year = {2015},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2786763.2694392},
doi = {10.1145/2786763.2694392},
abstract = {All software in use today relies on libraries, including standard libraries (e.g., C, C++) and application-specific libraries (e.g., libxml, libpng). Most libraries are loaded in memory and dynamically linked when programs are launched, resolving symbol addresses across the applications and libraries. Dynamic linking has many benefits: It allows code to be reused between applications, conserves memory (because only one copy of a library is kept in memory for all the applications that share it), and allows libraries to be patched and updated without modifying programs, among numerous other benefits. However, these benefits come at the cost of performance. For every call made to a function in a dynamically linked library, a trampoline is used to read the function address from a lookup table and branch to the function, incurring memory load and branch operations. Static linking avoids this performance penalty, but loses all the benefits of dynamic linking. Given its myriad benefits, dynamic linking is the predominant choice today, despite the performance cost. In this work, we propose a speculative hardware mechanism to optimize dynamic linking by avoiding executing the trampolines for library function calls, providing the benefits of dynamic linking with the performance of static linking. Speculatively skipping the memory load and branch operations of the library call trampolines improves performance by reducing the number of executed instructions and gains additional performance by reducing pressure on the instruction and data caches, TLBs, and branch predictors. Because the indirect targets of library call trampolines do not change during program execution, our speculative mechanism never misspeculates in practice. We evaluate our technique on real hardware with production software and observe up to 4% speedup using only 1.5KB of on-chip storage.},
journal = {SIGARCH Comput. Archit. News},
month = {mar},
pages = {691–702},
numpages = {12},
keywords = {instruction elision, hardware memoization, dynamic linking, branch prediction}
}

@inproceedings{10.1145/2694344.2694392,
author = {Agrawal, Varun and Dabral, Abhiroop and Palit, Tapti and Shen, Yongming and Ferdman, Michael},
title = {Architectural Support for Dynamic Linking},
year = {2015},
isbn = {9781450328357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2694344.2694392},
doi = {10.1145/2694344.2694392},
abstract = {All software in use today relies on libraries, including standard libraries (e.g., C, C++) and application-specific libraries (e.g., libxml, libpng). Most libraries are loaded in memory and dynamically linked when programs are launched, resolving symbol addresses across the applications and libraries. Dynamic linking has many benefits: It allows code to be reused between applications, conserves memory (because only one copy of a library is kept in memory for all the applications that share it), and allows libraries to be patched and updated without modifying programs, among numerous other benefits. However, these benefits come at the cost of performance. For every call made to a function in a dynamically linked library, a trampoline is used to read the function address from a lookup table and branch to the function, incurring memory load and branch operations. Static linking avoids this performance penalty, but loses all the benefits of dynamic linking. Given its myriad benefits, dynamic linking is the predominant choice today, despite the performance cost. In this work, we propose a speculative hardware mechanism to optimize dynamic linking by avoiding executing the trampolines for library function calls, providing the benefits of dynamic linking with the performance of static linking. Speculatively skipping the memory load and branch operations of the library call trampolines improves performance by reducing the number of executed instructions and gains additional performance by reducing pressure on the instruction and data caches, TLBs, and branch predictors. Because the indirect targets of library call trampolines do not change during program execution, our speculative mechanism never misspeculates in practice. We evaluate our technique on real hardware with production software and observe up to 4% speedup using only 1.5KB of on-chip storage.},
booktitle = {Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {691–702},
numpages = {12},
keywords = {instruction elision, hardware memoization, dynamic linking, branch prediction},
location = {Istanbul, Turkey},
series = {ASPLOS '15}
}

